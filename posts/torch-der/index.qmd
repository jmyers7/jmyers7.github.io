---
title: "PyTorch and derivatives: A mathematical perspective"
toc: true
date: "2026-02-03"
jupyter: site
categories: []
draft: true
bibliography: /Users/johnmyers/dev/site/aux-files/references.bib
csl: /Users/johnmyers/dev/site/aux-files/american-mathematical-society-numeric.csl
# image: "sigalg-logo.png"
open-graph:
    title: "PyTorch and derivatives: A mathematical perspective"
    # image: "https://johnmyers-phd.com/posts/torch-der/torch-der-logo.png"
---

::: {.content-hidden}
$$
{{< include /aux-files/custom.tex >}}
$$
:::

## Tensors as Hilbert spaces

::: {#def-tensor}
Let $I$ be a non-empty finite set that we call an *index set*, with elements $i\in I$ called *indices*. An *$I$-indexed tensor* is a function
$$
a : I \to \mathbb{R}, \quad i \mapsto a_i.
$$
We write $\mathbb{R}^I$ for the set of all $I$-indexed tensors, and we call it the *tensor space* of $I$-indexed tensors.
:::

For example, if $I = \{0,1,2\}$, then an $I$-indexed tensor $a$ is just a selection of three real numbers:

$$
0 \mapsto a_0, \quad 1 \mapsto a_1, \quad 2 \mapsto a_2,
$$
[Notice the use of "$0$-based indexing" here, which is standard in Python, but somewhat confusing for those accustomed to "$1$-based indexing" in mathematics.]{.aside}

which we may write in vector form:

$$
a = \begin{bmatrix} a_0 \\ a_1 \\ a_2 \end{bmatrix}.
$$

If $I = \{0,1\}\times \{0,1,2\}$, then an $I$-indexed tensor $b$ is a selection of six real numbers:

$$
(0,0) \mapsto b_{00}, \quad (0,1) \mapsto b_{01}, \quad (0,2) \mapsto b_{02}, \quad (1,0) \mapsto b_{10}, \quad (1,1) \mapsto b_{11}, \quad (1,2) \mapsto b_{12},
$$

which we may write in matrix form:

$$
b = \begin{bmatrix} b_{00} & b_{01} & b_{02} \\ b_{10} & b_{11} & b_{12} \end{bmatrix}.
$$

Note that the decision to express the tensor $a$ as a column vector, rather than a row vector, and to express $b$ as a $2\times 3$ matrix, rather than a $3\times 2$ matrix, is a just matter of convention. The tensors themselves are just collections of numbers indexed by the elements of the index set $I$, independent of any particular visual representation.

Tensors are $\bbr$-valued **functions** first and foremost, and any sort of function space like $\mathbb{R}^I$ carries a vector space structure over $\mathbb{R}$. Indeed, the operations are defined pointwise: for $a, b \in \mathbb{R}^I$ and $c \in \mathbb{R}$, we define

$$
(a + b)_i = a_i + b_i, \quad (c a)_i = c a_i,
$$

for all $i \in I$. The zero tensor $0 \in \mathbb{R}^I$ is defined by $0_i = 0$ for all $i \in I$.

But more than this, tensor spaces also carry a natural inner product structure, which is also defined pointwise:

::: {#def-tensor-hilbert}
Let $I$ be a non-empty finite index set, and consider the tensor space $\mathbb{R}^i$. The *inner product* of two tensors $a, b \in \mathbb{R}^I$ is defined as
$$
\langle a, b \rangle = \sum_{i\in I} a_i b_i.
$$
:::

When $I$ is a set of the form $I = \{1,2,\ldots,m\}$, so that tensors in $\bbr^I$ may be represented as column vectors, the inner product is the familiar dot product of two vectors.

One can show easily that the inner product defined above really is an inner product in the precise mathematical sense: it is positive definite, symmetric, and bilinear. Thus, the tensor space $\mathbb{R}^I$ is a finite-dimensional Hilbert space.

The tensor spaces that one works with in practice—and in PyTorch in particular—have index sets of the form

$$
I = \{0,1,\ldots,m_1-1\} \times \{0,1,\ldots,m_2-1\} \times \cdots \times \{0,1,\ldots,m_k-1\},
$$

for some $k$ called the *rank* of the tensor, and some positive integers $m_1, m_2, \ldots, m_k$. In this case, we will often write

$$
\mathbb{R}^{m_1 \times m_2 \times \cdots \times m_k}
$$

to stand for $\mathbb{R}^I$, where $I$ is the index set above. The rank of a tensor is the number of indices required to specify its entries, and the *shape* of a tensor is the tuple $(m_1, m_2, \ldots, m_k)$ that specifies the range of values for each index.

For example, we would write $\mathbb{R}^3$ to stand for $\mathbb{R}^{\{0,1,2\}}$, which consists of rank-$1$ tensors of shape $(3)$, i.e., column vectors with three entries. Similarly, we would write $\mathbb{R}^{2\times 3}$ to stand for $\mathbb{R}^{\{0,1\}\times \{0,1,2\}}$, which consists of rank-$2$ tensors of shape $(2,3)$, i.e., $2\times 3$ matrices. And so on.

Random integer-valued tensors of various shapes are easily generated in PyTorch using the `torch.randint` function. For example, the following code generates a random rank-$4$ tensor of shape $(2, 3, 4, 5)$ with integer values between $0$ and $9$:

```{python}
import torch

torch.manual_seed(42)

a = torch.randint(low=0, high=10, size=(2, 3, 4, 5))

```

The rank of a tensor is accessible in PyTorch as the `ndim` attribute, and the shape of a tensor is accessible as the `shape` attribute:

```{python}
print("The rank of the tensor is:", a.ndim)
print("The shape of the tensor is:", a.shape)
```

The inner product on tensors spaces is implemented in PyTorch as the `torch.tensordot` function. For example, the following code block generates two random rank-$2$ tensors of shapce $(2,3)$, and then forms their inner product:

```{python}
a = torch.randint(low=0, high=10, size=(2, 3))
b = torch.randint(low=0, high=10, size=(2, 3))
inner_product = torch.tensordot(a, b, dims=a.ndim)

print("The first tensor is:\n", a)
print("The second tensor is:\n", b)
print("The inner product of the two tensors is:", inner_product)
```

Just to check, let's compute the inner product manually, and see if it mathches the result from `torch.tensordot`:

```{python}
from itertools import product

I = product(range(2), range(3))
manual_inner_product = sum(a[i, j] * b[i, j] for i, j in I)

print("The manually computed inner product is:", manual_inner_product)
```

The reader may have noticed the parameter `dims=a.ndim` in the call to `torch.tensordot`. Its usage is explained well in the [PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.tensordot.html){target="_blank"}: If $a$ and $b$ are tensors of rank $m$ and $n$, respectively, then `torch.tensordot(a, b, dims=d)` produces a tensor $r$ of rank $m+n-2d$ by summing over the last $d$ indices of $a$ and the first $d$ indices of $b$:

$$
r_{i_0,\ldots,i_{m+n-2d-1}} = \sum_{k_0,\ldots,k_{d-1}} a_{i_0,i_1,\ldots,i_{m-d-1},k_0,k_1,\ldots,k_{d-1}} b_{k_0,k_1,\ldots,k_{d-1},i_{m-d},\ldots,i_{m+n-2d-1}}.
$$
[Of course, in order for this to be well-defined, the last $d$ indices of $a$ and the first $d$ indices of $b$ must have the same range of values.]{.aside}

To produce the inner product of $a$ and $b$, as specified above, we must sum the tensors over *all* their indices, which explains the use of `dims=a.ndim` in our call to `torch.tensordot`. In this case, $m=n=2$ and $d=2$, so that the rank of the resulting tensor is $m+n-2d = 0$, i.e., a scalar.

An even more flexible and general way to compute tensors of the form $r$ produced by the `torch.tensordot` function described above is to use the `torch.einsum` function, which implements a form of [Einstein summation notation](https://en.wikipedia.org/wiki/Einstein_notation){target="_blank"}.

For a simple example, let's suppose that we have two tensors of the form

$$
a_{ijk} \quad \text{and} \quad b_{pqrs},
$$

by which we mean that $a$ is rank-$3$ and $b$ is rank-$4$. We suppose further that the range of values of $i$ matches the range of values of $q$, and the range of values of $j$ matches the range of values of $s$. For example, the following code block produces two random integer-valued tensors with these properties:

```{python}
# The following index ranges match: i ↔ q and j ↔ s
a = torch.randint(low=0, high=10, size=(5, 3, 4))  # indices = i,j,k
b = torch.randint(low=0, high=10, size=(2, 5, 4, 3))  # indices = p,q,r,s
```

In this case, we may compute the rank-$3$ tensor $u$ with entries

$$
u_{kpr} = \sum_{i,j} a_{ijk}b_{pirj}.
$$

```{python}

einsum_result = torch.einsum("ijk,pirj", a, b)
print("The result of the Einstein summation is:\n", einsum_result)
```


## Derivatives

The general type of derivative that I define below for a function $f: \calh \to \calk$ between two normed vector spaces relies on a special case of the derivative of a function of the form $g: \bbr \to \calk$. But this latter type of derivative is defined exactly how one would expect, given by the following limit:

$$
g'(t) = \lim_{h \to 0} \frac{g(t+h) - g(t)}{h}.
$$ {#eq-t-derivative}

If the limit exists, then $g$ is said to be *differentiable* at $t$. Even though this is the same definition as we saw in our first calculus course, note that the numerator of the difference quotient is a vector in $\calk$.

The following convenient theorem allows us to compute derivatives of certain special types of functions $g: \bbr \to \calk$ using familiar rules from single-variable calculus:

::: {#thm-der-rules}
Let $g: \bbr \to \calk$ be a function of the form

$$
g(t) = w_0 + w_1 f_1(t) + \cdots + w_n f_n(t),
$$

where the $f_i: \bbr \to \bbr$ are real-valued, differentiable functions (in the usual sense), and the $w_i \in \calk$ are fixed vectors. Then $g$ is differentiable, and its derivative is given by

$$
g'(t) = w_1 f_1'(t) + \cdots + w_n f_n'(t),
$$

for all $t\in \bbr$.
:::

For example, suppose that $g: \bbr \to \calk$ is a polynomial function with coefficients in $\calk$:

$$
g(t) = w_0 + w_1 t + w_2 t^2 + \cdots + w_n t^n,
$$

where $w_0, w_1, \ldots, w_n \in \calk$ are fixed vectors. Then the theorem tells us that $g$ is differentiable, and that its derivative is given by the familiar formula

$$
g'(t) = w_1 + 2 w_2 t + \cdots + n w_n t^{n-1}.
$$

Returning to $f$, our goal is to understand how the value of the function $f(x)$ changes on small changes in the input $x \in \calh$. To this end, we consider a small perturbation of $x$ in the direction of a vector $v \in \calh$, which is given by $x + tv$ for small values of $t \in \bbr$. We then have the function

$$
g: \bbr \to \calk, \quad g(t) = f(x + tv),
$$

which is of the type that we just discussed. As the limit of a difference quotient, its derivative at $t=0$ (if it exists) looks like

$$
g'(0) = \lim_{h\to 0} \frac{f(x + hv) - f(x)}{h}.
$$

This leads us to:

::: {#def-derivative}
Let $f: \calh \to \calk$ be a function between two normed vector spaces. The *derivative of $f$ at a point $x \in \calh$, in the direction $v \in \calh$*, is defined as

$$
f'(x)(v) = \frac{d}{dt} f(x + tv)\Big|_{t=0},
$$ {#eq-derivative}
provided the $t$-derivative exists. If the derivative exists for all $x,v \in \mathcal{H}$, we say that $f$ is *differentiable*.
:::

Very often, you will see the $t$-derivative in the definition of $f'(x)(v)$ expressed as a limit of a difference quotient:

$$
\frac{d}{dt} f(x + tv)\Big|_{t=0} = \lim_{t\to 0} \frac{f(x + tv) - f(x)}{t}.
$$

This can be somewhat confusing, however, because the variable $t$ is playing different roles on the two sides of the equation.

Strictly speaking, the type of differentiability I've defined here is called [*Gâteaux differentiability*](https://en.wikipedia.org/wiki/Gateaux_derivative){target="_blank"}, and it is a weaker condition than the more commonly used notion of [*Fréchet differentiability*](https://en.wikipedia.org/wiki/Fr%C3%A9chet_derivative){target="_blank"}. Roughly speaking, a function is Fréchet differentiable at a point if it can be well approximated by a linear function in a neighborhood of that point, whereas Gâteaux differentiability only requires the existence of directional derivatives. There are some regularity conditions under which Gâteaux differentiability implies Fréchet differentiability, but I will mostly ignore them. For my purposes, I will assume that all differentiable functions in the above sense are also Fréchet differentiable, and so the derivative of a differentiable function yields a linear map $f'(x): \mathcal{H} \to \mathcal{K}$ with $v \mapsto f'(x)(v)$.

The reader may have noticed a potential ambiguity: If $g: \bbr\to \calk$ is a function, then it has a derivative *vector* $g'(t)\in \calk$ computed according to ([-@eq-t-derivative]), but it also has a derivative *function* $g'(t):\mathbb{R} \to \calk$ computed according to ([-@eq-derivative]) (provided that the derivatives exist, of course). What's the relationship between these two derivatives? The answer is given by the equation

$$
g'(t)(v) = g'(t)v,
$$

where the $g'(t)$ on the left side of the equation is the *function* defined by ([-@eq-derivative]), while the $g'(t)$ on the right side is the *vector* computed by ([-@eq-t-derivative]). This is a very easy consequence of the chain rule that we will study later, but it can also be proved directly in the standard analytic style using $\epsilon$-$\delta$ arguments. Admittedly, having a single piece of notation standing for two different things is a bit confusing. I hope the reader will not be too bothered by it, and will be able to discern from context which meaning is intended when there's possibility of ambiguity.

## Example: Least-squares

A concrete example is in order. Let $A$ be a fixed $m\times n$ matrix, $b\in \bbr^m$ a fixed vector, and consider the function

$$
f: \bbr^n \to \bbr, \quad f(x) = \|Ax - b\|^2.
$$

This function, of course, is extremely relevant in applications, since its minimizers are the solutions to the least-squares problem $Ax = b$. We will compute the derivative $f'(x)$, where the domain $\bbr^n$ is equipped with the standard inner product $\langle x, y \rangle = x^\top y$ and induced norm. The first step is to rewrite

$$
f(x) = \langle Ax - b, Ax - b \rangle = (x^\top A^\top - b^\top)(Ax - b) = x^\top A^\top Ax - 2b^\top Ax + b^\top b.
$$ {#eq-fx-expansion}

Then, for fixed $x,v\in \bbr^n$, we define the function $g: \bbr \to \bbr$ by setting

$$
g(t) = f(x + tv) = (x^\top + tv^\top)A^\top A(x + tv) - 2b^\top A(x + tv) + b^\top b.
$$

Expanding the right-hand side, we have

$$
g(t) = (x^\top A^\top Ax- 2b^\top Ax  + b^\top b) + 2v^\top( A^\top Ax - A^\top b)t + (v^\top A^\top Av)t^2,
$$ {#eq-gx-expansion}

which reveals $g$ as a quadratic polynomial in the variable $t$ with real coefficients. Its derivative is

$$
g'(t) = 2v^\top(A^\top Ax-  A^\top b) + 2 (v^\top A^\top Av) t,
$$
[The reader might have noticed that in the expression ([-@eq-fx-expansion]), $f(x)$ appears as a *product* of the two terms $(x^\top A^\top - b^\top)$ and $(Ax - b)$. The reader may well have wondered if there was some sort of product rule for derivatives which would be relevant here. This is indeed the case, and we leave the interested reader to formulate it, prove it, and apply it on their own.]{.aside}

which gives us

$$
f'(x)(v) = g'(0) = 2v^\top(A^\top Ax-  A^\top b).
$$

The condition for stationarity of $f$ at a point $x$ is that its derivative $f'(x)(v)$ vanishes identically in $v$, which is equivalent to the condition that $A^\top Ax - A^\top b = 0$, i.e., that $x$ is a solution to the familiar normal equations $A^\top Ax = A^\top b$.

As the reader might imagine, the coefficient $v^\top A^\top Av$ of the $t^2$-term in the expansion ([-@eq-gx-expansion]) of $g(t)$ is related to the second derivative of $f$. We will explore this below.

## Partial derivatives and Jacobians

For the applications I have in mind, it is always the case that the normed vector spaces $\calh$ and $\calk$ are finite-dimensional Hilbert spaces equipped with fixed choices of orthonormal bases. If $\{\epsilon_i\}$ is such a basis for $\mathcal{K}$, and if $f: \mathcal{H} \to \mathcal{K}$ is a function, then we have real-valued *component functions*

$$
f_i: \mathcal{H} \to \mathbb{R}, \quad f_i(x) = \langle f(x), \epsilon_i \rangle,
$$

one for each basis vector $\epsilon_i$. One may show that if $f$ is differentiable, then so too are each of the component functions $f_i$, and moreover, we have the fundamental link between the derivative of $f$ and the derivatives of its component functions:

$$
f_i'(x)(v) = \langle f'(x)(v), \epsilon_i \rangle,
$$ {#eq-component-derivative}

for all $x, v \in \mathcal{H}$ and all $i$. This leads us to the following central definition:

::: {#def-partial-derivative}
Let $f: \mathcal{H} \to \mathcal{K}$ be a differentiable function between two finite-dimensional Hilbert spaces, and let $\{e_j\}$ and $\{\epsilon_i\}$ be orthonormal bases for $\mathcal{H}$ and $\mathcal{K}$, respectively. The *partial derivative* of the $i$-th component function $f_i$ at a point $x \in \mathcal{H}$ with respect to the basis vector $e_j$ is defined as
$$
\frac{\partial f_i}{\partial e_j}(x) = f_i'(x)(e_j).
$$
:::

Invoking equation ([-@eq-component-derivative]), we see that the partial derivative can be equivalently expressed as

$$
\frac{\partial f_i}{\partial e_j}(x) = \left\langle f'(x)(e_j), \epsilon_i \right\rangle.
$$

In the case that $f$ is real-valued, so that $\mathcal{K} = \mathbb{R}$, we will *always* take the basis of $\mathbb{R}$ to be $\{1\}$, and so the partial derivatives of a real-valued function $f$ at a point $x$ with respect to the basis vectors $\{e_j\}$ of $\mathcal{H}$ are given by

$$
\frac{\partial f}{\partial e_j}(x) = \langle f'(x)(e_j), 1 \rangle = f'(x)(e_j).
$$

The importance of the partial derivatives is that they are the entries in matrix representations of derivatives:

::: {#def-jacobian}
Let $f: \mathcal{H} \to \mathcal{K}$ be a differentiable function between two finite-dimensional Hilbert spaces, and let $\{e_j\}$ and $\{\epsilon_i\}$ be orthonormal bases for $\mathcal{H}$ and $\mathcal{K}$, respectively. The *Jacobian matrix* of $f$ at a point $x \in \mathcal{H}$ is the matrix
$$
Jf(x)_{ij} = \frac{\partial f_i}{\partial e_j}(x).
$$
:::

One must take care in interpreting a Jacobian "matrix" as a matrix in the usual way, since the index sets for the bases $\{e_j\}$ and $\{\epsilon_i\}$ may be more complicated than just $\{1, \ldots, n\}$ and $\{1, \ldots, m\}$, as is the case when $\mathcal{H}$ and $\mathcal{K}$ are tensor spaces like $\mathbb{R}^{m\times n}$ or $\mathbb{R}^{m\times n \times p}$.

The sense in which the Jacobian matrix $Jf(x)$ represents the derivative $f'(x)$ may be described as follows. Suppose given a vector $v\in \mathcal{H}$, and perform a Fourier expansion relative to the basis $\{e_j\}$:

$$
v = \sum_j \langle v, e_j \rangle e_j = \sum_j v_j e_j,
$$

where $v_j = \langle v, e_j \rangle$. Then, for each $j$, we also have the Fourier expansion of $f'(x)(e_j)$ relative to the basis $\{\epsilon_i\}$:

$$
f'(x)(e_j) = \sum_i \langle f'(x)(e_j), \epsilon_i \rangle \epsilon_i = \sum_i \frac{\partial f_i}{\partial e_j}(x) \epsilon_i.
$$

Thus:

$$
f'(x)(v) = \sum_j v_j f'(x)(e_j) = \sum_i \left[ \sum_j v_j \frac{\partial f_i}{\partial e_j}(x) \right] \epsilon_i.
$$ {#eq-jacobian-representation}

The reader will notice the familiar matrix multiplication pattern in the square brackets. If, in particular, the indices $i$ and $j$ range over $\{1, \ldots, m\}$ and $\{1, \ldots, n\}$, respectively, then the Jacobian matrix $Jf(x)$ is an $m\times n$ matrix, and the expression in the square brackets is just the $i$-th entry of the column vector

$$
\begin{bmatrix}
\sum_j v_j \frac{\partial f_1}{\partial e_j}(x) \\
\vdots \\
\sum_j v_j \frac{\partial f_m}{\partial e_j}(x)
\end{bmatrix}=\begin{bmatrix} \frac{\partial f_1}{\partial e_1}(x) & \cdots & \frac{\partial f_1}{\partial e_n}(x) \\ \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial e_1}(x) & \cdots & \frac{\partial f_m}{\partial e_n}(x) \end{bmatrix} \begin{bmatrix} v_1 \\ \vdots \\ v_n \end{bmatrix}.
$$

## Dual spaces and gradients

With derivatives and Jacobians in hand, we now move toward defining the gradient, which requires the Riesz Representation Theorem. To state it, however, we need to first introduce the notion of the *dual* of a (finite-dimensional) Hilbert space. This is the vector space $\mathcal{H}^*$ of all linear functionals $\phi: \mathcal{H} \to \mathbb{R}$, where a *linear functional* is a linear map from $\mathcal{H}$ to $\mathbb{R}$. Its vector space structure is given by pointwise addition and scalar multiplication, so that for $\phi, \psi \in \mathcal{H}^*$ and $a\in \mathbb{R}$, we have that

$$
(\phi + \psi)(v) = \phi(v) + \psi(v)
$$

and

$$(a\phi)(v) = a\phi(v)
$$

for all $v \in \mathcal{H}$. In fact, these definitions work for *any* vector space, not just Hilbert spaces. But the interest in Hilbert spaces is that they have a built-in mechanism for producing linear functionals from vectors. Indeed, for each $v \in \mathcal{H}$, we can define a linear functional

$$
v^\ast: \mathcal{H} \to \mathbb{R}, \quad v^\ast(w) = \langle v, w \rangle,
$$

which is sometimes called the *covector* associated to $v$. Linearity of the inner product in the second argument ensures that the covector is a linear functional, and so $v^\ast \in \mathcal{H}^*$. Moreover, the mapping $v \mapsto v^\ast$ is a linear map from $\mathcal{H}$ to $\mathcal{H}^*$, in the sense that it preserves vector addition and scalar multiplication.

The Riesz Representation Theorem states that *every* linear functional on a finite-dimensional Hilbert space arises as an inner product against a unique vector, called its *Riesz representation*:

::: {#thm-riesz}
### Riesz Representation Theorem — finite-dimensional version
Let $\mathcal{H}$ be a finite-dimensional Hilbert space. For every linear functional $\phi: \mathcal{H} \to \mathbb{R}$, there exists a unique vector $v_\phi \in \mathcal{H}$ such that $\phi = v_\phi^\ast$. In particular, the mapping $v\mapsto v^\ast$ is a linear isomorphism from $\mathcal{H}$ to $\mathcal{H}^*$.
:::

Using the Riesz isomorphism from $\mathcal{H}$ to $\mathcal{H}^\ast$, we can transport the inner product from $\mathcal{H}$ to $\mathcal{H}^*$, and thus equip $\mathcal{H}^*$ with the structure of a Hilbert space. In particular, for $\phi, \psi \in \mathcal{H}^*$, we can define their inner product as

$$
\langle \phi, \psi \rangle = \langle v_\phi, v_\psi \rangle.
$$

When the dual space $\mathcal{H}^\ast$ is equipped with this inner product, we call it the *dual Hilbert space* of $\mathcal{H}$. The Riesz isomorphism is then an *isometric* isomorphism between the Hilbert space $\mathcal{H}$ and its dual Hilbert space $\mathcal{H}^*$.

We have the mapping $(-)^\ast: \mathcal{H} \to \mathcal{H}^*$ that acts on vectors by sending $v$ to the covector $v^\ast$. A similar mapping acts on a linear map $T: \mathcal{H} \to \mathcal{K}$, where $\mathcal{H}$ and $\mathcal{K}$ are finite-dimensional Hilbert spaces. This is the *adjoint* of $T$, denoted by $T^\ast: \mathcal{K} \to \mathcal{H}$, and it is defined as the unique linear map such that

$$
\langle T(v), w \rangle = \langle v, T^\ast(w) \rangle,
$$

for all $v\in \mathcal{H}$ and $w \in \mathcal{K}$. Notice, in particular, that if we have orthonormal bases $\{e_j\}$ and $\{\epsilon_i\}$ for $\mathcal{H}$ and $\mathcal{K}$, respectively, then

$$
\langle T(e_j), \epsilon_i \rangle = \langle e_j, T^\ast(\epsilon_i) \rangle,
$$

so that the $ij$-th entry of the "matrix" representation of $T$ relative to these bases is equal to the $ji$-th entry of the "matrix" representation of $T^\ast$ relative to the same bases. Again, we must take care in interpreting these "matrices" as matrices in the usual way—just as above when we discussed Jacobian matrices—since the index sets for the bases $\{e_j\}$ and $\{\epsilon_i\}$ may be more complicated than just $\{1, \ldots, n\}$ and $\{1, \ldots, m\}$.

We now obtain the *gradient* by smacking the derivative with the Riesz theorem:

::: {#def-gradient}
Let $\mathcal{H}$ be a finite-dimensional Hilbert space, and let $f: \mathcal{H} \to \mathbb{R}$ be a differentiable function. The *gradient* of $f$ at a point $x\in \mathcal{H}$ is the Riesz representation of the linear functional $f'(x)\in \mathcal{H}^\ast$, denoted by $\nabla f(x) \in \mathcal{H}$. In particular, we have that
$$
f'(x)(v) = \left\langle \nabla f(x), v \right\rangle,
$$
for all $v \in \mathcal{H}$.
:::

Do notice that the gradient is only defined for real-valued functions, not functions with values in a general Hilbert space.

Existence and uniqueness of the gradient follow from the general [Riesz representation theorem](https://en.wikipedia.org/wiki/Riesz_representation_theorem){target="_blank"}, though in the present finite-dimensional setting, both can be demonstrated easily by direct construction. Indeed, if $\{e_j\}$ is an orthonormal basis for $\mathcal{H}$, and if we *assume* that $\nabla f(x)$ exists with the stated property, then for each $e_j$, we have that

$$
\nabla f(x) = \sum_j \langle \nabla f(x), e_j \rangle e_j = \sum_j f'(x)(e_j) e_j,
$$

which shows that the gradient is unique if it exists. To *prove* that it exists, we can simply define $\nabla f(x)$ to be the vector given by the right-hand side of the above equation, and then verify that it satisfies the required property. (By the way, this argument is essentially the standard proof of the Riesz representation theorem as stated above.) Furthermore, since $f'(x)(e_j) = \frac{\partial f}{\partial e_j}(x)$, the Fourier expansion of the gradient relative to the basis $\{e_j\}$ is given by

$$
\nabla f(x) = \sum_j \frac{\partial f}{\partial e_j}(x) e_j,
$$

which should be familiar to anyone who has taken a course in multivariable calculus.

One of the most useful properties of the gradient is that it "points" in the direction of greatest increase of the function. This is made precise in the following result:

::: {#thm-gradient-max-increase}
Let $\mathcal{H}$ be a finite-dimensional Hilbert space, let $f: \mathcal{H} \to \mathbb{R}$ be a differentiable function, and suppose the gradient $\nabla f(x)$ is nonzero at a point $x\in \mathcal{H}$. Suppose we consider the value of the derivative $f'(x)(v)$ as a function of unit vectors $v \in \mathcal{H}$. Then:[Note that this statement of the extremizing property of the gradient makes no reference to any "angles" between vectors, that are often invoked in the usual geometric explanation of this property.]{.aside}

1. The derivative is maximized at $v = \nabla f(x)/\|\nabla f(x)\|$, and the maximum value is $\|\nabla f(x)\|$.

2. The derivative is minimized at $v = -\nabla f(x)/\|\nabla f(x)\|$, and the minimum value is $-\|\nabla f(x)\|$.
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
The proof uses the [Cauchy-Schwarz inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality){target="_blank"}, which states that for all $v, w \in \mathcal{H}$, we have that
$$
|\langle v, w \rangle| \leq \|v\| \|w\|,
$$

with equality if and only if $v$ and $w$ are linearly dependent. Applying this to the inner product $\langle \nabla f(x), v \rangle$, where $v$ is a unit vector, we get that

$$
|f'(x)(v)| = |\langle \nabla f(x), v \rangle| \leq \|\nabla f(x)\| \|v\| = \|\nabla f(x)\|,
$$

which may be written as the compound inequality

$$
- \|\nabla f(x)\| \leq f'(x)(v) \leq \|\nabla f(x)\|,
$$

with equality at one of the two ends if and only if $v$ is a scalar multiple of $\nabla f(x)$. It remains only to note that

$$
f'(x)(\pm \nabla f(x)) = \langle \nabla f(x), \pm\nabla f(x) \rangle = \pm \|\nabla f(x)\|^2,
$$

which finishes the proof.
:::

The gradient may be obtained through the adjoint action of the derivative, as stated in the following important result:

::: {#thm-gradient-adjoint}
Let $\mathcal{H}$ be a finite-dimensional Hilbert space, and let $f: \mathcal{H} \to \mathbb{R}$ be a differentiable function. Then the gradient of $f$ at a point $x\in \mathcal{H}$ is given by the adjoint of the derivative of $f$ at $x$ applied to the scalar $1$:
$$
\nabla f(x) = f'(x)^\ast(1).
$$
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
The proof is a one-liner. For any $v \in \mathcal{H}$, we first observe that

$$
f'(x)(v) = \langle 1, f'(x)(v) \rangle =  \langle f'(x)^\ast(1), v \rangle,
$$

and then we invoke the uniqueness of the Riesz representation to conclude that $\nabla f(x) = f'(x)^\ast(1)$.
:::

We work toward ending this long section by studying how the adjoint and derivative operations interact with function composition. Fortunately, both stories could not be simpler. For the adjoint, we have that

$$
(S\circ T)^\ast = T^\ast \circ S^\ast,
$$

for linear maps $\mathcal{H} \xrightarrow{T} \mathcal{K} \xrightarrow{S} \mathcal{L}$ on finite-dimensional Hilbert spaces. The proof uses the uniqueness of the adjoint. For any $v \in \mathcal{H}$ and $w \in \mathcal{L}$, we have that

$$
\langle (S\circ T)(v), w \rangle = \langle S(T(v)), w \rangle = \langle T(v), S^\ast(w) \rangle = \langle v, T^\ast(S^\ast(w)) \rangle,
$$

so that $(S\circ T)^\ast(w) = T^\ast(S^\ast(w))$, and thus $(S\circ T)^\ast = T^\ast \circ S^\ast$.

## The chain rule

For derivatives and composition, we have:

::: {#thm-chain-rule}
### Chain Rule
Let $\mathcal{H} \xrightarrow{f} \mathcal{K} \xrightarrow{g} \mathcal{L}$ be differentiable functions on finite-dimensional Hilbert spaces. Then the composition $g\circ f: \mathcal{H} \to \mathcal{L}$ is differentiable, and its derivative at a point $x\in \mathcal{H}$ is given by the composition of the derivatives of $f$ and $g$ at the appropriate points: [We do not prove the Chain Rule here. The interested reader may consult Section 2.3 of @Coleman2012.]{.aside}
$$
(g\circ f)'(x) = g'(f(x)) \circ f'(x).
$$
:::

## Higher-order derivatives and Hessian tensors

Suppose that $f: \calh \to \calk$ is a differentiable function between two normed vector spaces. Given points $x,u\in \calh$, we have the derivative $f'(x)(u) \in \calk$. We may wonder how this value changes as we vary $x$, which leads us to consider the function

$$
t\mapsto f'(x+tv)(u)
$$

of the real variable $t$, where $v$ is a second direction vector in $\calh$ along with the first direction vector $u$. If this function were differentiable, then we could take its derivative at $t=0$, resulting in

$$
\frac{d}{dt} f'(x+tv)(u)\Big|_{t=0} = \frac{d}{dt} \left[ \frac{d}{ds} f(x + su + tv)\Big|_{s=0} \right]\Big|_{t=0} = \frac{\partial^2}{\partial t \partial s} f(x + su + tv)\Big|_{s=t=0}.
$$

This inspires the following general definition:

::: {#def-second-derivative}

Let $f: \calh \to \calk$ be a function between two normed vector spaces. The *second derivative of $f$ at a point $\in \calh$, in the directions of $u,v\in \calh$,* is defined as

$$
f''(x)(u,v) = \frac{\partial^2}{\partial t \partial s} f(x + su + tv)\Big|_{s=t=0},
$$

provided that the partial derivatives exist. If the second derivative exists for all $x,u,v\in \calh$, we say that $f$ is *twice differentiable*.
:::

Just like in the case of the first derivative, nothing in the definition guarantees that the dependence of the second derivative on the direction vectors $v$ and $u$ has any sort of desirable properties. What I have defined here is called the [*second-order Gâteaux derivative*](https://en.wikipedia.org/wiki/Gateaux_derivative#Higher_derivatives){target="_blank"}, which is the weaker of the two common notions of second-order differentiability. In the presence of some regularity conditions—that I mostly ignore—the second-order Gâteaux derivative is equivalent to the stronger notion of the [*second-order Fréchet derivative*](https://en.wikipedia.org/wiki/Fr%C3%A9chet_derivative#Higher_derivatives){target="_blank"}. In particular, if $f$ is twice Fréchet differentiable, then the second derivative $f''(x)$ is a symmetric bilinear map from $\calh \times \calh$ to $\calk$. I shall sidestep the technicalities of these regularity conditions, and simply assume that all our functions are twice Fréchet differentiable, so that the second derivative as defined above is a symmetric bilinear map $f''(x): \calh \times \calh \to \calk$ with $(u,v) \mapsto f''(x)(u,v)$.


## Enter PyTorch: Computational examples of derivatives

Wowza. A hearty slap on the back for ya', if you endured the previous section and have made it this far. That was all very abstract, very theoretical, and very *mathematical* (some of you might not view that last adjective as a compliment). But while it's important for a person to have a solid grasp on the theory, it's pretty useless if they cannot apply it in practice. In this section, I'll show you how all that theory can be beautifully illuminated by working through concrete examples using PyTorch.

Let's consider a differentiable function
$$
f: \bbr^{m\times n} \to \bbr^{p\times q}.
$$

Given two vectors $x$ and $v$ in $\mathbb{R}^{m\times n}$, we can express the vector $f'(x)(v)\in \bbr^{p\times q}$ as a Fourier expansion relative to the standard orthonormal basis vectors $\epsilon_{ij}\in \mathbb{R}^{p\times q}$ identified in the previous section:

$$
f'(x)(v) = \sum_{i,j} \left\langle f'(x)(v), \epsilon_{ij} \right\rangle \epsilon_{ij}.  \tag*{$\begin{Bmatrix} 0\leq i < p \\ 0 \leq j < q \end{Bmatrix}$}
$$

Similarly, we can write

$$
v = \sum_{k,l} \left\langle v, e_{kl} \right\rangle e_{kl} = \sum_{k,l} v_{kl} e_{kl}, \tag*{$\begin{Bmatrix} 0\leq k < m \\ 0 \leq l < n \end{Bmatrix}$}
$$

where $e_{kl}$ are the standard orthonormal basis vectors of $\mathbb{R}^{m\times n}$ and $v_{kl} = \langle v, e_{kl} \rangle$. Substituting this into the expression for $f'(x)(v)$ and using linearity, we get that:

$$
f'(x)(v) = \sum_{k,l} v_{kl} f'(x)(e_{kl}). \tag*{$\begin{Bmatrix} 0\leq k < m \\ 0 \leq l < n \end{Bmatrix}$}
$$

Therefore, we can write

$$
f'(x)(v) = \sum_{i,j} \left[ \sum_{k,l}  v_{kl} \left\langle f'(x)(e_{kl}), \epsilon_{ij} \right\rangle \right]\epsilon_{ij}. \tag*{$\begin{Bmatrix} 0\leq i < p \\ 0 \leq j < q \\ 0 \leq k < m \\ 0 \leq l < n \end{Bmatrix}$}
$$

But

$$
\left\langle f'(x)(e_{kl}), \epsilon_{ij} \right\rangle = \frac{\partial f_{ij}}{\partial x_{kl}}(x),
$$

and so this last equation becomes

$$
f'(x)(v) = \sum_{i,j} \left[ \sum_{k,l}  v_{kl} \frac{\partial f_{ij}}{\partial x_{kl}}(x) \right]\epsilon_{ij}.  \tag*{$\begin{Bmatrix} 0\leq i < p \\ 0 \leq j < q \\ 0 \leq k < m \\ 0 \leq l < n \end{Bmatrix}$}
$$
