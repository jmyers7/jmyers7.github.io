---
title: "Diffusion"
toc: true
draft: true
jupyter: blog-env
categories: [Stochastic processes, Random walk, Probability theory, Martingale, SigAlg, Python]
include-in-header: 
  - file: /Users/johnmyers/dev/johnmyers-phd/aux-files/commands.tex
bibliography: /Users/johnmyers/dev/johnmyers-phd/aux-files/references.bib
---



## Time-evolution of transition probabilities: diffusion

Imagine that, at time $0$, you attempt to predict the future value $X_t$ of a random walk at some future time $t$. From the plots of the trajectories immediately above, it is evident that your prediction will *always* be uncertain, and that this uncertainty grows larger as $t$ increases. The probability distribution that describes this uncertainty is exactly the law of the random variable $X_t$, which we might write as

$$
P(X_t = x \mid X_0 = a)
$$

to emphasize that the random walk starts at $X_0 = a$. When written like this, the law is understood as a *transition probability distribution*, since it gives the probabilities of transitioning from the state $X_0 = a$ at time $0$ to the various possible states $X_t=x$ at time $t$. The fact that this probability distribution "spreads out" as $t$ increases is known as *diffusion*, and it is a fundamental property of random walks and many other stochastic processes.

In fact, if the random walk has probability parameter $p$, then we have

$$
V(\Delta X_t) = V(2B_t - 1) = 4pq,
$$ 

where we set $q=1-p$, and where $B$ is the IID Bernoulli process used to construct the increments in the previous section. Since the increments are independent, it then follows that

$$
V(X_t) = \sum_{s=1}^t V(\Delta X_s) = 4tpq.
$$ {#eq-variance-random-walk}

Thus, the variances of a random walk grow linearly as a function of time $t$. As for the expectations, we have
$$
E(X_t) = X_0 + \sum_{s=1}^t E(\Delta X_s) = X_0 + t(p-q),
$$ {#eq-expectation-random-walk}

so the expectations also grow linearly as a function of time $t$.

These observations, along with the plots in the previous section, suggest that the time-evolution of the law of $X_t$ consists of a "peak" that moves linearly with time (according to the expectation) while simultaneously diffusing (according to the variance).

This evolution may be observed experimentally in SigAlg, as shown in the following plot. We simulate 10,000 trajectories of a random walk with $p=0.7$ and then plots histograms of the values of $X_t$ for various $t$ in a "ridgeline" plot:

```{python}
# | fig-align: center
# | code-fold: true

from matplotlib.colors import LinearSegmentedColormap

blue = "#3399FF"
purple = "#AA77CC"

T = Time.discrete(length=200)

X = RandomWalk(
    p=0.7,
    name="X",
    time=T,
).from_simulation(
    n_trajectories=10_000,
    random_state=42,
)

_, ax = plt.subplots(figsize=(7, 5))

n_plots = 8
time_step = 25  # 200 = 8 * 25

times = [time_step * k for k in range(1, n_plots + 1)]
cmap = LinearSegmentedColormap.from_list("conditional_cmap", [yellow, purple, blue])
colors = [cmap(i / (n_plots - 1)) for i in range(n_plots)]

for color, t in zip(colors, times):
    probabilities = X[t - 1].range.probability_measure.data
    probabilities.index = X[t - 1].range.data.values

    ax.bar(
        x=probabilities.index,
        height=-probabilities.values * 175,
        width=0.8,
        color=color,
        bottom=t,
    )

ax.invert_yaxis()
ax.set_xlabel("state")
ax.set_ylabel("time")
ax.set_title(
    "Time-evolution of the probability distribution\nof a random walk",
)
plt.tight_layout()
plt.show()

```

As we expected, the peak of the distribution drifts toward higher values as time increases, while simultaneously diffusing.

It is not difficult to write down a formula for the last of $X_t$; in fact, we accomplish this in [@thm-explicit-discrete-law]. However, it is also possible to describe the time-evolution of the law of $X_t$ using a special *difference equation*, which is the discrete starting point for the Fokker-Planck partial differential equation that we will study later.

::: {#thm-discrete-diffusion-equation}
## Discrete Diffusion Equation

Let $X$ be a random walk with probability parameter $p$, starting at position $a$, and write $q=1-p$. If we write

$$
f(x,t) = P(X_t = x)
$$

for the probability function of $X_t$, then for all integers $t \geq 0$ and all $x \in \mathbb{Z}$, we have
$$
f(x,t+1) - f(x,t) = p \big( f(x-1,t) - f(x,t) \big) + q \big( f(x+1,t) - f(x,t) \big),
$$

along with the initial conditions
$$
f(a, 0) = 1, \quad f(x,0) = 0 \text{ for } x \neq a.
$$
:::

::: {.callout-note collapse="true" icon="false"}
## Proof.
From the law of total probability, we have

$$
P(X_{t+1}=x) = \sum_{y\in \mathbb{Z}} P(X_{t+1}=x \mid X_t = y) P(X_t = y).
$$

However, the conditional probabilities on the right-hand vanish except when $y = x-1$ or $y = x+1$, since the random walk can only move one unit to the left or right at each time step. Thus, we have

$$
f(x,t+1) = p f(x-1,t) + q f(x+1,t).
$$

Then, if we subtract $f(x,t)$ from both sides and use that $p+q=1$, we get

$$
f(x,t+1) - f(x,t) = p \big( f(x-1,t) - f(x,t) \big) + q \big( f(x+1,t) - f(x,t) \big),
$$

as claimed.
:::

Thus, the (forward) time increment of the probability function $f(x,t)$ is given by a weighted sum of the spatial increments to the left and right, with weights given by the probabilities $p$ and $q$. We will see this same sort of structure in the Fokker-Planck equation, where a time derivative is equal to an expression involving spatial derivatives.




::: {#thm-explicit-discrete-law}
## Law of Random Walk
Let $X$ be a random walk with probability parameter $p$, starting position $a$, and write $q=1-p$. Then the probability function $f(x,t)$ of $X_t$ is given by the formula

$$
f(x,t) = \binom{t}{\frac{x-a+t}{2}} p^{\frac{x-a+t}{2}} q^{\frac{t-x+a}{2}},
$$

for all integers $t\geq 0$ and all

$$
x \in \{ a-t, a-t+2, a-t+4, \ldots, a+t \}.
$$
:::

::: {.callout-note collapse="true" icon="false"}
## Proof.

For each integer $t\geq 1$, we have that

$$
X_t = a + \sum_{s=1}^t \Delta X_s = a + \sum_{s=1}^t (2B_s - 1) = a-t + 2Y,
$$

where $Y$ is a binomial random variable with parameters $t$ and $p$. Then $X_t=x$ if and only if $Y = \frac{x - a + t}{2}$, so the result follows from the probability function of the binomial distribution.
:::

## Scaled random walks

We now construct a scaled random walk. First, we fix a position $x\in \bbr$ and a time $t>0$. We then have a spatial interval from the starting position $a$ to the position $x$, of length $|x - a|$, and also a time interval from $0$ to $t$, of length $t$. We chop these intervals into $m$ and $n$ subintervals of equal lengths

$$
h = \frac{|x - a|}{m} \quad \text{and} \quad \tau = \frac{t}{n},
$$ {#eq-step-sizes}

respectively. Note that as $m,n\to \infty$, the step sizes $h$ and $\tau$ both shrink to zero. The goal is to understand what happens to as these step sizes vanish.

For each integer $k=1,2,\ldots,n$, we define the increments

$$
\Delta X_{k\tau}^{(h,\tau)} = h(2B_k - 1),
$$

so that each increment is a random variable that takes the value $h$ with probability $p$ and the value $-h$ with probability $1-p$. Then, we define

$$
X_t^{(h,\tau)} = a + \sum_{k=1}^n \Delta X_{k\tau}^{(h,\tau)}
$$

and compute

$$
E\big(X_t^{(h,\tau)}\big) = a + t\cdot   \frac{h}{\tau} (p - q),
$$

and

$$
V\big(X_t^{(h,\tau)}\big) = t\cdot 4\frac{h^2}{\tau} p q.
$$

Now, let the step sizes $h$ and $\tau$ go to $0$. From these formulas, it is clear that we need to do this in a controlled way, otherwise the expectations and variances will either blow up to infinity or vanish. In particular, if $h^2$ goes to zero faster than $\tau$, then the variance will shrink to zero, and the random variable $X_t^{(h,\tau)}$ will be deterministic in the limit. Conversely, if $\tau$ goes to zero faster than $h^2$, then the variance will blow up to infinity, and the random walk will become wildly erratic in the limit. To avoid these extremes, we impose a *scaling condition* that $h^2$ and $\tau$ go to zero at the same rate; that is, we require that there exists a positive constant $D>0$ such that

$$
\frac{h^2}{\tau} = D
$$

as $h,\tau \to 0$. This constant (which has units of squared length over time) is known as a *diffusion coefficient*.

However, note that this scaling condition creates a problem in the expectation. Indeed, unless $p=q=1/2$, the expectation will still blow up to either positive or negative infinity as $h,\tau \to 0$. To avoid this, we need to also impose a second condition that $p$ and $q$ depend on $h$ and $\tau$ in such a way that there is a constant $\mu \in \mathbb{R}$ such that

$$
D\cdot \frac{p-q}{h} = \mu
$$

as $h,\tau\to 0$. Note that this condition indeed implies that $p$ and $q$ both approach $1/2$ as $h,\tau \to 0$. This constant (which has units of length over time) is known as a *drift coefficient*.

Putting all this together, we find that under these scaling conditions, the expectations and variances of the scaled random walk converge to

$$
E(X_t^{(h,\tau)}) \to a + \mu t,
$$

and

$$
V(X_t^{(h,\tau)}) \to Dt.
$$

Now, is it actually possible to have $h$ and $\tau$ go to $0$ while satisfying these scaling conditions? Yes! For example, we can set $n=m^2$ and

$$
p = \frac{1}{2} + \frac{1}{2m},
$$

and then let $m\to \infty$.

```{python}
# | fig-align: center
# | code-fold: true

def gen_random_walk(m, x, t, n_trajectories, random_state=None):
    n = m**2  # number of time steps
    h = x / m  # size of space step
    tau = t / n  # size of time step
    p = 0.5 + 1 / (2 * m)  # probability of up move

    T = Time.continuous(start=tau, stop=t, num_points=n)

    B = IIDProcess(
        distribution=bernoulli(p=p),
        time=T,
        name="B",
    ).from_simulation(
        n_trajectories=n_trajectories,
        random_state=random_state,
    )

    # Construct the increment processes
    Delta_X = h * (2 * B - 1)

    # Add the initial state
    a = RandomVariable(domain=B.domain, name=0).from_constant(0)
    Delta_X.add_initial_state(a)

    # Construct the random walk processes by cumulative summation
    X = Delta_X.cumsum(name="X")

    return X


# Number of space steps
m1 = 10
m2 = 30

x = 30  # space endpoint
t = 10  # time endpoint

n_trajectories = 1

X1 = gen_random_walk(m1, x, t, n_trajectories, random_state=42).with_name("X1")
X2 = gen_random_walk(m2, x, t, n_trajectories, random_state=42).with_name("X2")

_, ax = plt.subplots(figsize=(6, 5))
X1.plot_trajectories(
    ax=ax,
    colors=[yellow],
    plot_kwargs={"linewidth": 1},
)
X2.plot_trajectories(
    ax=ax,
    colors=[blue],
    plot_kwargs={"linewidth": 1},
)
ax.set_title("Scaled random walks")
plt.show()
```


```{python}
# | fig-align: center
# | code-fold: true

n_trajectories = 50

X1 = gen_random_walk(m1, x, t, n_trajectories, random_state=42).with_name("X1")
X2 = gen_random_walk(m2, x, t, n_trajectories, random_state=42).with_name("X2")

_, ax = plt.subplots(figsize=(6, 5))
X1.plot_trajectories(
    ax=ax,
    colors=[yellow],
    plot_kwargs={"linewidth": 1, "alpha": 0.3},
)
X2.plot_trajectories(
    ax=ax,
    colors=[blue],
    plot_kwargs={"linewidth": 1, "alpha": 0.3},
)
ax.set_title("Scaled random walks")
plt.show()
```


## Bibliography