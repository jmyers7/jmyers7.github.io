---
title: "Linear regression"
date: "2025-09-26"
toc: true
categories: []
image: "thumbnail.png"
draft: true
---

## Introduction

## Reference on information theory

## Linear regression as a PGM

## Entropy of a gaussian random variable

Suppose that $X \sim \mathcal{N}(\mu,\sigma^2)$. Then

$$
\begin{align*}
H(X) &= - \int f(x) \log{(f(x))} \ \text{d}x \\
&= - \int f(x)\left[ \log\left( \frac{1}{\sqrt{2\pi \sigma^2}}\right) - \frac{1}{2\sigma^2}(x-\mu^2) \right]  \ \text{d} x \\
&= \frac{1}{2}\log(2\pi \sigma^2) \int f(x) \ \text{d}x + \frac{1}{2\sigma^2}\int f(x)(x-\mu)^2 \ \text{d} x \\ 
&= \frac{1}{2}\log(2\pi \sigma^2) + \frac{1}{2} \\
&= \frac{1}{2}\log(2\pi e \sigma^2).
\end{align*}
$$

::: {#thm-test}

Let $X$ be a gaussian random variable with mean $\mu$ and standard deviation $\sigma$. Then

$$
H(X) = \frac{1}{2} \log(2\pi e \sigma^2),
$$

where $e$ is the base of the natural logarithm.

:::



## Mutual information in a linear regression model

Let's suppose that we have a linear regression model

$$
Y = \beta_0 + \beta_1 X + \varepsilon,
$$

where $X$ and $\varepsilon$ are independent and

$$
X \sim \mathcal{N}(\mu_X,\sigma_X^2), \quad \varepsilon \sim \mathcal{N}(0,\sigma_\varepsilon^2).
$$

Then

$$
Y\sim \mathcal{N}(\mu_X, \beta_1^2\sigma_X^2 + \sigma_\varepsilon^2).
$$

Using that

$$
(Y \mid X=x) \sim \mathcal{N}(\beta_0+\beta_1x, \sigma_\varepsilon^2),
$$

we get

$$
H(Y\mid X=x) = \frac{1}{2}\log(2\pi e \sigma_\varepsilon^2),
$$

So that

$$
H(Y\mid X) = \int H(Y\mid X=x) f(x) \ \text{d}x = \frac{1}{2}\log(2\pi e \sigma_\varepsilon^2).
$$

Thus

$$
\begin{align*}
I(X,Y) &= H(Y) - H(Y\mid X) \\
&= \frac{1}{2}\log\left[2\pi e(\beta_1^2\sigma_X^2+\sigma_\varepsilon^2)\right] - \frac{1}{2}\log(2\pi e \sigma_\varepsilon^2) \\
&= \frac{1}{2}\log \left( \frac{\beta_1^2\sigma_X^2}{\sigma_\varepsilon^2} + 1 \right).
\end{align*}
$$


## Conclusion

## Mathematical appendix

::: {#thm-lin-reg-equiv}

Let $U$ and $Y$ be two random variables. The following relationships between $U$ and $Y$ are equivalent:

1. An equation
$$
Y = U + \varepsilon,
$$
  where $\varepsilon \sim \mathcal{N}(0,\sigma^2)$ is a random variable (for some fixed parameter $\sigma>0$), and where $U$ and $\varepsilon$ are independent.
2. A conditional distribution
$$
(Y\mid U=u) \sim \mathcal{N}(u,\sigma^2)
$$
  for each $u$, where $\sigma>0$ is a fixed parameter.

:::

To prove this, we consider the functions

$$
r,s: \mathbb{R}^2 \to \mathbb{R}^2
$$

with

$$
r(u,y) = (u, y-u) \quad \text{and} \quad s(u,e) = (u, u + e).
$$

Notice that $r$ and $s$ are mututal inverses, and that

$$
|\det{\left(\nabla r(u,y)\right)}| = |\det{\left(\nabla s(u,e)\right)}| = 1.
$$

Now, suppose that we have the three random variables $U$, $Y$, and $\varepsilon$ as in the first set up. Then we have

$$
(U,Y) = s(U,\varepsilon)
$$

where $s$ is the function above. Applying a [density transformation](https://johnmyers-phd.com/book/chapters/07-more-prob.html#density-transformations) reveals that

$$
f_{(U,Y)}(u,y) = f_{(U,\varepsilon)}(r(u,y)) = f_U(u)f_\varepsilon(y-u) ,
$$

where the second inequality follows from indpendence of $U$ and $\varepsilon$. But

$$
f_\varepsilon(y-u) = \frac{1}{\sigma \sqrt{2\pi}}\exp \left[ - \frac{1}{2} \left( \frac{y-u}{\sigma}\right)^2\right],
$$

so that indeed

$$
(Y\mid U=u) \sim \mathcal{N}(u,\sigma^2),
$$

since $f_{Y|U}(y|u) = f_\varepsilon(y-u)$.

Conversely, suppose that we have a pair of random variables $U$ and $Y$ as in the second set up. Define the random variable $\varepsilon$ via the equation

$$
(U,\varepsilon) = r(U,Y).
$$

Then, we have

$$
f_{(U,\varepsilon)}(u,e) = f_{(U,Y)}(s(u,e)) = f_{(U,Y)}(u,u+e) = f_U(u)f_{Y|U}(u+e|u).
$$ {#eq-factor}


But

$$
f_{Y|U}(u+e|u) = \frac{1}{\sigma\sqrt{2\pi}} \exp \left[ -\frac{1}{2} \left( \frac{e}{\sigma}\right)^2\right],
$$

and hence

$$
f_\varepsilon(e) = \int_\mathbb{R} f_{(U,\varepsilon)}(u,e) \ \text{d}u = \frac{1}{\sigma\sqrt{2\pi}} \exp \left[ -\frac{1}{2} \left( \frac{e}{\sigma}\right)^2\right] \int_\mathbb{R} f_U(u) \ \text{d}u.
$$

This shows $\varepsilon \sim \mathcal{N}(0,\sigma^2)$. Also, the factorization in @eq-factor shows that $U$ and $\varepsilon$ are independent.