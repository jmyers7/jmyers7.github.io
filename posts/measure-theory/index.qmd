---
title: "Measure theory"
date: "2025-10-17"
bibliography: /Users/johnmyers/Documents/work/johnmyers-phd/aux-files/references.bib
toc: true
categories: [Information theory, Probability theory, Measure theory, Sigma-algebras, Conditional expectations, Hilbert spaces, Linear algebra, Python]
image: "thumbnail.svg"
draft: true
---

::: {.content-hidden}
$$
{{< include /aux-files/custom.tex >}}
$$
:::

## Introduction

In this post we explore the relationship between random variables, algebras of sets, and Hilbert spaces. We begin by showing how random variables induce algebras of observable events through their level sets, and we characterize the refinement relations between these algebras in terms of the behavior of the random variables on the atoms of pre-existing algebras. Next, we introduce the $L^2$-spaces associated to algebras of sets, demonstrating that these spaces are Hilbert spaces with inner products defined via expectation. Finally, we connect the refinement of algebras to the nesting of their corresponding $L^2$-spaces, providing a geometric perspective on information refinement.

```{python}
#| echo: true
#| code-fold: true
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from IPython.display import HTML
from sigmahaus.StochasticProcesses import FirstOrderMarkovChain, conditional_exp, DiscreteIID
from sigmahaus.Utilities import print_html_df

np.random.seed(42)
plt.style.use(
    "../../aux-files/custom-theme.mplstyle"
)
yellow = "#FFC300"
blue = "#3399FF"
pink = "#FF3399"
colors = [yellow, pink, blue]

```

## Random variables as information carriers

First, we set the scene and recall a few basic concepts from the previous post. We let $\Omega$ be a finite set, conceptualized as the set of outcomes of an experiment. An *algebra of sets* $\calF$ in $\Omega$ is a collection of subsets of $\Omega$ that contains $\Omega$ and is closed under complements and finite unions. Algebras capture information about which events can be observed in the experiment: if $A \in \calF$, then the event $A$ is observable, meaning that an observer can determine whether the outcome $\omega \in A$ or not. Algebras can be partially ordered by the refinement relation: an algebra $\calF$ *refines* another algebra $\calG$ if $\calG \subset \calF$. In this case, $\calF$ contains at least as much information as $\calG$, since every event observable in $\calG$ is also observable in $\calF$.

As we will see in this section, functions on $\Omega$ also carry information. In particular, *random variables* are functions $X:\Omega \to \bbr$ that assign a real number to each outcome $\omega \in \Omega$. They induce algebras through their *level sets*, which are the preimages of singletons in $\bbr$. In what follows, it will be convenient to allow the codomain of $X$ to be an arbitrary finite-dimensional euclidean space $\bbr^n$, not just $\bbr$, in which case a function $X:\Omega \to \bbr^n$ is often called a *random vector*.

::: {#def-rv-part}
Let $X:\Omega\to \bbr^n$ be a random vector on a finite set $\Omega$. For $x\in \bbr^n$, sets of the form

$$
X^{-1}(x) \defeq \{ \omega \in \Omega : X(\omega)=x\}
$$

are called *level sets*.
:::

As you may easily check, the nonempty level sets of a random vector $X$ partition its domain, and through this partition the random vector generates an algebra of sets in $\Omega$ denoted $\sigma(X)$. The level sets are the *atoms* of this algebra, i.e., they are the minimal nonempty sets in $\sigma(X)$. The algebra $\sigma(X)$ captures precisely the information an observer gains by learning the value of $X$. If the observer knows $X(\omega) = x$, they know that $\omega \in X^{-1}(x)$ but cannot distinguish between outcomes within this level set. Thus, $\sigma(X)$ represents the coarsest partition of $\Omega$ consistent with the information provided by $X$.

Now we introduce some notation for algebras generated by multiple random vectors. Above, we've seen algebras generated by one random variables. In fact, we may form algebras generated by any finite number of random variables, as follows.

::: {#def-algebra-gen-rv}
Let $X_1,X_2,\ldots,X_m$ be random vectors on a finite set $\Omega$. The *algebra generated* by $X_1,X_2,\ldots,X_m$ is the algebra of sets in $\Omega$, denoted by $\sigma(X_1,X_2,\ldots,X_m)$, whose atoms are all nonempty intersections of the form 
$$
\bigcap_{i=1}^m X_i^{-1}(x_i),
$$

where $x_i$ is a value of $X_i$ for each $i$.
:::

In fact, if we are given a finite set of random variables $X_1,X_2,\ldots,X_m$ as in the theorem, then we may form the random vector

$$
X: \Omega \to \bbr^m, \quad X(\omega) = (X_1(\omega), X_2(\omega), \ldots, X_m(\omega)).
$$

Then, as you may easily check, the algebra generated by $X_1,X_2,\ldots,X_m$ is simply the algebra generated by the random vector $X$:

$$
\sigma(X_1,X_2,\ldots,X_m) = \sigma(X).
$$

Now, very often the set $\Omega$ comes equipped with a pre-existing algebra $\calF$ of observable events (perhaps as part of a probability space $(\Omega,\calF,P)$). In this case, it is natural to ask how the information encoded in $\sigma(X)$ relates to that in $\calF$. The first possiblity is that neither algebra refines the other, meaning that $\sigma(X)$ and $\calF$ contain different kinds of information. The other possibilities are that one algebra refines the other, meaning that one contains at least as much information as the other. The following theorem characterizes these refinement relations in terms of the behavior of $X$ on the atoms of $\calF$.

::: {#thm-rv-refine}
## Random variables and refinements
Let $\calF$ be an algebra of sets in a finite set $\Omega$, and let $X:\Omega \to \bbr^n$ be a random vector.

1. The algebra $\calF$ refines $\sigma(X)$ (equivalently, $\sigma(X) \subset \mathcal{F}$) if and only if $X$ is constant on every atom in $\calF$.

2. The algebra $\sigma(X)$ refines $\calF$ (equivalently, $\calF \subset \sigma(X)$) if and only if every atom in $\calF$ is a union of level sets of $X$.
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
The algebra $\calF$ refines $\sigma(X)$ if and only if every nonempty level set is a union of atoms in $\calF$. This proves the result in the first statement. A symmetric argument using the same theorem proves the second statement.
:::

Random vectors satisfying the first condition in @thm-rv-refine arise frequently enough to warrant their own terminology.

::: {#def-measurable}
Let $\calF$ be an algebra of sets in a finite set $\Omega$. A random vector $X$ on $\Omega$ will be called *$\calF$-measurable* if $\calF$ refines the algebra $\sigma(X)$, i.e., $\sigma(X) \subset \calF$.
:::

Returning to our coin-flipping scenario, we see that $Y$ is *only* $\calF_3$-measureable, since the atoms in this algebra are the only ones on which $Y$ is constant. (Of course, $\calF_3$ is the power set of $\Omega$.) The information afforded by $Y$ is not comparable to that of $\calF_1$ or $\calF_2$, since there are no subset containments between $\sigma(Y)$ and $\calF_1$ or $\calF_2$.

If $X$ and $Y$ are both random vectors on $\Omega$, then we may compare the information they encode by comparing their algebras $\sigma(X)$ and $\sigma(Y)$. Then @thm-rv-refine tells us *when* one algebra refines the other, but not *how* the vectors relate. The next theorem provides this missing piece: refinement is equivalent to functional dependence.

::: {#thm-functional-rep}
## Functional representation
Let $X$ and $Y$ be two $\bbr^n$-valued random vectors on a finite set $\Omega$. The following statements are equivalent:

::: {.column-margin}
@thm-functional-rep holds for general measurable spaces, not just finite sets. The proof requires the Doob-Dynkin lemma and properties of $\sigma$-algebras. See Lemma 1.14 in @Kallenberg2021 for the general statement and proof. Our finite setting allows a more elementary approach.
:::

1. The algebra $\sigma(Y)$ refines $\sigma(X)$.
2. There exists a function $f:\bbr^n \to \bbr^n$ such that $X = f \circ Y$.
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
Suppose that $\sigma(Y)$ refines $\sigma(X)$ and write

$$
X = \sum_{i=1}^m x_i I_{A_i} \quad \text{and} \quad Y = \sum^n_{j=1} y_j I_{B_j},
$$

where $I_{A_i}$ and $I_{B_j}$ are indicator functions for the level sets

$$
A_i = X^{-1}(x_i) \quad \text{and} \quad B_j = Y^{-1}(y_j).
$$

Then every level set $B_j$ is contained in a unique level set $A_i$, and we may define a function 

$$
h:\{y_1,\ldots,y_n\} \to \bbr
$$

by setting $h(y_j) = x_i$ if $B_j \subset A_i$. Then, define

$$
\pi: \bbr \to \bbr, \quad \pi(y) = \sum_{j=1}^n \delta_{y_j}(y),
$$

where $\delta_{y_j}$ is a Dirac delta function. If we then set $f = h \circ \pi$ and choose $\omega$ in an arbitrary $B_j$, we have

$$
f(Y(\omega)) = (h\circ \pi)(y_j) = h(y_j) = x_i = X(\omega).
$$

Since $B_j$ and $\omega$ were chosen arbitrarily, this proves that (1) implies (2).

To prove the converse, suppose that $X = f \circ Y$ for some function $f:\bbr \to \bbr$. To prove that $\sigma(Y)$ refines $\sigma(X)$, by @thm-rv-refine it will suffice to show that every level set of $Y$ is  contained in a level set of $X$. So, let $y \in \bbr$ and consider the level set $B = Y^{-1}(y)$. If $B$ is nonempty, then for every $\omega \in B$, we have $X(\omega) = f(Y(\omega)) = f(y)$. Thus, $B \subset X^{-1}(f(y))$, and the proof is complete.
:::

## Example: a coin with momentum

Let's explore these ideas with a concrete example. Consider a coin-flipping experiment where the outcome at each time step depends on the previous outcomes, introducing a "momentum" or "inertia" effect. Specifically, if the last flip was heads, the coin is more likely to land heads again, and vice versa for tails. This scenario can be modeled using a Markov chain.

Suppose that we write $X_t$ for the result of the $t$-th coin flip, where $X_t = 1$ represents heads and $X_t = 0$ represents tails. The first coin is unbiased, so

$$
P(X_1=1) = P(X_1 = 0) = 0.5
$$

However, the conditional probabilities for subsequent flips are given by the transition matrix

$$
\begin{bmatrix}
P(X_t=0 \mid X_{t-1}=0) & P(X_t=1 \mid X_{t-1}=0) \\
P(X_t=0 \mid X_{t-1}=1) & P(X_t=1 \mid X_{t-1}=1)
\end{bmatrix} = 
\begin{bmatrix}
0.7 & 0.3 \\
0.2 & 0.8
\end{bmatrix}.
$$

Suppose that we flip the coin four times, producing a Markov chain of length $4$. All $2^4 = 16$ possible outcomes $(X_1,X_2,X_3,X_4)$ form the sample space $\Omega$ of this experiment. The probability of each outcome may be computed using the initial probabilities and the transition matrix. We implement $\Omega$ and these probabilities as a data frame:

```{python}
# | echo: true
# | code-fold: true
# | fig-align: center

# P(X_1 = 0) = 0.5, P(X_1 = 1) = 0.5
init_prob = np.array([0.5, 0.5])

# P(X_{n+1} = 0 | X_n = 0) = 0.7, P(X_{n+1} = 1 | X_n = 0) = 0.3
# P(X_{n+1} = 0 | X_n = 1) = 0.2, P(X_{n+1} = 1 | X_n = 1) = 0.8
transition_matrix = np.array([[0.7, 0.3], [0.2, 0.8]])

# Instantiate Markov chain modeling coin flips with momentum
mc = FirstOrderMarkovChain(
    transition_matrix,
    init_prob,
)

# Display the sample space
mc.setup_sample_space(trajectory_length=4)
print_html_df(df=mc.omega, title="probability space for coin flips with momentum")

```

<br>

We can simulate longer chains of coin flips using this Markov chain model. Below, we simulate a single trajectory of length 50 and plot the individual coin flip outcomes as well as the cumulative number of heads over time.

```{python}
# | echo: true
# | code-fold: true
# | fig-align: center

# Reset chain length to 50 and simulate a single chain
mc.simulate(trajectory_length=50, num_trajectories=1)

# Setup the figure and axes
fig, axes = plt.subplots(ncols=2, figsize=(8, 4))

# Plot individual coin flip outcomes
mc.plot_simulations(
    ax=axes[0],
    simulation_kwargs={"shift": 1},
    plot_kwargs={"marker": "o"},
)
axes[0].set_xlabel("number of flips")
axes[0].set_ylabel("heads (1) or tails (0)")
axes[0].set_title("individual coin flip outcomes")

# Plot cumulative number of heads
mc.plot_simulations(
    ax=axes[1],
    simulation_kwargs={"cumulative": True, "shift": 1},
    plot_kwargs={"marker": "o"},
)
axes[1].set_xlabel("number of flips")
axes[1].set_ylabel("number of heads")
axes[1].set_title("cumulative number of heads")

fig.suptitle("coin flip simulation with momentum")
plt.tight_layout()
plt.show()

```

We can also simulate multiple trajectories to see how the number of heads varies across different runs of the experiment. Below, we simulate 10 chains of length 250 and plot the cumulative number of heads for each simulation.

```{python}
# | echo: true
# | code-fold: true
# | fig-align: center

# Reset the chain length and simulate 10 chains
mc.simulate(trajectory_length=250, num_trajectories=10)

# Plot the number of heads over time for each simulation
_, ax = plt.subplots()
mc.plot_simulations(
    ax=ax,
    colors=colors,
    simulation_kwargs={"cumulative": True},
)
ax.set_title("10 coin flip simulations with momentum")
ax.set_xlabel("number of flips")
ax.set_ylabel("number of heads")
plt.tight_layout()
plt.show()

```





## Hilbert spaces of random variables

We've seen how random variables encode information through the algebras they generate. But random variables also have numerical values, which allows us to perform algebraic operations: we can add them, multiply them by scalars, and compute their expected values. This algebraic structure becomes particularly rich when we introduce an inner product, turning the space of random variables into a Hilbert space. This latter structure provides a geometric perspective on random variables, where concepts like orthogonality, projection, and distance become meaningful. This perspective will prove essential for understanding conditional expectation, which emerges naturally as orthogonal projection onto subspaces determined by algebras.

First, we recall that the pointwise sum and scalar multiple of random variables are again random variables, and these operations have the usual properties of commutativity, associativity, *etc*. Thus, the set of all random variables on a finite set $\Omega$ forms a vector space over the real numbers $\bbr$. The subspaces of this vector space that are most relevant to us are those consisting of random variables that are measurable with respect to a given algebra of sets.

We shall also need the concept of *expectation* of a measurable random variable. If $(\Omega,\calF,P)$ is a finite probability space and $X:\Omega \to \bbr$ is a random variable, then the *expectation* of $X$ is defined as

$$
E(X) \defeq \int_\Omega X \ dP,
$$ {#eq-def-expectation}

where the integral on the right is the Lebesgue integral with respect to the probability measure $P$. In the finite setting, this integral reduces to the familiar weighted average of the values of the random variable, where the weights are the probabilities of the level sets of $X$:

::: {.column-margin}
The benefit of using the Lebesgue integral ([-@eq-def-expectation]) to define expectation is that it generalizes directly to "continuous" sample spaces equipped with probability measures on $\sigma$-algebras. In that more general setting, expectation cannot be defined as a simple finite weighted sum as in ([-@eq-expectation-finite]), though it remains a Lebesgue integral. Thus, the Lebesgue integral allows us to treat both discrete and continuous probability theory in a unified manner.
:::

$$
\int_\Omega X \ dP = \sum_{i=1}^m x_i P(A_i),
$$ {#eq-expectation-finite}

where $A_1, A_2, \ldots, A_m$ are the level sets of $X$ and $x_i$ is the value of $X$ on $A_i$. Note that in order for the probability $P(A_i)$ to be defined, we must have $A_i \in \calF$, which is guaranteed if $X$ is $\calF$-measurable.

Finally, two $\calF$-measurable random variables $X$ and $Y$ on $(\Omega,\calF,P)$ will be said to be *equal almost surely* if

$$
P(\{\omega \in \Omega : X(\omega) \neq Y(\omega)\}) = 0.
$$

Almost sure equality is an equivalence relation on the set of $\calF$-measurable random variables.

With these ingredients in place, we may now define the $L^2$-space associated to an algebra of sets on a finite sample space.

::: {#def-L2}
::: {.column-margin}
The $L^2$ notation is borrowed from the case of a general sample space $\Omega$ equipped with a probability measure $P$ on a $\sigma$-algebra $\calF$. In that case, $L^2(\Omega,\calF,P)$ consists of all (equivalence classes of) $\calF$-measurable random variables $X:\Omega \to \bbr$ such that $E(X^2) < \infty$. In our finite setting, this condition on the expectation of $X^2$ is automatically satisfied.
:::
Let $(\Omega,\calF,P)$ be a finite probability space. We write $L^2(\Omega,\calF,P)$ for the set of all $\calF$-measurable random variables $X:\Omega \to \bbr$, where two random variables are identified if they are equal almost surely. When the set $\Omega$ and the measure $P$ are understood from context, we will abbreviate $L^2(\Omega,\calF,P)$ as $L^2(\calF)$.
:::

So, technically $L^2(\calF)$ is a set of equivalence classes of $\calF$-measurable random variables, but for ease of exposition and notation we will often refer to elements of $L^2(\calF)$ as if they were random variables themselves. This will not cause too much confusion.

One should check that $L^2(\calF)$ is indeed a vector subspace of the vector space of *all* random variables on $\Omega$, which amounts to checking that sums and scalar multiples of $\calF$-measurable random variables are again $\calF$-measurable. We will leave this task to the interested reader.

Beyond inducing equivalence classes under almost sure equality, the probability measure $P$ plays no role in the definition of $L^2(\calF)$ itself. However, it does define an inner product on this space via expectation:

::: {#thm-inner-prod}
## $L^2$-spaces are Hilbert spaces
::: {.column-margin}
A *Hilbert space* is a complete inner product space. Since $L^2(\calF)$ is finite-dimensional (see below), it is automatically complete, so the existence of an inner product suffices to make it a Hilbert space.
:::
Let $(\Omega,\calF,P)$ be a finite probability space. The map
$$
\langle -, - \rangle : L^2(\calF) \times L^2(\calF) \to \bbr, \quad \langle X, Y \rangle = E(XY)
$$
defines an inner product on $L^2(\calF)$.
:::

Using the Lebesgue integral notation, the inner product can be written as

$$
\langle X, Y \rangle = \int_\Omega XY \ dP.
$$ {#eq-inner-prod-integral}

But if we write $X$ and $Y$ as linear combinations of indicator functions,

$$
X = \sum_{i=1}^m x_i I_{A_i} \quad \text{and} \quad Y = \sum_{j=1}^n y_j I_{B_j}
$$

where $A_i$ and $B_j$ are the level sets of $X$ and $Y$, respectively, then ([-@eq-inner-prod-integral]) becomes

$$
\langle X, Y \rangle = \sum_{i,j} x_i y_j P(A_i \cap B_j).
$$

One should check that $\langle -, -\rangle$ satisfies the properties of an inner product: linearity in the first argument, symmetry, and positive-definiteness. Again, we shall leave this task to the interested reader. We only note that the proof of positive-definiteness involves showing that $\langle X,X\rangle=0$ if and only if $X$ is almost surely equal to the zero random variable. Remember that elements in $L^2(\calF)$ are equivalence classes under almost sure equality!


The inner product induces a norm

$$
\|X\|_2 = \sqrt{\langle X, X \rangle} = \sqrt{E(X^2)},
$$

which measures the "magnitude" of a random variable. This is precisely the so-called *$L^2$-norm*, and the corresponding metric $d(X, Y) = \|X - Y\|$ measures the mean-squared distance between random variables. In statistics, this is the *root mean squared error* (*RMSE*) between $X$ and $Y$.

When algebras are nested, their corresponding $L^2$-spaces are nested as well:

::: {#thm-nested-subspaces}
Let $(\Omega, \mathcal{F}, P)$ be a finite probability space. If $\mathcal{G} \subset \mathcal{H}$ are two subalgebras of $\mathcal{F}$, then
$$
L^2(\mathcal{G}) \subset L^2(\mathcal{H}) \subset L^2(\mathcal{F}).
$$

Moreover, these subset inclusions are ones of vector subspaces.
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
Since $\mathcal{G} \subset \mathcal{H}$, every $\mathcal{G}$-measurable random variable is also $\mathcal{H}$-measurable. Thus, $L^2(\mathcal{G}) \subset L^2(\mathcal{H})$. A symmetric argument shows that $L^2(\mathcal{H}) \subset L^2(\mathcal{F})$. The fact that these inclusions are ones of vector subspaces follows from the fact that sums and scalar multiples of measurable random variables are measurable with respect to the same algebra.
:::

This geometric picture connects directly to information refinement: as algebras refine (capturing more information), their $L^2$-spaces expand in kind (allowing more functions to be represented). Applied to our coin-flip example the nested sequence

$$
\mathcal{F}_1 \subset \mathcal{F}_2 \subset \mathcal{F}_3
$$

induces a nested sequence

$$
L^2(\mathcal{F}_1) \subset L^2(\mathcal{F}_2) \subset L^2(\mathcal{F}_3)
$$

of $L^2$-spaces, where each larger subspace represents more refined information about the outcomes. 

Finally, we note that the $L^2$-spaces associated to algebras have particularly simple and useful orthogonal bases, consisting of indicator functions of the atoms of the algebra. Issues arise when atoms have zero probability, but these are easily handled by working with equivalence classes under almost sure equality.

::: {#thm-basis-L2}
## Bases of $L^2$-spaces
Let $(\Omega,\calF,P)$ be a finite probability space, let $\calG$ be a subalgebra of $\calF$, and let $B_1, B_2, \ldots, B_m$ be the atoms of $\calG$ with positive probability.

1. The indicator functions $I_{B_1}, I_{B_2}, \ldots, I_{B_m}$ form an orthogonal basis of $L^2(\calG)$.

2. The $L^2$-norm of $I_{B_i}$ is $\sqrt{P(B_i)}$.

3. The vector space dimension of $L^2(\calG)$ is equal to the number of atoms in $\calG$ with positive probability.
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
Suppose that we write

$$
B_1,B_2,\ldots,B_m, B_{m+1},B_{m+2},\ldots,B_n,
$$

where the sets $B_{m+1},B_{m+2},\ldots,B_n$ are the atoms of $\calG$ with zero probability (which may not exist). Let $X\in L^2(\calG)$ be a random variable. Since $X$ is $\calG$-measurable, it is constant on each atom in $\calG$. Thus, we may write

$$
X = \sum_{i=1}^n x_i I_{B_i},
$$

for some values $x_i \in \bbr$ (not necessarily distinct). However, since $P(B_i) = 0$ for $i = m+1, m+2, \ldots, n$, we have

$$
X = \sum_{i=1}^m x_i I_{B_i}
$$

almost surely. Thus, since elements in $L^2(\calG)$ are technically equivalence classes under almost sure equality, we have that $X$ is a linear combination of the indicator functions $I_{B_1}, I_{B_2}, \ldots, I_{B_m}$. Thus, these indicator functions span $L^2(\calG)$. To see that they are orthogonal, we suppose $i\neq j$. Then, $B_i \cap B_j =\emptyset$, and so

$$
\langle I_{B_i}, I_{B_j} \rangle = E(I_{B_i} I_{B_j}) = E(I_{B_i\cap B_j})= E(0) = 0.
$$

Orthogonality implies linear independence, so the proof is complete.
:::



## Conditional expectations as projections

::: {#def-conditional-expectation}
Let $(\Omega, \mathcal{F}, P)$ be a finite probability space, $\mathcal{G} \subset \mathcal{F}$ a subalgebra, and $X \in L^2(\mathcal{F})$ a random variable. A *conditional expectation* of $X$ given $\mathcal{G}$ is a random variable $E(X\mid \calG) \in L^2(\mathcal{G})$ such that

$$
\int_B X \ dP = \int_B E(X \mid \mathcal{G}) \ dP.
$$ {#eq-averaging-property}

for all $B \in \mathcal{G}$. This is called the *averaging property* of conditional expectation.
:::

Any two conditional expectations of $X$ given $\calG$ are almost surely equal, so the conditional expectation is unique up to almost sure equality. A particular choice of conditional expectation is called a *version*.

The averaging property ([-@eq-averaging-property]) defines conditional expectation abstractly, but doesn't immediately suggest how to compute it. The key insight is that this property has a geometric interpretation. To uncover it, we rewrite the averaging property as

$$
\int_\Omega \left[X - E(X \mid \mathcal{G})\right] I_B \ dP = 0,
$$

which, using the inner product from @thm-inner-prod, becomes

$$
\langle X - E(X \mid \mathcal{G}), I_B \rangle = 0.
$$

Since the indicator functions $\{I_B : B \in \mathcal{G}\}$ span $L^2(\mathcal{G})$ (by @thm-basis-L2), the residual $X - E(X \mid \mathcal{G})$ is orthogonal to every element of $L^2(\mathcal{G})$. Thus, conditional expectation $E(X\mid \calG)$ is characterized as the orthogonal projection of $X$ onto the subspace $L^2(\mathcal{G})$.

Computing this projection is straightforward. Let $B_1,B_2,\ldots,B_m$ be the atoms of $\calG$ with positive probability. By @thm-basis-L2, the normalized indicator functions

$$
\frac{I_{B_1}}{\sqrt{P(B_1)}}, \frac{I_{B_2}}{\sqrt{P(B_2)}}, \ldots, \frac{I_{B_m}}{\sqrt{P(B_m)}}
$$

form an orthonormal basis of $L^2(\calG)$. The conditional expectation of $X$ given $\calG$ is the orthogonal projection onto this basis:

$$
E(X\mid \calG) = \sum_{i=1}^m \left\langle X, \frac{I_{B_i}}{\sqrt{P(B_i)}} \right\rangle \frac{I_{B_i}}{\sqrt{P(B_i)}}.
$$

Simplifying using the definition of the inner product yields

$$
E(X\mid \calG) = \sum_{i=1}^m \frac{1}{P(B_i)}\left(\int_{B_i} X \ dP \right)I_{B_i}.
$$ {#eq-cond-exp-compute}

To evaluate the integrals in this formula, we write $X$ in terms of its level sets:

$$
X = \sum_{j=1}^n x_j I_{A_j},
$$

where $A_1,A_2,\ldots,A_n$ are the level sets of $X$. Then

$$
\int_{B_i} X \ dP = \sum_{j=1}^n x_j P(A_j \cap B_i).
$$

Substituting into ([-@eq-cond-exp-compute]) gives an explicit formula in terms of conditional probabilities:

$$
E(X\mid \calG) = \sum_{i=1}^m \left(\sum_{j=1}^n x_j P(A_j \mid B_i)\right)I_{B_i}
$$ {#eq-cond-exp-compute-2}

where $P(A_j \mid B_i) = P(A_j \cap B_i)/P(B_i)$ is the conditional probability of $A_j$ given $B_i$. This formula connects our geometric framework to the elementary definition of conditional probability.

We summarize this projection interpretation in the following theorem:

::: {#thm-cond-exp-proj}
## Conditional expectation as orthogonal projection
Let $(\Omega, \mathcal{F}, P)$ be a finite probability space, $\mathcal{G} \subset \mathcal{F}$ a subalgebra, and $X \in L^2(\mathcal{F})$ a random variable. Then, the orthogonal projection of $X$ onto the subspace $L^2(\mathcal{G})$ is a conditional expectation of $X$ given $\mathcal{G}$. This projection is given by @eq-cond-exp-compute-2.
:::

So, given an $\calF$-measurable random variable $X$ and a coarser algebra $\mathcal{G}$, we may trivially write

$$
X = E(X\mid \calG) + \left[X - E(X \mid \mathcal{G})\right].
$$

This decomposition gives us two components: a $\mathcal{G}$-measurable part $E(X\mid \calG)$ that captures all information in $X$ knowable from $\mathcal{G}$, and a residual $X - E(X \mid \mathcal{G})$ orthogonal to every $\calG$-measurable random variable, representing pure noise relative to $\calG$.

In the case that $\calG = \{\emptyset, \Omega\}$ is the trivial algebra, we have

$$
E(X \mid \calG) = E(X),
$$

the constant random variable (taking value $E(X)$ everywhere). The trivial algebra represents having no information at all, so the best prediction of $X$ without any information is simply its expectation.

The fact that $E(X \mid \mathcal{G})$ is the "best" $\mathcal{G}$-measurable approximation to $X$ can be made numerically precise, as in the following theorem:

::: {#thm-best-approx}
## Best $L^2$-approximation
Let $(\Omega, \mathcal{F}, P)$ be a finite probability space, $\mathcal{G} \subset \mathcal{F}$ a subalgebra, and $X \in L^2(\mathcal{F})$ a random variable. Then,

$$
E\left[(X - E(X \mid \mathcal{G}))^2\right] = \min_{Y \in L^2(\mathcal{G})} E\left[(X - Y)^2\right].
$$
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
This follows directly from the fact that orthogonal projection minimizes distance in inner product spaces. See, for example, Proposition 6.36 in [@Axler1997].
:::

Beyond optimality, the projection interpretation of conditional expectation immediately yields several other important properties, which we now develop.

::: {#thm-linearity}
## Linearity of conditional expectation
Let $(\Omega, \mathcal{F}, P)$ be a finite probability space, $\mathcal{G} \subset \mathcal{F}$ a subalgebra, and $X, Y \in L^2(\mathcal{F})$ random variables. For any scalars $a,b \in \bbr$, we have
$$
E(aX + bY \mid \mathcal{G}) = a E(X \mid \mathcal{G}) + b E(Y \mid \mathcal{G})
$$

almost surely.
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
Choose the version of conditional expectation given by orthogonal projection onto $L^2(\calG)$ as in @thm-cond-exp-proj. Since orthogonal projection is linear, the desired result follows immediately.
:::

Linearity tells us that conditional expectation behaves well under linear combinations. But what happens when we condition on the product $XY$ of two random variables? If $Y$ is already $\mathcal{G}$-measurable, then $Y$ is "known" given the information in $\mathcal{G}$, so it should factor out of the conditional expectation. This intuition is formalized by the following property:

::: {#thm-pull-out}
## Pull-out property
Let $(\Omega, \mathcal{F}, P)$ be a finite probability space, $\mathcal{G} \subset \mathcal{F}$ a subalgebra, and $X \in L^2(\mathcal{F})$ a random variable. If $Y$ is a $\calG$-measurable random variable, 

$$
E(XY \mid \mathcal{G}) = Y E(X \mid \mathcal{G}).
$$

:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
We will prove that the random variable $Y E(X \mid \mathcal{G})$ has the defining property of a conditional expectation of $XY$ given $\calG$. To this end, we let $B$ be a set in $\calG$. Then $YI_B$ is $\calG$-measurable, so that if we choose the version of $E(X\mid \calG)$ given by orthogonal projection onto $L^2(\calG)$  as in @thm-cond-exp-proj, we must have

$$
\langle X - E(X \mid \mathcal{G}), Y I_B \rangle = 0.
$$

However, this inner product is

$$
\langle X - E(X \mid \mathcal{G}), Y I_B \rangle = \int_B \left[XY - Y E(X \mid \mathcal{G})\right] \ dP,
$$

so we get

$$
\int_B XY \ dP = \int_B Y E(X \mid \mathcal{G}) \ dP.
$$

This completes the proof.
:::

The pull-out property shows that $\mathcal{G}$-measurable factors can be extracted from conditional expectations. But what happens when we have a nested sequence of algebras $\mathcal{G} \subset \mathcal{H} \subset \mathcal{F}$, representing increasingly refined information states? 

Intuitively, if we first condition on the finer information $\mathcal{H}$ and then "forget" some information by conditioning on the coarser $\mathcal{G}$, we should get the same result as conditioning directly on $\mathcal{G}$. This is the content of the following property (also called the *law of iterated expectations*):

::: {#thm-tower}
## Tower property
Let $(\Omega, \mathcal{F}, P)$ be a finite probability space, and let $\mathcal{G} \subset \mathcal{H} \subset \mathcal{F}$ be subalgebras. If $X \in L^2(\mathcal{F})$ is a random variable, then 

$$
E(E(X \mid \mathcal{H}) \mid \mathcal{G}) = E(X \mid \mathcal{G}).
$$ {#eq-tower}

:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
We have the inclusion of vector subspaces

$$
L^2(\mathcal{G}) \subset L^2(\mathcal{H}) \subset L^2(\mathcal{F}).
$$

If we choose the versions of the conditional expectations given by orthogonal projection  as in @thm-cond-exp-proj, then the left-hand side of ([-@eq-tower]) is the orthogonal projection of $X$ onto $L^2(\mathcal{H})$ followed by the orthogonal projection onto $L^2(\mathcal{G})$. However, since the subspaces are nested, this composition is simply the orthogonal projection of $X$ onto $L^2(\mathcal{G})$, which is the right-hand side of the desired equation.
:::




## Return to the coin flip with momentum


```{python}
# | echo: true
# | code-fold: true
# | fig-align: center

mc.setup_sample_space(trajectory_length=4)
print_html_df(df=mc.omega, title="probability space for coin flips with momentum")

```


```{python}
# | echo: true
# | code-fold: true
# | fig-align: center

# Define the random variable S3 = X1 + X2 + X3
def S3(omega):
    return omega["X1"] + omega["X2"] + omega["X3"]


# Compute the conditional expectation of S3 given X1 and X2
cond_exp_S3_given_X1X2 = conditional_exp(
    omega=mc.omega, RV=S3, sigma_algebra=["X1", "X2"], name="E(S3 | X1, X2)"
)

# Print the conditional expectation
print_html_df(
    df=cond_exp_S3_given_X1X2,
    title="conditional expectation E(S3 | X1, X2)",
)

```

We have a submartingale!

```{python}
# | echo: true
# | code-fold: true
# | fig-align: center


# Define the random variable S2 = X1 + X2
def S2(omega):
    return omega["X1"] + omega["X2"]


# Make a copy of the sample space and compute S2
omega = mc.omega.copy()
omega["S2"] = omega.apply(S2, axis=1)

# S2 is constant on the atoms of sigma(X1, X2), so we group
# by X1 and X2 and get unique values of S2 by using first(). Then
# we convert to a dataframe and turn the multi-index into columns.
S2 = omega.groupby(["X1", "X2"])["S2"].first().to_frame().reset_index()

# Print the comparison of E(S3 | X1,X2) to S2
print_html_df(
    df=cond_exp_S3_given_X1X2.merge(S2, on=["X1", "X2"]),
    title="comparing E(S3 | X1,X2) to S2",
)

```


```{python}
#| echo: true
#| code-fold: true
#| fig-align: center

iid = DiscreteIID(prob=np.array([0.3, 0.4, 0.3]))

_, ax = plt.subplots()
iid.simulate(trajectory_length=50, num_trajectories=1).plot_simulations(ax=ax, colors=colors)
plt.tight_layout()
plt.show()

```


## Conclusion

## Bibliography