---
title: "Measure theory"
date: "2025-10-17"
bibliography: /Users/johnmyers/Documents/work/johnmyers-phd/aux-files/references.bib
toc: true
categories: [Information theory, Probability theory, Measure theory, Sigma-algebras, Conditional expectations, Hilbert spaces, Linear algebra, R]
image: "thumbnail.svg"
draft: true
---

::: {.content-hidden}
$$
{{< include /aux-files/custom.tex >}}
$$
:::

## Introduction

In probability theory, σ-fields encode information. By mapping each σ-field to its corresponding Hilbert space $L^2(\calF)$, we translate information structure into linear algebra: conditioning becomes orthogonal projection, and independence becomes orthogonality. This post develops that dictionary concretely.

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center

# Load required packages
library("ggplot2") # For plotting
library("knitr") # For table formatting (used with kable())
library("dplyr") # For data manipulation
library("latex2exp") # For LaTeX rendering in plots

# Source custom theme file
# Note: Replace this path with your own theme file location
source("../../aux-files/custom-theme.R")

# Extract custom colors from theme
# Note: These color definitions depend on your custom theme file
yellow <- custom_colors[["yellow"]]
blue <- custom_colors[["blue"]]

# Apply custom ggplot2 theme
theme_set(custom_theme())

```

## Random variables as information carriers

First, we set the scene and recall a few basic concepts from the previous post. We let $\Omega$ be a finite set, conceptualized as the set of outcomes of an experiment. An *algebra of sets* $\calF$ in $\Omega$ is a collection of subsets of $\Omega$ that contains $\Omega$ and is closed under complements and finite unions. Algebras capture information about which events can be observed in the experiment: if $A \in \calF$, then the event $A$ is observable, meaning that an observer can determine whether the outcome $\omega \in A$ or not. Algebras can be partially ordered by the refinement relation: an algebra $\calF$ *refines* another algebra $\calG$ if $\calG \subset \calF$. In this case, $\calF$ contains at least as much information as $\calG$, since every event observable in $\calG$ is also observable in $\calF$.

As we will see in this section, functions on $\Omega$ also carry information. In particular, *random variables* are functions $X:\Omega \to \bbr$ that assign a real number to each outcome $\omega \in \Omega$. They induce algebras through their *level sets*, which are the preimages of singletons in $\bbr$. In what follows, it will be convenient to allow the codomain of $X$ to be an arbitrary finite-dimensional euclidean space $\bbr^n$, not just $\bbr$, in which case a function $X:\Omega \to \bbr^n$ is often called a *random vector*.

::: {#def-rv-part}
Let $X:\Omega\to \bbr^n$ be a random vector on a finite set $\Omega$. For $x\in \bbr^n$, sets of the form

$$
X^{-1}(x) \defeq \{ \omega \in \Omega : X(\omega)=x\}
$$

are called *level sets*.
:::

As you may easily check, the nonempty level sets of a random vector $X$ partition its domain, and through this partition the random vector generates an algebra of sets in $\Omega$ denoted $\sigma(X)$. The level sets are the *atoms* of this algebra, i.e., they are the minimal nonempty sets in $\sigma(X)$. The algebra $\sigma(X)$ captures precisely the information an observer gains by learning the value of $X$. If the observer knows $X(\omega) = x$, they know that $\omega \in X^{-1}(x)$ but cannot distinguish between outcomes within this level set. Thus, $\sigma(X)$ represents the coarsest partition of $\Omega$ consistent with the information provided by $X$.

For an example, we consider the experiment of flipping a coin three times that we introduced in the [previous post](../info-2/). The sample space is

$$
\Omega = \{000, 001, 010, 011, 100, 101, 110, 111\},
$$

where a binary string represents the results of the three flips, with a $0$ representing tails and a $1$ representing heads. We define three random variables

$$
X_1, X_2, X_3:\Omega \to \{0,1\}
$$

that record the results of the first, second, and third flips, respectively. For instance, $X_1(010) = 0$ since the first flip in the sequence $010$ is tails. We introduce a fourth random variable

$$
Y:\Omega \to \{0,1,2,3\}
$$

that records the total number of heads across all three flips, i.e.,

$$
Y = X_1 + X_2 + X_3.
$$

The following data frame displays the sample space $\Omega$ along with the values of the four random variables.

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center

# Create the sample space Omega as a data frame
# Each row represents one of the 8 possible outcomes (binary sequences of length 3)
Omega <- data.frame(
  X1 = c(0, 0, 0, 0, 1, 1, 1, 1), # First coin flip: 0 = tail, 1 = head
  X2 = c(0, 0, 1, 1, 0, 0, 1, 1), # Second coin flip
  X3 = c(0, 1, 0, 1, 0, 1, 0, 1) # Third coin flip
)

# Add random variable Y: total number of heads across all three flips
Omega <- Omega %>%
  mutate(Y = X1 + X2 + X3)

# Display the sample space as a formatted table
kable(
  Omega,
  caption = "sample space $\\Omega$ with random variables $X_1, X_2, X_3$, and $Y$"
)
```

<br>

Notice that the two level sets of $X_1$ are

$$
\begin{align*}
A_1 &= X_1^{-1}(0) = \{000, 001, 010, 011\}, \\ 
A_2 &= X_1^{-1}(1) = \{100, 101, 110, 111\}.
\end{align*}
$$

The reader will note that these are exactly the atoms of the algebra $\calF_1$ we considered in the [previous post](../info-2/). Thus, $\sigma(X_1) = \calF_1$.

Similarly, the level sets of $X_2$ and $X_3$ are

$$
\begin{align*}
X_2^{-1}(0) &= \{000, 001, 100, 101\}, & X_2^{-1}(1) &= \{010, 011, 110, 111\}, \\ 
X_3^{-1}(0) &= \{000, 010, 100, 110\}, & X_3^{-1}(1) &= \{001, 011, 101, 111\}.
\end{align*}
$$

If we form all pairwise intersections of the level sets of $X_1$ and $X_2$, we obtain the sets

$$
B_1 = \{000,001\}, \quad B_2 = \{010,011\}, \quad B_3 = \{100,101\}, \quad B_4 = \{110,111\},
$$

which were the atoms of the algebra $\calF_2$ from the [previous post](../info-2/). In the notation to be introduced below (see @def-algebra-gen-rv), this means that

$$
\sigma(X_1, X_2) = \calF_2,
$$

or that $\calF_2$ is the algebra generated by $X_1$ and $X_2$. If we form all $3$-fold intersections of the level sets of $X_1$, $X_2$, and $X_3$, we obtain the singletons

$$
\begin{gather*}
C_1 = \{000\}, \ C_2 = \{001\}, \ C_3 = \{010\}, \ C_4 = \{011\}, \\
C_5 = \{100\}, \ C_6 = \{101\}, \ C_7 = \{110\}, \ C_8 = \{111\},
\end{gather*}
$$

which were the atoms of the algebra $\calF_3$ from the [previous post](../info-2/). Again, in the notation to be introduced below (see @def-algebra-gen-rv), this means that

$$
\sigma(X_1, X_2, X_3) = \calF_3,
$$

or that $\calF_3$ is the algebra generated by $X_1, X_2,$ and $X_3$. Thus, the nested sequence of algebras

$$
\calF_1 \subset \calF_2 \subset \calF_3
$$

may be rewritten as

$$
\sigma(X_1) \subset \sigma(X_1, X_2) \subset \sigma(X_1, X_2, X_3).
$$

Finally, the level sets of $Y$ are

$$
\begin{align*}
D_1 &= Y^{-1}(0) = \{000\}, \\
D_2 &= Y^{-1}(1) = \{001,010,100\}, \\
D_3 &= Y^{-1}(2) = \{011,101,110\}, \\
D_4 &= Y^{-1}(3) = \{111\}.
\end{align*}
$$

In the [previous post](../info-2/), we had columns in our data frame indicating which outcomes belonged to which atoms $A_i$, $B_i$, and $C_i$; let's add these columns here as well, along with columns for the $D_i$:

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center

# Group outcomes by the result of the first flip (X1)
# and assign a group ID to track which partition set (A1 or A2) each outcome belongs to
Omega <- Omega %>%
  group_by(X1) %>%
  mutate(Ai = cur_group_id()) %>% # Ai = 1 for A1 (X1=0), Ai = 2 for A2 (X1=1)
  ungroup() %>%
  as.data.frame()

# Group outcomes by the results of the first two flips (X1 and X2)
# and assign a group ID to track which partition set (B1, B2, B3, or B4) each outcome belongs to
Omega <- Omega %>%
  group_by(X1, X2) %>%
  mutate(Bi = cur_group_id()) %>% # Bi = 1 for B1, Bi = 2 for B2, etc.
  ungroup() %>%
  as.data.frame()

# Group outcomes by all three flips (X1, X2, and X3)
# Each combination of (X1, X2, X3) uniquely identifies one outcome
Omega <- Omega %>%
  group_by(X1, X2, X3) %>%
  mutate(Ci = cur_group_id()) %>% # Ci = 1 for C1, Ci = 2 for C2, etc.
  ungroup() %>%
  as.data.frame()

Omega <- Omega %>%
  group_by(Y) %>%
  mutate(Di = cur_group_id()) %>% # Di = 1 for D1, Di = 2 for D2, etc.
  ungroup() %>%
  as.data.frame()

# Display a formatted table showing the sample space Omega with random variables X1, X2, X3, and Y, including atoms Ai, Bi, Ci, and Di.
kable(
  Omega,
  caption = "sample space $\\Omega$ with random variables $X_1, X_2, X_3$, and $Y$, along with atoms $A_i$, $B_i$, $C_i$, and $D_i$"
)

```

<br>

We will return to this data frame momentarily, but first we introduce some notation for algebras generated by multiple random vectors. Above, we've seen algebras generated by one, two, and three random variables. In fact, we may form algebras generated by any finite number of random variables, as follows.

::: {#def-algebra-gen-rv}
Let $X_1,X_2,\ldots,X_m$ be random vectors on a finite set $\Omega$. The *algebra generated* by $X_1,X_2,\ldots,X_m$ is the algebra of sets in $\Omega$, denoted by $\sigma(X_1,X_2,\ldots,X_m)$, whose atoms are all nonempty intersections of the form 
$$
\bigcap_{i=1}^m X_i^{-1}(x_i),
$$

where $x_i$ is a value of $X_i$ for each $i$.
:::

In fact, if we are given a finite set of random variables $X_1,X_2,\ldots,X_m$ as in the theorem, then we may form the random vector

$$
X: \Omega \to \bbr^m, \quad X(\omega) = (X_1(\omega), X_2(\omega), \ldots, X_m(\omega)).
$$

Then, as you may easily check, the algebra generated by $X_1,X_2,\ldots,X_m$ is simply the algebra generated by the random vector $X$:

$$
\sigma(X_1,X_2,\ldots,X_m) = \sigma(X).
$$

Now, very often the set $\Omega$ comes equipped with a pre-existing algebra $\calF$ of observable events (perhaps as part of a probability space $(\Omega,\calF,P)$). In this case, it is natural to ask how the information encoded in $\sigma(X)$ relates to that in $\calF$. The first possiblity is that neither algebra refines the other, meaning that $\sigma(X)$ and $\calF$ contain different kinds of information. The other possibilities are that one algebra refines the other, meaning that one contains at least as much information as the other. The following theorem characterizes these refinement relations in terms of the behavior of $X$ on the atoms of $\calF$.

::: {#thm-rv-refine}
## Random variables and refinements
Let $\calF$ be an algebra of sets in a finite set $\Omega$, and let $X:\Omega \to \bbr^n$ be a random vector.

1. The algebra $\calF$ refines $\sigma(X)$ (equivalently, $\sigma(X) \subset \mathcal{F}$) if and only if $X$ is constant on every atom in $\calF$.

2. The algebra $\sigma(X)$ refines $\calF$ (equivalently, $\calF \subset \sigma(X)$) if and only if every atom in $\calF$ is a union of level sets of $X$.
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
The algebra $\calF$ refines $\sigma(X)$ if and only if every nonempty level set is a union of atoms in $\calF$. This proves the result in the first statement. A symmetric argument using the same theorem proves the second statement.
:::

Random vectors satisfying the first condition in @thm-rv-refine arise frequently enough to warrant their own terminology.

::: {#def-measurable}
Let $\calF$ be an algebra of sets in a finite set $\Omega$. A random vector $X$ on $\Omega$ will be called *$\calF$-measurable* if $\calF$ refines the algebra $\sigma(X)$, i.e., $\sigma(X) \subset \calF$.
:::

Returning to our coin-flipping scenario, we see that $Y$ is *only* $\calF_3$-measureable, since the atoms in this algebra are the only ones on which $Y$ is constant. (Of course, $\calF_3$ is the power set of $\Omega$.) The information afforded by $Y$ is not comparable to that of $\calF_1$ or $\calF_2$, since there are no subset containments between $\sigma(Y)$ and $\calF_1$ or $\calF_2$.

If $X$ and $Y$ are both random vectors on $\Omega$, then we may compare the information they encode by comparing their algebras $\sigma(X)$ and $\sigma(Y)$. Then @thm-rv-refine tells us *when* one algebra refines the other, but not *how* the vectors relate. The next theorem provides this missing piece: refinement is equivalent to functional dependence.

::: {#thm-functional-rep}
## Functional representation
Let $X$ and $Y$ be two $\bbr^n$-valued random vectors on a finite set $\Omega$. The following statements are equivalent:

::: {.column-margin}
@thm-functional-rep holds for general measurable spaces, not just finite sets. The proof requires the Doob-Dynkin lemma and properties of $\sigma$-algebras. See Lemma 1.14 in @Kallenberg2021 for the general statement and proof. Our finite setting allows a more elementary approach.
:::

1. The algebra $\sigma(Y)$ refines $\sigma(X)$.
2. There exists a function $f:\bbr^n \to \bbr^n$ such that $X = f \circ Y$.
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
Suppose that $\sigma(Y)$ refines $\sigma(X)$ and write

$$
X = \sum_{i=1}^m x_i I_{A_i} \quad \text{and} \quad Y = \sum^n_{j=1} y_j I_{B_j},
$$

where $I_{A_i}$ and $I_{B_j}$ are indicator functions for the level sets

$$
A_i = X^{-1}(x_i) \quad \text{and} \quad B_j = Y^{-1}(y_j).
$$

Then every level set $B_j$ is contained in a unique level set $A_i$, and we may define a function 

$$
h:\{y_1,\ldots,y_n\} \to \bbr
$$

by setting $h(y_j) = x_i$ if $B_j \subset A_i$. Then, define

$$
\pi: \bbr \to \bbr, \quad \pi(y) = \sum_{j=1}^n \delta_{y_j}(y),
$$

where $\delta_{y_j}$ is a Dirac delta function. If we then set $f = h \circ \pi$ and choose $\omega$ in an arbitrary $B_j$, we have

$$
f(Y(\omega)) = (h\circ \pi)(y_j) = h(y_j) = x_i = X(\omega).
$$

Since $B_j$ and $\omega$ were chosen arbitrarily, this proves that (1) implies (2).

To prove the converse, suppose that $X = f \circ Y$ for some function $f:\bbr \to \bbr$. To prove that $\sigma(Y)$ refines $\sigma(X)$, by @thm-rv-refine it will suffice to show that every level set of $Y$ is  contained in a level set of $X$. So, let $y \in \bbr$ and consider the level set $B = Y^{-1}(y)$. If $B$ is nonempty, then for every $\omega \in B$, we have $X(\omega) = f(Y(\omega)) = f(y)$. Thus, $B \subset X^{-1}(f(y))$, and the proof is complete.
:::

Returning again to our coin-flipping example, we see that according to the theorem $Y$ is supposed to be a function of the random vector $(X_1, X_2,X_3)$, since we observed that $\calF_3 = \sigma(X_1, X_2, X_3)$ refines $\sigma(Y)$. Indeed, we *defined* $Y$ to be a function of this random vector via the equation

$$
Y = X_1 + X_2 + X_3,
$$

so $Y = f \circ (X_1,X_2,X_3)$ where $f$ is summation.

## Hilbert spaces of random variables

We've seen how random variables encode information through the algebras they generate. But random variables also have numerical values, which allows us to perform algebraic operations: we can add them, multiply them by scalars, and compute their expected values. This algebraic structure becomes particularly rich when we introduce an inner product, turning the space of random variables into a Hilbert space. This latter structure provides a geometric perspective on random variables, where concepts like orthogonality, projection, and distance become meaningful. This perspective will prove essential for understanding conditional expectation, which emerges naturally as orthogonal projection onto subspaces determined by algebras.

First, we recall that the pointwise sum and scalar multiple of random variables are again random variables, and these operations have the usual properties of commutativity, associativity, *etc*. Thus, the set of all random variables on a finite set $\Omega$ forms a vector space over the real numbers $\bbr$. The subspaces of this vector space that are most relevant to us are those consisting of random variables that are measurable with respect to a given algebra of sets.

We shall also need the concept of *expectation* of a measurable random variable. If $(\Omega,\calF,P)$ is a finite probability space and $X:\Omega \to \bbr$ is a random variable, then the *expectation* of $X$ is defined as

$$
E(X) \defeq \int_\Omega X \ dP,
$$ {#eq-def-expectation}

where the integral on the right is the Lebesgue integral with respect to the probability measure $P$. In the finite setting, this integral reduces to the familiar weighted average of the values of the random variable, where the weights are the probabilities of the level sets of $X$:

::: {.column-margin}
The benefit of using the Lebesgue integral ([-@eq-def-expectation]) to define expectation is that it generalizes directly to "continuous" sample spaces equipped with probability measures on $\sigma$-algebras. In that more general setting, expectation cannot be defined as a simple finite weighted sum as in ([-@eq-expectation-finite]), though it remains a Lebesgue integral. Thus, the Lebesgue integral allows us to treat both discrete and continuous probability theory in a unified manner.
:::

$$
\int_\Omega X \ dP = \sum_{i=1}^m x_i P(A_i),
$$ {#eq-expectation-finite}

where $A_1, A_2, \ldots, A_m$ are the level sets of $X$ and $x_i$ is the value of $X$ on $A_i$. Note that in order for the probability $P(A_i)$ to be defined, we must have $A_i \in \calF$, which is guaranteed if $X$ is $\calF$-measurable.

Finally, two $\calF$-measurable random variables $X$ and $Y$ on $(\Omega,\calF,P)$ will be said to be *equal almost surely* if

$$
P(\{\omega \in \Omega : X(\omega) \neq Y(\omega)\}) = 0.
$$

Almost sure equality is an equivalence relation on the set of $\calF$-measurable random variables.

With these ingredients in place, we may now define the $L^2$-space associated to an algebra of sets on a finite sample space.

::: {#def-L2}
::: {.column-margin}
The $L^2$ notation is borrowed from the case of a general sample space $\Omega$ equipped with a probability measure $P$ on a $\sigma$-algebra $\calF$. In that case, $L^2(\Omega,\calF,P)$ consists of all (equivalence classes of) $\calF$-measurable random variables $X:\Omega \to \bbr$ such that $E(X^2) < \infty$. In our finite setting, this condition on the expectation of $X^2$ is automatically satisfied.
:::
Let $(\Omega,\calF,P)$ be a finite probability space. We write $L^2(\Omega,\calF,P)$ for the set of all $\calF$-measurable random variables $X:\Omega \to \bbr$, where two random variables are identified if they are equal almost surely. When the set $\Omega$ and the measure $P$ are understood from context, we will abbreviate $L^2(\Omega,\calF,P)$ as $L^2(\calF)$.
:::

So, technically $L^2(\calF)$ is a set of equivalence classes of $\calF$-measurable random variables, but for ease of exposition and notation we will often refer to elements of $L^2(\calF)$ as if they were random variables themselves. This will not cause too much confusion.

One should check that $L^2(\calF)$ is indeed a vector subspace of the vector space of *all* random variables on $\Omega$, which amounts to checking that sums and scalar multiples of $\calF$-measurable random variables are again $\calF$-measurable. We will leave this task to the interested reader.

Beyond inducing equivalence classes under almost sure equality, the probability measure $P$ plays no role in the definition of $L^2(\calF)$ itself. However, it does define an inner product on this space via expectation:

::: {#thm-inner-prod}
## $L^2$-spaces are Hilbert spaces
::: {.column-margin}
A *Hilbert space* is a complete inner product space. Since $L^2(\calF)$ is finite-dimensional (see below), it is automatically complete, so the existence of an inner product suffices to make it a Hilbert space.
:::
Let $(\Omega,\calF,P)$ be a finite probability space. The map
$$
\langle -, - \rangle : L^2(\calF) \times L^2(\calF) \to \bbr, \quad \langle X, Y \rangle = E(XY)
$$
defines an inner product on $L^2(\calF)$.
:::

Using the Lebesgue integral notation, the inner product can be written as

$$
\langle X, Y \rangle = \int_\Omega XY \ dP.
$$ {#eq-inner-prod-integral}

But if we write $X$ and $Y$ as linear combinations of indicator functions,

$$
X = \sum_{i=1}^m x_i I_{A_i} \quad \text{and} \quad Y = \sum_{j=1}^n y_j I_{B_j}
$$

where $A_i$ and $B_j$ are the level sets of $X$ and $Y$, respectively, then ([-@eq-inner-prod-integral]) becomes

$$
\langle X, Y \rangle = \sum_{i,j} x_i y_j P(A_i \cap B_j).
$$

One should check that $\langle -, -\rangle$ satisfies the properties of an inner product: linearity in the first argument, symmetry, and positive-definiteness. Again, we shall leave this task to the interested reader. We only note that the proof of positive-definiteness involves showing that $\langle X,X\rangle=0$ if and only if $X$ is almost surely equal to the zero random variable. Remember that elements in $L^2(\calF)$ are equivalence classes under almost sure equality!

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center

# Define transition probabilities for a two-state Markov chain
# alpha: probability of staying in state 1 given current state is 1
# beta: probability of transitioning to state 1 given current state is 0
# theta: initial probability of starting in state 1
alpha <- 0.7
beta <- 0.3
theta <- 0.5

# Compute transition probability P(Y | X) for the Markov chain
# Y: next state (0 or 1)
# X: current state (0 or 1)
trans_prob <- function(Y, X) {
  case_when(
    Y == 0 & X == 0 ~ 1 - beta, # Stay in state 0 given state 0
    Y == 0 & X == 1 ~ 1 - alpha, # Transition to state 0 given state 1
    Y == 1 & X == 0 ~ beta, # Transition to state 1 given state 0
    Y == 1 & X == 1 ~ alpha, # Stay in state 1 given state 1
  )
}

# Compute initial probability distribution
# P(X = 1) = theta, P(X = 0) = 1 - theta
init_prob <- function(X) theta^X * (1 - theta)^(1 - X)

# Compute joint probability P(X1, X2, X3) for a three-step Markov chain
# Uses chain rule: P(X1, X2, X3) = P(X1) * P(X2 | X1) * P(X3 | X2)
joint_prob <- function(X1, X2, X3) {
  init_prob(X1) * trans_prob(X2, X1) * trans_prob(X3, X2)
}

# Add joint probability column to the sample space data frame
# Each row gets its probability under the Markov chain model
Omega %>%
  mutate(p = joint_prob(X1, X2, X3))

```


The inner product induces a norm

$$
\|X\|_2 = \sqrt{\langle X, X \rangle} = \sqrt{E(X^2)},
$$

which measures the "magnitude" of a random variable. This is precisely the so-called *$L^2$-norm*, and the corresponding metric $d(X, Y) = \|X - Y\|$ measures the mean-squared distance between random variables. In statistics, this is the *root mean squared error* (*RMSE*) between $X$ and $Y$.

When algebras are nested, their corresponding $L^2$-spaces are nested as well:

::: {#thm-nested-subspaces}
Let $(\Omega, \mathcal{F}, P)$ be a finite probability space. If $\mathcal{G} \subset \mathcal{H}$ are two subalgebras of $\mathcal{F}$, then
$$
L^2(\mathcal{G}) \subset L^2(\mathcal{H}) \subset L^2(\mathcal{F}).
$$

Moreover, these subset inclusions are ones of vector subspaces.
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
Since $\mathcal{G} \subset \mathcal{H}$, every $\mathcal{G}$-measurable random variable is also $\mathcal{H}$-measurable. Thus, $L^2(\mathcal{G}) \subset L^2(\mathcal{H})$. A symmetric argument shows that $L^2(\mathcal{H}) \subset L^2(\mathcal{F})$. The fact that these inclusions are ones of vector subspaces follows from the fact that sums and scalar multiples of measurable random variables are measurable with respect to the same algebra.
:::

This geometric picture connects directly to information refinement: as algebras refine (capturing more information), their $L^2$-spaces expand in kind (allowing more functions to be represented). Applied to our coin-flip example the nested sequence

$$
\mathcal{F}_1 \subset \mathcal{F}_2 \subset \mathcal{F}_3
$$

induces a nested sequence

$$
L^2(\mathcal{F}_1) \subset L^2(\mathcal{F}_2) \subset L^2(\mathcal{F}_3)
$$

of $L^2$-spaces, where each larger subspace represents more refined information about the outcomes. 

Finally, we note that the $L^2$-spaces associated to algebras have particularly simple and useful orthogonal bases, consisting of indicator functions of the atoms of the algebra. Issues arise when atoms have zero probability, but these are easily handled by working with equivalence classes under almost sure equality.

::: {#thm-basis-L2}
## Bases of $L^2$-spaces
Let $(\Omega,\calF,P)$ be a finite probability space, let $\calG$ be a subalgebra of $\calF$, and let $B_1, B_2, \ldots, B_m$ be the atoms of $\calG$ with positive probability.

1. The indicator functions $I_{B_1}, I_{B_2}, \ldots, I_{B_m}$ form an orthogonal basis of $L^2(\calG)$.

2. The $L^2$-norm of $I_{B_i}$ is $\sqrt{P(B_i)}$.

3. The vector space dimension of $L^2(\calG)$ is equal to the number of atoms in $\calG$ with positive probability.
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
Suppose that we write

$$
B_1,B_2,\ldots,B_m, B_{m+1},B_{m+2},\ldots,B_n,
$$

where the sets $B_{m+1},B_{m+2},\ldots,B_n$ are the atoms of $\calG$ with zero probability (which may not exist). Let $X\in L^2(\calG)$ be a random variable. Since $X$ is $\calG$-measurable, it is constant on each atom in $\calG$. Thus, we may write

$$
X = \sum_{i=1}^n x_i I_{B_i},
$$

for some values $x_i \in \bbr$ (not necessarily distinct). However, since $P(B_i) = 0$ for $i = m+1, m+2, \ldots, n$, we have

$$
X = \sum_{i=1}^m x_i I_{B_i}
$$

almost surely. Thus, since elements in $L^2(\calG)$ are technically equivalence classes under almost sure equality, we have that $X$ is a linear combination of the indicator functions $I_{B_1}, I_{B_2}, \ldots, I_{B_m}$. Thus, these indicator functions span $L^2(\calG)$. To see that they are orthogonal, we suppose $i\neq j$. Then, $B_i \cap B_j =\emptyset$, and so

$$
\langle I_{B_i}, I_{B_j} \rangle = E(I_{B_i} I_{B_j}) = E(I_{B_i\cap B_j})= E(0) = 0.
$$

Orthogonality implies linear independence, so the proof is complete.
:::



## Conditional expectations as projections

::: {#def-conditional-expectation}
Let $(\Omega, \mathcal{F}, P)$ be a finite probability space, $\mathcal{G} \subset \mathcal{F}$ a subalgebra, and $X \in L^2(\mathcal{F})$ a random variable. A *conditional expectation* of $X$ given $\mathcal{G}$ is a random variable $E(X\mid \calG) \in L^2(\mathcal{G})$ such that

$$
\int_B X \ dP = \int_B E(X \mid \mathcal{G}) \ dP.
$$ {#eq-averaging-property}

for all $B \in \mathcal{G}$. This is called the *averaging property* of conditional expectation.
:::

Any two conditional expectations of $X$ given $\calG$ are almost surely equal, so the conditional expectation is unique up to almost sure equality. A particular choice of conditional expectation is called a *version*.

The averaging property ([-@eq-averaging-property]) defines conditional expectation abstractly, but doesn't immediately suggest how to compute it. The key insight is that this property has a geometric interpretation. To uncover it, we rewrite the averaging property as

$$
\int_\Omega \left[X - E(X \mid \mathcal{G})\right] I_B \ dP = 0,
$$

which, using the inner product from @thm-inner-prod, becomes

$$
\langle X - E(X \mid \mathcal{G}), I_B \rangle = 0.
$$

Since the indicator functions $\{I_B : B \in \mathcal{G}\}$ span $L^2(\mathcal{G})$ (by @thm-basis-L2), the residual $X - E(X \mid \mathcal{G})$ is orthogonal to every element of $L^2(\mathcal{G})$. Thus, conditional expectation $E(X\mid \calG)$ is characterized as the orthogonal projection of $X$ onto the subspace $L^2(\mathcal{G})$.

Computing this projection is straightforward. Let $B_1,B_2,\ldots,B_m$ be the atoms of $\calG$ with positive probability. By @thm-basis-L2, the normalized indicator functions

$$
\frac{I_{B_1}}{\sqrt{P(B_1)}}, \frac{I_{B_2}}{\sqrt{P(B_2)}}, \ldots, \frac{I_{B_m}}{\sqrt{P(B_m)}}
$$

form an orthonormal basis of $L^2(\calG)$. The conditional expectation of $X$ given $\calG$ is the orthogonal projection onto this basis:

$$
E(X\mid \calG) = \sum_{i=1}^m \left\langle X, \frac{I_{B_i}}{\sqrt{P(B_i)}} \right\rangle \frac{I_{B_i}}{\sqrt{P(B_i)}}.
$$

Simplifying using the definition of the inner product yields

$$
E(X\mid \calG) = \sum_{i=1}^m \frac{1}{P(B_i)}\left(\int_{B_i} X \ dP \right)I_{B_i}.
$$ {#eq-cond-exp-compute}

To evaluate the integrals in this formula, we write $X$ in terms of its level sets:

$$
X = \sum_{j=1}^n x_j I_{A_j},
$$

where $A_1,A_2,\ldots,A_n$ are the level sets of $X$. Then

$$
\int_{B_i} X \ dP = \sum_{j=1}^n x_j P(A_j \cap B_i).
$$

Substituting into ([-@eq-cond-exp-compute]) gives an explicit formula in terms of conditional probabilities:

$$
E(X\mid \calG) = \sum_{i=1}^m \left(\sum_{j=1}^n x_j P(A_j \mid B_i)\right)I_{B_i}
$$ {#eq-cond-exp-compute-2}

where $P(A_j \mid B_i) = P(A_j \cap B_i)/P(B_i)$ is the conditional probability of $A_j$ given $B_i$. This formula connects our geometric framework to the elementary definition of conditional probability.

We summarize this projection interpretation in the following theorem:

::: {#thm-cond-exp-proj}
## Conditional expectation as orthogonal projection
Let $(\Omega, \mathcal{F}, P)$ be a finite probability space, $\mathcal{G} \subset \mathcal{F}$ a subalgebra, and $X \in L^2(\mathcal{F})$ a random variable. Then, the orthogonal projection of $X$ onto the subspace $L^2(\mathcal{G})$ is a conditional expectation of $X$ given $\mathcal{G}$. This projection is given by @eq-cond-exp-compute-2.
:::

So, given an $\calF$-measurable random variable $X$ and a coarser algebra $\mathcal{G}$, we may trivially write

$$
X = E(X\mid \calG) + \left[X - E(X \mid \mathcal{G})\right].
$$

This decomposition gives us two components: a $\mathcal{G}$-measurable part $E(X\mid \calG)$ that captures all information in $X$ knowable from $\mathcal{G}$, and a residual $X - E(X \mid \mathcal{G})$ orthogonal to every $\calG$-measurable random variable, representing pure noise relative to $\calG$.

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center

# Set theta (probability of heads for each flip)
theta <- 0.6

# Add probability measure: p = theta^Y * (1-theta)^(3-Y)
Omega <- Omega %>%
  mutate(p = theta^Y * (1 - theta)^(3 - Y))

# Compute E[Y | F1]
E_Y_given_F1 <- Omega %>%
  group_by(Ai) %>%
  mutate(
    pAi = sum(p),
    p_cond = p / pAi
  ) %>%
  summarize(E_Y_given_F1 = sum(Y * p_cond))

kable(E_Y_given_F1, caption = "$E(Y \\mid \\mathcal{F}_1)$")

# Compute E[Y | F2]
E_Y_given_F2 <- Omega %>%
  group_by(Bi) %>%
  mutate(
    pBi = sum(p),
    p_cond = p / pBi
  ) %>%
  summarize(E_Y_given_F2 = sum(Y * p_cond))

kable(E_Y_given_F2, caption = "$E(Y \\mid \\mathcal{F}_2)$")

# Compute E[Y | F3]
# Note: Should equal Y since Y is F3-measurable
E_Y_given_F3 <- Omega %>%
  group_by(Ci) %>%
  mutate(
    pCi = sum(p),
    p_cond = p / pCi
  ) %>%
  summarize(E_Y_given_F3 = sum(Y * p_cond))

kable(E_Y_given_F3, caption = "$E(Y \\mid \\mathcal{F}_3)$")

```

<br>

In the case that $\calG = \{\emptyset, \Omega\}$ is the trivial algebra, we have

$$
E(X \mid \calG) = E(X),
$$

the constant random variable (taking value $E(X)$ everywhere). The trivial algebra represents having no information at all, so the best prediction of $X$ without any information is simply its expectation.

The fact that $E(X \mid \mathcal{G})$ is the "best" $\mathcal{G}$-measurable approximation to $X$ can be made numerically precise, as in the following theorem:

::: {#thm-best-approx}
## Best $L^2$-approximation
Let $(\Omega, \mathcal{F}, P)$ be a finite probability space, $\mathcal{G} \subset \mathcal{F}$ a subalgebra, and $X \in L^2(\mathcal{F})$ a random variable. Then,

$$
E\left[(X - E(X \mid \mathcal{G}))^2\right] = \min_{Y \in L^2(\mathcal{G})} E\left[(X - Y)^2\right].
$$
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
This follows directly from the fact that orthogonal projection minimizes distance in inner product spaces. See, for example, Proposition 6.36 in [@Axler1997].
:::

Beyond optimality, the projection interpretation of conditional expectation immediately yields several other important properties, which we now develop.

::: {#thm-linearity}
## Linearity of conditional expectation
Let $(\Omega, \mathcal{F}, P)$ be a finite probability space, $\mathcal{G} \subset \mathcal{F}$ a subalgebra, and $X, Y \in L^2(\mathcal{F})$ random variables. For any scalars $a,b \in \bbr$, we have
$$
E(aX + bY \mid \mathcal{G}) = a E(X \mid \mathcal{G}) + b E(Y \mid \mathcal{G})
$$

almost surely.
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
Choose the version of conditional expectation given by orthogonal projection onto $L^2(\calG)$ as in @thm-cond-exp-proj. Since orthogonal projection is linear, the desired result follows immediately.
:::

Linearity tells us that conditional expectation behaves well under linear combinations. But what happens when we condition on the product $XY$ of two random variables? If $Y$ is already $\mathcal{G}$-measurable, then $Y$ is "known" given the information in $\mathcal{G}$, so it should factor out of the conditional expectation. This intuition is formalized by the following property:

::: {#thm-pull-out}
## Pull-out property
Let $(\Omega, \mathcal{F}, P)$ be a finite probability space, $\mathcal{G} \subset \mathcal{F}$ a subalgebra, and $X \in L^2(\mathcal{F})$ a random variable. If $Y$ is a $\calG$-measurable random variable, 

$$
E(XY \mid \mathcal{G}) = Y E(X \mid \mathcal{G}).
$$

:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
We will prove that the random variable $Y E(X \mid \mathcal{G})$ has the defining property of a conditional expectation of $XY$ given $\calG$. To this end, we let $B$ be a set in $\calG$. Then $YI_B$ is $\calG$-measurable, so that if we choose the version of $E(X\mid \calG)$ given by orthogonal projection onto $L^2(\calG)$  as in @thm-cond-exp-proj, we must have

$$
\langle X - E(X \mid \mathcal{G}), Y I_B \rangle = 0.
$$

However, this inner product is

$$
\langle X - E(X \mid \mathcal{G}), Y I_B \rangle = \int_B \left[XY - Y E(X \mid \mathcal{G})\right] \ dP,
$$

so we get

$$
\int_B XY \ dP = \int_B Y E(X \mid \mathcal{G}) \ dP.
$$

This completes the proof.
:::

The pull-out property shows that $\mathcal{G}$-measurable factors can be extracted from conditional expectations. But what happens when we have a nested sequence of algebras $\mathcal{G} \subset \mathcal{H} \subset \mathcal{F}$, representing increasingly refined information states? 

Intuitively, if we first condition on the finer information $\mathcal{H}$ and then "forget" some information by conditioning on the coarser $\mathcal{G}$, we should get the same result as conditioning directly on $\mathcal{G}$. This is the content of the following property (also called the *law of iterated expectations*):

::: {#thm-tower}
## Tower property
Let $(\Omega, \mathcal{F}, P)$ be a finite probability space, and let $\mathcal{G} \subset \mathcal{H} \subset \mathcal{F}$ be subalgebras. If $X \in L^2(\mathcal{F})$ is a random variable, then 

$$
E(E(X \mid \mathcal{H}) \mid \mathcal{G}) = E(X \mid \mathcal{G}).
$$ {#eq-tower}

:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
We have the inclusion of vector subspaces

$$
L^2(\mathcal{G}) \subset L^2(\mathcal{H}) \subset L^2(\mathcal{F}).
$$

If we choose the versions of the conditional expectations given by orthogonal projection  as in @thm-cond-exp-proj, then the left-hand side of ([-@eq-tower]) is the orthogonal projection of $X$ onto $L^2(\mathcal{H})$ followed by the orthogonal projection onto $L^2(\mathcal{G})$. However, since the subspaces are nested, this composition is simply the orthogonal projection of $X$ onto $L^2(\mathcal{G})$, which is the right-hand side of the desired equation.
:::

We now apply these abstract results to our running example of three coin flips. This will make the geometric picture concrete and show how conditional expectations improve predictions as information refines.

Recall our nested sequence of algebras:
$$
\mathcal{F}_1 = \sigma(X_1) \subset \mathcal{F}_2 = \sigma(X_1, X_2) \subset \mathcal{F}_3 = \sigma(X_1, X_2, X_3),
$$
where $\mathcal{F}_1$ has 2 atoms, $\mathcal{F}_2$ has 4 atoms, and $\mathcal{F}_3$ has 8 atoms. We'll compute conditional expectations of $Y = X_1 + X_2 + X_3$ (total number of heads) given each algebra.

**Predicting with no information: $E(Y)$**

With no information about any flips, the best prediction is the unconditional expectation:
$$
E(Y) = 0 \cdot P(Y=0) + 1 \cdot P(Y=1) + 2 \cdot P(Y=2) + 3 \cdot P(Y=3)
$$

This is a constant function on $\Omega$—the same prediction for all outcomes.

**Predicting after the first flip: $E(Y \mid \mathcal{F}_1)$**

By @thm-basis-L2, $E(Y \mid \mathcal{F}_1)$ must be constant on each atom of $\mathcal{F}_1$. The atoms are:
- $A_1 = \{000, 001, 010, 011\}$ (first flip is 0)
- $A_2 = \{100, 101, 110, 111\}$ (first flip is 1)

By the averaging property, on $A_1$:
$$
E(Y \mid \mathcal{F}_1)|_{A_1} = \frac{\int_{A_1} Y \, dP}{P(A_1)} = E(Y \mid X_1 = 0) = 0 + E(X_2) + E(X_3) = 2\theta
$$

Similarly, on $A_2$:
$$
E(Y \mid \mathcal{F}_1)|_{A_2} = E(Y \mid X_1 = 1) = 1 + E(X_2) + E(X_3) = 1 + 2\theta
$$

Thus:
$$
E(Y \mid \mathcal{F}_1) = 2\theta \cdot I_{A_1} + (1 + 2\theta) \cdot I_{A_2}
$$

This shows how the prediction updates based on the first flip: if $X_1 = 0$, we predict $2\theta$ more heads; if $X_1 = 1$, we predict $1 + 2\theta$ total heads.

**The tower property in action**

We can verify the tower property by computing:
$$
E(E(Y \mid \mathcal{F}_2) \mid \mathcal{F}_1)
$$

By @thm-tower, this should equal $E(Y \mid \mathcal{F}_1)$. We'll verify this computationally in the code below.

[Add R code computing these conditional expectations and showing the tower property]


## Conclusion

## Bibliography