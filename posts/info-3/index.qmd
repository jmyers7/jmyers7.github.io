---
title: "From coin flips to measure theory: a geometric introduction"
date: "2025-10-17"
bibliography: /Users/johnmyers/Documents/work/johnmyers-phd/aux-files/references.bib
toc: true
categories: [Information theory, Probability theory, Measure theory, Sigma-algebras, Conditional expectations, Hilbert spaces, Linear algebra, R]
image: "thumbnail.svg"
draft: true
---

::: {.content-hidden}
$$
{{< include /aux-files/custom.tex >}}
$$
:::

## Introduction

This is the third post in the series on information theory.

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center

# Load required packages
library("ggplot2") # For plotting
library("knitr") # For table formatting (used with kable())
library("dplyr") # For data manipulation
library("latex2exp") # For LaTeX rendering in plots

# Source custom theme file
# Note: Replace this path with your own theme file location
source("../../aux-files/custom-theme.R")

# Extract custom colors from theme
# Note: These color definitions depend on your custom theme file
yellow <- custom_colors[["yellow"]]
blue <- custom_colors[["blue"]]

# Apply custom ggplot2 theme
theme_set(custom_theme())

```

## Random variables as information carriers

First, we set the scene and recall a few basic concepts from the previous post. We let $\Omega$ be a finite set, conceptualized as the set of outcomes of an experiment. An *algebra of sets* $\calF$ in $\Omega$ is a collection of subsets of $\Omega$ that contains $\Omega$ and is closed under complements and finite unions. Algebras capture information about which events can be observed in the experiment: if $A \in \calF$, then the event $A$ is observable, meaning that an observer can determine whether the outcome $\omega \in A$ or not. Algebras can be partially ordered by the refinement relation: an algebra $\calF$ *refines* another algebra $\calG$ if $\calG \subset \calF$. In this case, $\calF$ contains at least as much information as $\calG$, since every event observable in $\calG$ is also observable in $\calF$.

As we will see in this section, functions on $\Omega$ also carry information. In particular, *random variables* are functions $X:\Omega \to \bbr$ that assign a real number to each outcome $\omega \in \Omega$. They induce algebras through their *level sets*, which are the preimages of singletons in $\bbr$. In what follows, it will be convenient to allow the codomain of $X$ to be an arbitrary finite-dimensional euclidean space $\bbr^n$, not just $\bbr$, in which case a function $X:\Omega \to \bbr^n$ is often called a *random vector*.

::: {#def-rv-part}
Let $X:\Omega\to \bbr^n$ be a random vector on a finite set $\Omega$. For $x\in \bbr^n$, sets of the form

$$
X^{-1}(x) \defeq \{ \omega \in \Omega : X(\omega)=x\}
$$

are called *level sets*.
:::

As you may easily check, the nonempty level sets of a random vector $X$ partition its domain, and through this partition the random vector generates an algebra of sets in $\Omega$ denoted $\sigma(X)$. The level sets are the *atoms* of this algebra, i.e., they are the minimal nonempty sets in $\sigma(X)$. The algebra $\sigma(X)$ captures precisely the information an observer gains by learning the value of $X$. If the observer knows $X(\omega) = x$, they know that $\omega \in X^{-1}(x)$ but cannot distinguish between outcomes within this level set. Thus, $\sigma(X)$ represents the coarsest partition of $\Omega$ consistent with the information provided by $X$.

For an example, we consider the experiment of flipping a coin three times that we introduced in the [previous post](../info-2/). The sample space is

$$
\Omega = \{000, 001, 010, 011, 100, 101, 110, 111\},
$$

where a binary string represents the results of the three flips, with a $0$ representing tails and a $1$ representing heads. We define three random variables

$$
X_1, X_2, X_3:\Omega \to \{0,1\}
$$

that record the results of the first, second, and third flips, respectively. For instance, $X_1(010) = 0$ since the first flip in the sequence $010$ is tails. We introduce a fourth random variable

$$
Y:\Omega \to \{0,1,2,3\}
$$

that records the total number of heads across all three flips, i.e.,

$$
Y = X_1 + X_2 + X_3.
$$

The following data frame displays the sample space $\Omega$ along with the values of the four random variables.

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center

# Create the sample space Omega as a data frame
# Each row represents one of the 8 possible outcomes (binary sequences of length 3)
Omega <- data.frame(
  X1 = c(0, 0, 0, 0, 1, 1, 1, 1), # First coin flip: 0 = tail, 1 = head
  X2 = c(0, 0, 1, 1, 0, 0, 1, 1), # Second coin flip
  X3 = c(0, 1, 0, 1, 0, 1, 0, 1) # Third coin flip
)

# Add random variable Y: total number of heads across all three flips
Omega <- Omega %>%
  mutate(Y = X1 + X2 + X3)

# Display the sample space as a formatted table
kable(
  Omega,
  caption = "sample space $\\Omega$ with random variables $X_1, X_2, X_3$, and $Y$"
)
```

<br>

Notice that the two level sets of $X_1$ are

$$
\begin{align*}
A_1 &= X_1^{-1}(0) = \{000, 001, 010, 011\}, \\ 
A_2 &= X_1^{-1}(1) = \{100, 101, 110, 111\}.
\end{align*}
$$

The reader will note that these are exactly the atoms of the algebra $\calF_1$ we considered in the [previous post](../info-2/). Thus, $\sigma(X_1) = \calF_1$.

Similarly, the level sets of $X_2$ and $X_3$ are

$$
\begin{align*}
X_2^{-1}(0) &= \{000, 001, 100, 101\}, & X_2^{-1}(1) &= \{010, 011, 110, 111\}, \\ 
X_3^{-1}(0) &= \{000, 010, 100, 110\}, & X_3^{-1}(1) &= \{001, 011, 101, 111\}.
\end{align*}
$$

If we form all pairwise intersections of the level sets of $X_1$ and $X_2$, we obtain the sets

$$
B_1 = \{000,001\}, \quad B_2 = \{010,011\}, \quad B_3 = \{100,101\}, \quad B_4 = \{110,111\},
$$

which were the atoms of the algebra $\calF_2$ from the [previous post](../info-2/). In the notation to be introduced below (see @def-algebra-gen-rv), this means that

$$
\sigma(X_1, X_2) = \calF_2,
$$

or that $\calF_2$ is the algebra generated by $X_1$ and $X_2$. If we form all $3$-fold intersections of the level sets of $X_1$, $X_2$, and $X_3$, we obtain the singletons

$$
\begin{gather*}
C_1 = \{000\}, \ C_2 = \{001\}, \ C_3 = \{010\}, \ C_4 = \{011\}, \\
C_5 = \{100\}, \ C_6 = \{101\}, \ C_7 = \{110\}, \ C_8 = \{111\},
\end{gather*}
$$

which were the atoms of the algebra $\calF_3$ from the [previous post](../info-2/). Again, in the notation to be introduced below (see @def-algebra-gen-rv), this means that

$$
\sigma(X_1, X_2, X_3) = \calF_3,
$$

or that $\calF_3$ is the algebra generated by $X_1, X_2,$ and $X_3$. Thus, the nested sequence of algebras

$$
\calF_1 \subset \calF_2 \subset \calF_3
$$

may be rewritten as

$$
\sigma(X_1) \subset \sigma(X_1, X_2) \subset \sigma(X_1, X_2, X_3).
$$

Finally, the level sets of $Y$ are

$$
\begin{align*}
D_1 &= Y^{-1}(0) = \{000\}, \\
D_2 &= Y^{-1}(1) = \{001,010,100\}, \\
D_3 &= Y^{-1}(2) = \{011,101,110\}, \\
D_4 &= Y^{-1}(3) = \{111\}.
\end{align*}
$$

In the [previous post](../info-2/), we had columns in our data frame indicating which outcomes belonged to which atoms $A_i$, $B_i$, and $C_i$; let's add these columns here as well, along with columns for the $D_i$:

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center

# Group outcomes by the result of the first flip (X1)
# and assign a group ID to track which partition set (A1 or A2) each outcome belongs to
Omega <- Omega %>%
  group_by(X1) %>%
  mutate(Ai = cur_group_id()) %>% # Ai = 1 for A1 (X1=0), Ai = 2 for A2 (X1=1)
  ungroup() %>%
  as.data.frame()

# Group outcomes by the results of the first two flips (X1 and X2)
# and assign a group ID to track which partition set (B1, B2, B3, or B4) each outcome belongs to
Omega <- Omega %>%
  group_by(X1, X2) %>%
  mutate(Bi = cur_group_id()) %>% # Bi = 1 for B1, Bi = 2 for B2, etc.
  ungroup() %>%
  as.data.frame()

# Group outcomes by all three flips (X1, X2, and X3)
# Each combination of (X1, X2, X3) uniquely identifies one outcome
Omega <- Omega %>%
  group_by(X1, X2, X3) %>%
  mutate(Ci = cur_group_id()) %>% # Ci = 1 for C1, Ci = 2 for C2, etc.
  ungroup() %>%
  as.data.frame()

Omega <- Omega %>%
  group_by(Y) %>%
  mutate(Di = cur_group_id()) %>% # Di = 1 for D1, Di = 2 for D2, etc.
  ungroup() %>%
  as.data.frame()

# Display a formatted table showing the sample space Omega with random variables X1, X2, X3, and Y, including atoms Ai, Bi, Ci, and Di.
kable(
  Omega,
  caption = "sample space $\\Omega$ with random variables $X_1, X_2, X_3$, and $Y$, along with atoms $A_i$, $B_i$, $C_i$, and $D_i$"
)

```

<br>

We will return to this data frame momentarily, but first we introduce some notation for algebras generated by multiple random vectors. Above, we've seen algebras generated by one, two, and three random variables. In fact, we may form algebras generated by any finite number of random variables, as follows.

::: {#def-algebra-gen-rv}
Let $X_1,X_2,\ldots,X_m$ be random vectors on a finite set $\Omega$. The *algebra generated* by $X_1,X_2,\ldots,X_m$ is the algebra of sets in $\Omega$, denoted by $\sigma(X_1,X_2,\ldots,X_m)$, whose atoms are all nonempty intersections of the form 
$$
\bigcap_{i=1}^m X_i^{-1}(x_i),
$$

where $x_i$ is a value of $X_i$ for each $i$.
:::

In fact, if we are given a finite set of random variables $X_1,X_2,\ldots,X_m$ as in the theorem, then we may form the random vector

$$
X: \Omega \to \bbr^m, \quad X(\omega) = (X_1(\omega), X_2(\omega), \ldots, X_m(\omega)).
$$

Then, as you may easily check, the algebra generated by $X_1,X_2,\ldots,X_m$ is simply the algebra generated by the random vector $X$:

$$
\sigma(X_1,X_2,\ldots,X_m) = \sigma(X).
$$

Now, very often the set $\Omega$ comes equipped with a pre-existing algebra $\calF$ of observable events (perhaps as part of a probability space $(\Omega,\calF,P)$). In this case, it is natural to ask how the information encoded in $\sigma(X)$ relates to that in $\calF$. The first possiblity is that neither algebra refines the other, meaning that $\sigma(X)$ and $\calF$ contain different kinds of information. The other possibilities are that one algebra refines the other, meaning that one contains at least as much information as the other. The following theorem characterizes these refinement relations in terms of the behavior of $X$ on the atoms of $\calF$.

::: {#thm-rv-refine}
## Random variables and refinements
Let $\calF$ be an algebra of sets in a finite set $\Omega$, and let $X:\Omega \to \bbr^n$ be a random vector.

1. The algebra $\calF$ refines $\sigma(X)$ (equivalently, $\sigma(X) \subset \mathcal{F}$) if and only if $X$ is constant on every atom in $\calF$.

2. The algebra $\sigma(X)$ refines $\calF$ (equivalently, $\calF \subset \sigma(X)$) if and only if every atom in $\calF$ is a union of level sets of $X$.
:::

::: {.callout-note collapse="true" icon="false"}
## Proof.
The algebra $\calF$ refines $\sigma(X)$ if and only if every nonempty level set is a union of atoms in $\calF$. This proves the result in the first statement. A symmetric argument using the same theorem proves the second statement.
:::

Random vectors satisfying the first condition in @thm-rv-refine arise frequently enough to warrant their own terminology.

::: {#def-measurable}
Let $\calF$ be an algebra of sets in a finite set $\Omega$. A random vector $X$ on $\Omega$ will be called *$\calF$-measurable* if $\calF$ refines the algebra $\sigma(X)$, i.e., $\sigma(X) \subset \calF$.
:::

Returning to our coin-flipping scenario, we see that $Y$ is *only* $\calF_3$-measureable, since the atoms in this algebra are the only ones on which $Y$ is constant. (Of course, $\calF_3$ is the power set of $\Omega$.) The information afforded by $Y$ is not comparable to that of $\calF_1$ or $\calF_2$, since there are no subset containments between $\sigma(Y)$ and $\calF_1$ or $\calF_2$.

If $X$ and $Y$ are both random vectors on $\Omega$, then we may compare the information they encode by comparing their algebras $\sigma(X)$ and $\sigma(Y)$. Then @thm-rv-refine tells us *when* one algebra refines the other, but not *how* the vectors relate. The next theorem provides this missing piece: refinement is equivalent to functional dependence.

::: {#thm-functional-rep}
## Functional representation
Let $X$ and $Y$ be two $\bbr^n$-valued random vectors on a finite set $\Omega$. The following statements are equivalent:

::: {.column-margin}
@thm-functional-rep holds for general measurable spaces, not just finite sets. The proof requires the Doob-Dynkin lemma and properties of $\sigma$-algebras. See Lemma 1.14 in @Kallenberg2021 for the general statement and proof. Our finite setting allows a more elementary approach.
:::

1. The algebra $\sigma(Y)$ refines $\sigma(X)$.
2. There exists a function $f:\bbr^n \to \bbr^n$ such that $X = f \circ Y$.
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
Suppose that $\sigma(Y)$ refines $\sigma(X)$ and write

$$
X = \sum_{i=1}^m x_i I_{A_i} \quad \text{and} \quad Y = \sum^n_{j=1} y_j I_{B_j},
$$

where $I_{A_i}$ and $I_{B_j}$ are indicator functions for the level sets

$$
A_i = X^{-1}(x_i) \quad \text{and} \quad B_j = Y^{-1}(y_j).
$$

Then every level set $B_j$ is contained in a unique level set $A_i$, and we may define a function 

$$
h:\{y_1,\ldots,y_n\} \to \bbr
$$

by setting $h(y_j) = x_i$ if $B_j \subset A_i$. Then, define

$$
\pi: \bbr \to \bbr, \quad \pi(y) = \sum_{j=1}^n \delta_{y_j}(y),
$$

where $\delta_{y_j}$ is a Dirac delta function. If we then set $f = h \circ \pi$ and choose $\omega$ in an arbitrary $B_j$, we have

$$
f(Y(\omega)) = (h\circ \pi)(y_j) = h(y_j) = x_i = X(\omega).
$$

Since $B_j$ and $\omega$ were chosen arbitrarily, this proves that (1) implies (2).

To prove the converse, suppose that $X = f \circ Y$ for some function $f:\bbr \to \bbr$. To prove that $\sigma(Y)$ refines $\sigma(X)$, by @thm-rv-refine it will suffice to show that every level set of $Y$ is  contained in a level set of $X$. So, let $y \in \bbr$ and consider the level set $B = Y^{-1}(y)$. If $B$ is nonempty, then for every $\omega \in B$, we have $X(\omega) = f(Y(\omega)) = f(y)$. Thus, $B \subset X^{-1}(f(y))$, and the proof is complete.
:::

Returning again to our coin-flipping example, we see that according to the theorem $Y$ is supposed to be a function of the random vector $(X_1, X_2,X_3)$, since we observed that $\calF_3 = \sigma(X_1, X_2, X_3)$ refines $\sigma(Y)$. Indeed, we *defined* $Y$ to be a function of this random vector via the equation

$$
Y = X_1 + X_2 + X_3,
$$

so $Y = f \circ (X_1,X_2,X_3)$ where $f$ is summation.

## Hilbert spaces of random variables

We've seen how random variables encode information through the algebras they generate. But random variables also have numerical values, which allows us to perform algebraic operations: we can add them, multiply them by scalars, and compute their expected values. This algebraic structure becomes particularly rich when we introduce an inner product, turning the space of random variables into a Hilbert space. This latter structure provides a geometric perspective on random variables, where concepts like orthogonality, projection, and distance become meaningful. This perspective will prove essential for understanding conditional expectation, which emerges naturally as orthogonal projection onto subspaces determined by algebras.

First, we recall that the pointwise sum and scalar multiple of random variables are again random variables, and these operations have the usual properties of commutativity, associativity, *etc*. Thus, the set of all random variables on a finite set $\Omega$ forms a vector space over the real numbers $\bbr$. The subspaces of this vector space that are most relevant to us are those consisting of random variables that are measurable with respect to a given algebra of sets.

We shall also need the concept of *expectation* of a measurable random variable. If $(\Omega,\calF,P)$ is a finite probability space and $X:\Omega \to \bbr$ is a random variable, then the *expectation* of $X$ is defined as

$$
E(X) \defeq \int_\Omega X \ dP,
$$ {#eq-def-expectation}

where the integral on the right is the Lebesgue integral with respect to the probability measure $P$. In the finite setting, this integral reduces to the familiar weighted average of the values of the random variable, where the weights are the probabilities of the level sets of $X$:

::: {.column-margin}
The benefit of using the Lebesgue integral ([-@eq-def-expectation]) to define expectation is that it generalizes directly to "continuous" sample spaces equipped with probability measures on $\sigma$-algebras. In that more general setting, expectation cannot be defined as a simple finite weighted sum as in ([-@eq-expectation-finite]), though it remains a Lebesgue integral. Thus, the Lebesgue integral allows us to treat both discrete and continuous probability theory in a unified manner.
:::

$$
\int_\Omega X \ dP = \sum_{i=1}^m x_i P(A_i),
$$ {#eq-expectation-finite}

where $A_1, A_2, \ldots, A_m$ are the level sets of $X$ and $x_i$ is the value of $X$ on $A_i$. Note that in order for the probability $P(A_i)$ to be defined, we must have $A_i \in \calF$, which is guaranteed if $X$ is $\calF$-measurable.

Finally, two $\calF$-measurable random variables $X$ and $Y$ on $(\Omega,\calF,P)$ will be said to be *equal almost surely* if

$$
P(\{\omega \in \Omega : X(\omega) \neq Y(\omega)\}) = 0.
$$

Almost sure equality is an equivalence relation on the set of $\calF$-measurable random variables.

With these ingredients in place, we may now define the $L^2$-space associated to an algebra of sets on a finite sample space.

::: {#def-L2}
::: {.column-margin}
The $L^2$ notation is borrowed from the case of a general sample space $\Omega$ equipped with a probability measure $P$ on a $\sigma$-algebra $\calF$. In that case, $L^2(\Omega,\calF,P)$ consists of all (equivalence classes of) $\calF$-measurable random variables $X:\Omega \to \bbr$ such that $E(X^2) < \infty$. In our finite setting, this condition on the expectation of $X^2$ is automatically satisfied.
:::
Let $(\Omega,\calF,P)$ be a finite probability space. We write $L^2(\Omega,\calF,P)$ for the set of all $\calF$-measurable random variables $X:\Omega \to \bbr$, where two random variables are identified if they are equal almost surely. When the set $\Omega$ and the measure $P$ are understood from context, we will abbreviate $L^2(\Omega,\calF,P)$ as $L^2(\calF)$.
:::

So, technically $L^2(\calF)$ is a set of equivalence classes of $\calF$-measurable random variables, but for ease of exposition and notation we will often refer to elements of $L^2(\calF)$ as if they were random variables themselves. This will not cause too much confusion.

One should check that $L^2(\calF)$ is indeed a vector subspace of the vector space of *all* random variables on $\Omega$, which amounts to checking that sums and scalar multiples of $\calF$-measurable random variables are again $\calF$-measurable. We will leave this task to the interested reader.

Beyond inducing equivalence classes under almost sure equality, the probability measure $P$ plays no role in the definition of $L^2(\calF)$ itself. However, it does define an inner product on this space via expectation:

::: {#thm-inner-prod}
## $L^2$-spaces are Hilbert spaces
::: {.column-margin}
A *Hilbert space* is a complete inner product space. Since $L^2(\calF)$ is finite-dimensional (see below), it is automatically complete, so the existence of an inner product suffices to make it a Hilbert space.
:::
Let $(\Omega,\calF,P)$ be a finite probability space. The map
$$
\langle -, - \rangle : L^2(\calF) \times L^2(\calF) \to \bbr, \quad \langle X, Y \rangle = E(XY)
$$
defines an inner product on $L^2(\calF)$.
:::

Using the Lebesgue integral notation, the inner product can be written as

$$
\langle X, Y \rangle = \int_\Omega XY \ dP.
$$ {#eq-inner-prod-integral}

But if we write $X$ and $Y$ as linear combinations of indicator functions,

$$
X = \sum_{i=1}^m x_i I_{A_i} \quad \text{and} \quad Y = \sum_{j=1}^n y_j I_{B_j}
$$

where $A_i$ and $B_j$ are the level sets of $X$ and $Y$, respectively, then ([-@eq-inner-prod-integral]) becomes

$$
\langle X, Y \rangle = \sum_{i,j} x_i y_j P(A_i \cap B_j).
$$

One should check that $\langle -, -\rangle$ satisfies the properties of an inner product: linearity in the first argument, symmetry, and positive-definiteness. Again, we shall leave this task to the interested reader. We only note that the proof of positive-definiteness involves showing that $\langle X,X\rangle=0$ if and only if $X$ is almost surely equal to the zero random variable. Remember that elements in $L^2(\calF)$ are equivalence classes under almost sure equality!

The inner product induces a norm

$$
\|X\| = \sqrt{\langle X, X \rangle} = \sqrt{E(X^2)},
$$

which measures the "magnitude" of a random variable. This is precisely the so-called *$L^2$-norm*, and the corresponding metric $d(X, Y) = \|X - Y\|$ measures the mean-squared distance between random variables. In statistics, this is the *root mean squared error* (*RMSE*) between $X$ and $Y$.

When algebras are nested, their corresponding $L^2$-spaces are nested as well:

::: {#thm-nested-subspaces}
Let $(\Omega, \mathcal{F}, P)$ be a finite probability space. If $\mathcal{G} \subset \mathcal{H}$ are two subalgebras of $\mathcal{F}$, then
$$
L^2(\mathcal{G}) \subset L^2(\mathcal{H}) \subset L^2(\mathcal{F}).
$$

Moreover, these subset inclusions are ones of vector subspaces.
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
Since $\mathcal{G} \subset \mathcal{H}$, every $\mathcal{G}$-measurable random variable is also $\mathcal{H}$-measurable. Thus, $L^2(\mathcal{G}) \subset L^2(\mathcal{H})$. A symmetric argument shows that $L^2(\mathcal{H}) \subset L^2(\mathcal{F})$. The fact that these inclusions are ones of vector subspaces follows from the fact that sums and scalar multiples of measurable random variables are measurable with respect to the same algebra.
:::

This geometric picture connects directly to information refinement: as algebras refine (capturing more information), their $L^2$-spaces expand in kind (allowing more functions to be represented). Applied to our coin-flip example the nested sequence

$$
\mathcal{F}_1 \subset \mathcal{F}_2 \subset \mathcal{F}_3
$$

induces a nested sequence

$$
L^2(\mathcal{F}_1) \subset L^2(\mathcal{F}_2) \subset L^2(\mathcal{F}_3)
$$

of $L^2$-spaces, where each larger subspace represents more refined information about the outcomes. 

Finally, we note that the $L^2$-spaces associated to algebras have particularly simple and useful orthogonal bases, consisting of indicator functions of the atoms of the algebra. Issues arise when atoms have zero probability, but these are easily handled by working with equivalence classes under almost sure equality.

::: {#thm-basis-L2}
## Bases of $L^2$-spaces
Let $(\Omega,\calF,P)$ be a finite probability space and $\calG$ a subalgebra of $\calF$. If $B_1, B_2, \ldots, B_m$ are the atoms of $\calG$ with positive probability, then the indicator functions $I_{B_1}, I_{B_2}, \ldots, I_{B_m}$ form an orthogonal basis of $L^2(\calG)$.
:::
::: {.callout-note collapse="true" icon="false"}
## Proof.
To see that the indicator functions are orthogonal, let $i \neq j$. Then,

$$
\langle  I_{B_i}, I_{B_j} \rangle = E(I_{B_i} I_{B_j}) = E(0) = 0.
$$

To see that the indicator functions span $L^2(\calG)$, let $X \in L^2(\calG)$ and write

$$
X = \sum_{j=1}^n x_j I_{A_j},
$$

where $A_1, A_2, \ldots, A_n$ are the level sets of $X$. Since $X$ is $\calG$-measurable, each level set $A_j$ is a union of atoms in $\calG$. Since we have

$$
I_{U \cup V} = I_U + I_V
$$

for *any* disjoint sets $U$ and $V$, it follows that each indicator function $I_{A_j}$ may be expressed as a sum of indicator functions of atoms in $\calG$. Thus, $X$ may be expressed as a linear combination of the indicator functions $I_{B_1}, I_{B_2}, \ldots, I_{B_m}$.
:::

## Conditional expectations as projections

We now turn to the central geometric construction: conditional expectation. Given a finite sample space $\Omega$, a random variable $X \in L^2(\Omega, \mathcal{F}, P)$, and a coarser algebra $\mathcal{G} \subset \mathcal{F}$, the conditional expectation $E(X \mid \mathcal{G})$ will emerge as the orthogonal projection of $X$ onto the subspace $L^2(\mathcal{G})$. This projection gives the best $\mathcal{G}$-measurable approximation to $X$ in the $L^2$ sense, geometrically realizing the idea of "optimal prediction given partial information."

## Conclusion

## Bibliography