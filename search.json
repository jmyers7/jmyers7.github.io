[
  {
    "objectID": "writings.html",
    "href": "writings.html",
    "title": "john myers, ph.d.",
    "section": "",
    "text": "The posts below range from technical deep-dives in probability, statistics, and machine learning to practical applications in finance and risk. Some draw from my expertise as a mathematician, others from my journey as a beginner. Written in three languages: Python and R for computation, mathematics for precision.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgebras & information\n\n\n\nInformation theory\n\nProbability theory\n\nMeasure theory\n\nEntropy\n\nMutual information\n\nSigma-algebras\n\nR\n\n\n\n\n\n\n\n\n\nOct 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEntropy & information\n\n\n\nInformation theory\n\nProbability theory\n\nEntropy\n\nSurprisal\n\nKL divergence\n\nMutual information\n\nPython\n\nSciPy\n\n\n\n\n\n\n\n\n\nOct 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian mixture models I: a first look\n\n\n\nMixture models\n\nGaussian mixture models\n\nProbabilistic graphical models\n\nMachine learning\n\nProbability theory\n\nPython\n\n\n\n\n\n\n\n\n\nSep 26, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/calculus-ii-fa-25.html",
    "href": "teaching/calculus-ii-fa-25.html",
    "title": "mat220 calculus II, fall 2025",
    "section": "",
    "text": "instructor:\n\n\njohn myers\n\n\n\n\noffice + office hours:\n\n\nmarano 175, 12-12:30 MWF\n\n\n\n\nhomework:\n\n\nlink\n\n\n\n\nsyllabus:\n\n\nlink\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndate\ntopics\ninfo + due dates\n\n\n\n\n15\n12.08 mon\nfinal exam, 8-10am, Lanigan 102\n\n\n\n14\n12.05 fri\n\n\n\n\n\n12.04 thu\n\nsection 11.6 hw due @ 9:30am\n\n\n\n12.03 wed\n11.8 power series   \nsection 11.5 hw due @ 12:30pm\n\n\n\n12.01 mon\n11.6 root and ratio tests  \n\n\n\n13\n11.21 fri\n11.5 alternating series  \n\n\n\n\n11.20 thu\nexam 2 on sections 7.4-11.4\nsection 11.4 hw due @ 9:30am  exam 2 extra review  review solutions\n\n\n\n11.19 wed\n11.4 comparison test  \nsection 11.3 hw due @ 12:30pm\n\n\n\n11.17 mon\n11.4 comparison test  \n\n\n\n12\n11.14 fri\n11.3 integral test  \nsection 11.2 hw due @ 12:30pm  quiz 12 on section 11.2\n\n\n\n11.13 thu\n11.2 series, part 2    11.3 integral test  \n\n\n\n\n11.12 wed\nno class\n\n\n\n\n11.10 mon\n11.2 series, part 2    11.3 integral test  \n\n\n\n11\n11.07 fri\n11.2 series, part 2  \nquiz 11 on section 11.1\n\n\n\n11.06 thu\n11.2 series, part 1  \nsection 11.1 hw due @ 12:30pm\n\n\n\n11.05 wed\n11.1 sequences, part 2    11.2 series, part 1  \n\n\n\n\n11.03 mon\n11.1 sequences, part 1    11.1 sequences, part 2  \n\n\n\n10\n10.31 fri\n11.1 sequences, part 1  \nsection 7.8 hw due @ 12:30pm  quiz 10 on sections 7.7 and 7.8\n\n\n\n10.30 thu\ngroup exploration 10\n\n\n\n\n10.29 wed\n7.8 improper integrals  \n\n\n\n\n10.27 mon\nno class\nsection 7.7 hw due @ 12:30pm\n\n\n9\n10.24 fri\n7.7 approximate integration, part 2    7.8 improper integrals  \nquiz 9 on section 7.4\n\n\n\n10.23 thu\n7.7 approximate integration, part 2  \n\n\n\n\n10.22 wed\n7.7 approximate integration, part 1  \nsection 7.4 hw due @ 12:30pm\n\n\n\n10.20 mon\n7.4 partial fractions, part 2  \n\n\n\n8\n10.17 fri\n7.4 partial fractions, part 1  \n\n\n\n\n10.16 thu\nexam 1 on sections 5.1-7.3\nexam 1 extra review  review solutions\n\n\n\n10.15 wed\nreview for exam\n\n\n\n\n10.13 mon\nno class\n\n\n\n7\n10.10 fri\nno class - fall break\n\n\n\n\n10.09 thu\nno class - fall break\n\n\n\n\n10.08 wed\n7.4 partial fractions, part 1  \nsection 7.3, part 2, hw due @ 12:30pm\n\n\n\n10.06 mon\nsec 800: class cancelled  sec 810: 7.3 trigonometric substitution, part 2  \nsection 7.3, part 1, hw due @ 12:30pm\n\n\n6\n10.03 fri\nsec 800: 7.3 trigonometric substitution, part 2    sec 810: class cancelled\nquiz 6 on section 7.1\n\n\n\n10.04 thu\n7.3 trigonometric substitution, part 1    7.3 trigonometric substitution, part 2  \n\n\n\n\n10.01 wed\n7.3 trigonometric substitution, part 1  \nsection 7.1 hw due @ 12:30pm\n\n\n\n09.29 mon\n7.1 integration by parts  \n\n\n\n5\n09.26 fri\n7.1 integration by parts  \nquiz 5 on sections 5.4 and 5.5\n\n\n\n09.25 thu\ngroup exploration 5\n\n\n\n\n09.24 wed\n7.1 integration by parts  \nsection 5.5 hw due @ 12:30pm\n\n\n\n09.22 mon\n5.5 average value of a function  \nsection 5.4 hw due @ 12:30pm\n\n\n4\n09.19 fri\n5.4 work  \nsection 5.3 hw due @ 12:30pmquiz 4 on section 5.3\n\n\n\n09.18 thu\ngroup exploration 4\n\n\n\n\n09.17 wed\n5.4 work  \n\n\n\n\n09.15 mon\n5.3 volumes by cylindrical shells  \nsection 5.2 hw due @ 12:30pm\n\n\n3\n09.12 fri\n5.2 volumes, part 2  \nquiz 3 on section 5.1\n\n\n\n09.11 thu\ngroup exploration 3\n\n\n\n\n09.10 wed\n5.2 volumes, part 1  5.2 volumes, part 2  \n\n\n\n\n09.08 mon\n5.2 volumes, part 1  \nsection 5.1 hw due @ 12:30pm\n\n\n2\n09.05 fri\n5.1 areas between curves  \n\n\n\n\n09.04 thu\ngroup exploration 2\ncalculus I review homework due @ 9:30am\n\n\n\n09.03 wed\ncalculus I review, part 2  \n\n\n\n1\n08.29 fri\nno class\n\n\n\n\n08.28 thu\nno class\n\n\n\n\n08.27 wed\ncalculus I review, part 1  \n\n\n\n\n08.25 mon\ncalculus I review, part 1"
  },
  {
    "objectID": "posts/gmm-1/index.html",
    "href": "posts/gmm-1/index.html",
    "title": "Gaussian mixture models I: a first look",
    "section": "",
    "text": "A mixture of gaussians represents one of the most elegant and practically useful concepts in probability theory and machine learning. At its core, this is a probabilistic model that assumes observed data arises from a combination of several underlying normal distributions, each contributing to the overall distribution with different weights and parameters. Rather than forcing all data points to conform to a single bell curve, a gaussian mixture allows for multiple clusters, each governed by its own mean, variance, and relative importance in the mixture. This creates a framework for modeling complex, multi-modal distributions that frequently appear in real-world scenarios. Consider, for example, the heights of adults in a population that includes both men and women, or the distribution of pixel intensities in an image containing multiple distinct objects. A single gaussian would poorly capture the true underlying structure, while a mixture model can naturally accommodate the multiple peaks and varying spreads that characterize such data. The beauty of this approach lies not only in its flexibility but also in its mathematical tractability, making it both theoretically sound and computationally feasible for a wide range of applications.\nFor example, suppose we were presented with the following data distribution, displayed here as a histogram:\n\n\n\n\n\n\n\n\n\nThree clusters are immediately evident: the first with a peak around \\(x=10\\), the second around \\(x=15\\), and the third near \\(x=21\\). The presence of three clusters with well-defined peaks suggests that the data might be modeled well by a mixture of three gaussians.\nIndeed, this synthetic dataset contains 1,000 observations sampled from a three-component mixture of gaussians, also called a gaussian mixture model or GMM. In this introductory post, I’ll explain the mathematical and graphical structure of the GMM that generated this data. Since I fixed the GMM parameters before sampling, we know the true underlying model. In a follow-up post, I’ll tackle the inverse problem: given only the dataset, how can we estimate the parameters and recover the original GMM?"
  },
  {
    "objectID": "posts/gmm-1/index.html#introduction",
    "href": "posts/gmm-1/index.html#introduction",
    "title": "Gaussian mixture models I: a first look",
    "section": "",
    "text": "A mixture of gaussians represents one of the most elegant and practically useful concepts in probability theory and machine learning. At its core, this is a probabilistic model that assumes observed data arises from a combination of several underlying normal distributions, each contributing to the overall distribution with different weights and parameters. Rather than forcing all data points to conform to a single bell curve, a gaussian mixture allows for multiple clusters, each governed by its own mean, variance, and relative importance in the mixture. This creates a framework for modeling complex, multi-modal distributions that frequently appear in real-world scenarios. Consider, for example, the heights of adults in a population that includes both men and women, or the distribution of pixel intensities in an image containing multiple distinct objects. A single gaussian would poorly capture the true underlying structure, while a mixture model can naturally accommodate the multiple peaks and varying spreads that characterize such data. The beauty of this approach lies not only in its flexibility but also in its mathematical tractability, making it both theoretically sound and computationally feasible for a wide range of applications.\nFor example, suppose we were presented with the following data distribution, displayed here as a histogram:\n\n\n\n\n\n\n\n\n\nThree clusters are immediately evident: the first with a peak around \\(x=10\\), the second around \\(x=15\\), and the third near \\(x=21\\). The presence of three clusters with well-defined peaks suggests that the data might be modeled well by a mixture of three gaussians.\nIndeed, this synthetic dataset contains 1,000 observations sampled from a three-component mixture of gaussians, also called a gaussian mixture model or GMM. In this introductory post, I’ll explain the mathematical and graphical structure of the GMM that generated this data. Since I fixed the GMM parameters before sampling, we know the true underlying model. In a follow-up post, I’ll tackle the inverse problem: given only the dataset, how can we estimate the parameters and recover the original GMM?"
  },
  {
    "objectID": "posts/gmm-1/index.html#conditional-pdfs-and-graphical-structures",
    "href": "posts/gmm-1/index.html#conditional-pdfs-and-graphical-structures",
    "title": "Gaussian mixture models I: a first look",
    "section": "Conditional PDFs and graphical structures",
    "text": "Conditional PDFs and graphical structures\nAs mentioned above, a GMM combines several individual gaussians, each called a component of the mixture. Generating a sample \\(x\\) from a GMM involves two steps:\n\nSelect a component from which to generate \\(x\\).\nSample \\(x\\) from the chosen component gaussian.\n\nThe key insight is that components need not be selected with equal probability. Each component has an associated weight that determines its selection probability in step 1.\nTo formalize the selection process, we introduce the categorical distribution. Suppose given non-negative real numbers \\(\\phi_1,\\phi_2,\\ldots,\\phi_m\\) that sum to \\(1\\):\n\\[\n\\phi_1 + \\phi_2 + \\cdots + \\phi_m =1.\n\\]\nWe say that a discrete random variable \\(Y\\) has a categorical distribution, written\n\\[\nY \\sim \\mathcal{C}at(\\phi_1,\\phi_2,\\ldots,\\phi_m),\n\\tag{1}\\]\nif its probability mass function has the form\n\\[\np(y) = \\begin{cases}\n\\phi_y & : y=1,2,\\ldots,m, \\\\\n0 & : \\text{otherwise}.\n\\end{cases}\n\\]\nWe can now describe GMM sampling more precisely. For a GMM with \\(m\\) components where the \\(i\\)-th component has weight \\(\\phi_i\\), and \\(Y\\) a categorical random variable as in Equation 1, the sampling scheme above becomes:\n\nSample \\(y\\) from the categorical distribution \\(Y\\).\nGiven \\(y\\), sample \\(x\\) from the \\(y\\)-th component gaussian.\n\nThis natural progression from component selection to data generation is often called forward sampling, for obvious reasons.\nWe can visualize this sampling process with a simple graphical representation:\n\n\n\n\n\nHere, \\(Y\\) represents the categorical random variable that encodes component weights, while \\(X\\) represents the random variable from which we sample our observation \\(x\\). The arrow captures the “forward” direction of the sampling process: first select a component, then generate data from it. This diagram illustrates the GMM as a simple example of a probabilistic graphical model (or PGM)—a topic I’ll explore in depth in future posts.\nThe mathematical relationship between these variables is straightforward. Given that component \\(y\\) has been selected, the conditional distribution of \\(X\\) is gaussian:\n\\[\n(X \\mid Y=y) \\sim \\mathcal{N}(\\mu_y,\\sigma_y^2).\n\\]\nThis conditional distribution corresponds exactly to the \\(y\\)-th component of our GMM. Notice that both the mean \\(\\mu_y\\)​ and standard deviation \\(\\sigma_y\\) are component-specific, allowing each gaussian in the mixture to have its own location and spread.\nA GMM thus has two different types of parameters:\n\nThe component weights \\(\\phi_1,\\phi_2,\\ldots,\\phi_m\\).\nFor each \\(y=1,2,\\ldots,m\\), the guassian parameters \\(\\mu_y\\) and \\(\\sigma_y\\).\n\nAs I mentioned in the introduction, in this post we assume that these parameters are fixed and known. Later, we will learn how to estimate them from data."
  },
  {
    "objectID": "posts/gmm-1/index.html#marginal-pdfs",
    "href": "posts/gmm-1/index.html#marginal-pdfs",
    "title": "Gaussian mixture models I: a first look",
    "section": "Marginal PDFs",
    "text": "Marginal PDFs\nThe marginal distribution of \\(X\\) follows directly from the law of total probability: \\[\nf(x) = \\sum_{y=1}^m p(y)f(x|y)  = \\sum_{y=1}^m \\phi_y f(x|y),\n\\tag{2}\\]\nwhere the conditional densities are normal:\n\\[\nf(x|y) = \\frac{1}{\\sigma_y\\sqrt{2\\pi}} \\exp \\left[ - \\frac{1}{2} \\left( \\frac{x-\\mu_y}{\\sigma_y}\\right)^2\\right].\n\\]\nThus, the marginal density \\(f(x)\\) is a convex linear combination of normal densities.\nNow let’s examine the specific \\(3\\)-component GMM that generated our dataset. The component parameters are:\n\\[\n(\\mu_1,\\sigma_1) = (10, 1), \\quad (\\mu_2,\\sigma_2) = (15, 2), \\quad (\\mu_3,\\sigma_3) = (21, 0.5),\n\\]\nwith weights:\n\\[\n(\\phi_1,\\phi_2,\\phi_3) = (0.2, 0.7, 0.1).\n\\]\nNotice that the middle component (centered at 15) dominates with 70% weight, while the outer components contribute 20% and 10% respectively.\nLet’s implement and visualize the component gaussians and the marginal PDF in Python. First, let’s load the parameters:\n\nnorm_params = [\n  {\"loc\": 10, \"scale\": 1},\n  {\"loc\": 15, \"scale\": 2},\n  {\"loc\": 21, \"scale\": 0.5}\n]\nweights = [0.2, 0.7, 0.1]\n\nNow, we plot the individual components, each scaled by its weight:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nmesh = np.linspace(7, 23, num=200)\n\nfor y, (param, weight) in enumerate(zip(norm_params, weights)):\n  X = norm(**param)\n  plt.plot(mesh, weight * X.pdf(mesh), color=colors[y], label=f\"component {y+1}\")\n\nplt.xlabel(\"x\")\nplt.ylabel(\"density\")\nplt.title(\"gaussian components\")\nplt.legend()\nfig = plt.gcf()  # Get current figure\nfig.set_size_inches(6, 4)\nplt.tight_layout()\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\nThe marginal PDF implementation follows Equation 2 directly:\n\ndef marginal_pdf(x):  \n  return sum([weight * norm(**param).pdf(x) for weight, param in zip(weights, norm_params)])\n\nFinally, we plot the theoretical marginal PDF against the empirical histogram:\n\nimport seaborn as sns\n\ndata = np.load(\"../../data/gmm-data.npy\")\n\nsns.histplot(data=data, color=yellow, alpha=0.25, ec=grey, zorder=2, stat=\"density\", label=\"data\")\nplt.plot(mesh, marginal_pdf(mesh), color=blue, label=\"marginal PDF\")\nplt.xlabel(\"x\")\nplt.ylabel(\"density\")\nplt.legend()\nfig = plt.gcf()  # Get current figure\nfig.set_size_inches(6, 4)\nplt.tight_layout()\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\nThe close agreement between our theoretical marginal PDF and the empirical histogram confirms that our synthetic dataset faithfully represents the underlying GMM. Notice how the theoretical curve captures all three modes: the subtle left peak around \\(x=10\\), the dominant central peak near \\(x=15\\), and the sharp right peak at \\(x=21\\). The varying heights and widths of these peaks directly reflect the component weights and standard deviations we specified."
  },
  {
    "objectID": "posts/gmm-1/index.html#sampling",
    "href": "posts/gmm-1/index.html#sampling",
    "title": "Gaussian mixture models I: a first look",
    "section": "Sampling",
    "text": "Sampling\nHaving established the theoretical foundation and visualized the marginal PDF, let’s now examine the process behind generating our synthetic dataset.\nBut this is easy. We first sample 1,000 values of \\(y\\) from the categorical random variable\n\\[\nY \\sim \\mathcal{C}at(0.2, 0.7, 0.1),\n\\]\nand then for each \\(y\\) we sample from the gaussian \\(X|Y=y\\). We check that our new generated sample coincides with the original dataset:\n\nnp.random.seed(42) # Needed to replicate the original dataset\n\nn_samples = 1000\ny_samples = np.random.choice([0, 1, 2], size=n_samples, p=weights)\nx_samples = np.array([norm(**norm_params[y]).rvs() for y in y_samples])\n\nprint(\"Are the datasets equal? \" + (\"Yes.\" if np.array_equal(data, x_samples) else \"No.\"))\n\nAre the datasets equal? No."
  },
  {
    "objectID": "posts/gmm-1/index.html#conclusion",
    "href": "posts/gmm-1/index.html#conclusion",
    "title": "Gaussian mixture models I: a first look",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we’ve built the GMM from the ground up, starting with its definition, exploring its conditional and marginal structure, and even sampling from it directly. But in practice, the real challenge is the inverse problem we mentioned earlier: we are usually given a dataset and want to fit a GMM by estimating its parameters. How do we tease apart the hidden components, estimate their weights, and recover the underlying mixture? That puzzle leads directly to one of the most beautiful iterative methods in statistics and machine learning: the expectation–maximization algorithm, which will be the focus of the next post in this series."
  },
  {
    "objectID": "posts/info-2/index.html",
    "href": "posts/info-2/index.html",
    "title": "Algebras & information",
    "section": "",
    "text": "How do we mathematically represent what someone knows? This question sits at the heart of probability theory, information theory, and decision-making under uncertainty. When an experiment produces an outcome but we don’t observe it directly, our knowledge is partial: we might know some facts about the outcome while remaining uncertain about others. This partial knowledge needs a precise mathematical framework.\nConsider a simple scenario: three coin flips produce a binary sequence like \\(011\\) or \\(101\\) where a \\(0\\) indicates that a tail was produced and \\(1\\) a head, but you don’t see the full result. If someone tells you only the outcome of the first flip, you’ve gained information, but how much? If they then reveal the second flip, you learn more, but again, how much? These questions about information content and uncertainty reduction require both a qualitative structure (what can you distinguish?) and a quantitative measure (how uncertain are you?).\nThe qualitative structure is captured by algebras of sets, which formalize the collection of events an observer can decide about. If you know the first flip, you can decide whether the full sequence starts with \\(0\\) or \\(1\\), partitioning the eight possible outcomes into two groups. Knowing the first two flips refines this partition into four groups, and so on. Each partition represents a distinct “information state,” and the nested structure\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3\n\\]\ncaptures progressive learning over time.\nBut algebras alone don’t quantify uncertainty; they only describe logical structure. To measure how much information each partition contains, we need entropy, along with the related notions of conditional entropy and mutual information. As partitions refine, conditional entropy decreases (less residual uncertainty) while mutual information increases (more shared information between algebras).\nIn a previous post, we introduced entropy, conditional entropy, and mutual information for probability distributions on finite sets. Here, we develop the same ideas from a measure-theoretic perspective, showing how information naturally emerges from the algebraic structure of events. This dual perspective (probabilistic and set-theoretic) provides both conceptual clarity and the foundation for extensions to infinite spaces, stochastic processes, and dynamical systems.\nWe’ll work primarily with a concrete example: three coin flips and the nested algebras they generate. Through explicit computations in R, we’ll see how entropy quantifies information at each stage of learning. The mathematical machinery we develop (algebras, partitions, and their correspondence) forms the backbone of modern probability theory. These ideas have powerful applications in gambling problems and games of chance, where players accumulate information over time, and in options pricing in mathematical finance, where nested filtrations model traders’ evolving information and determine fair prices for derivative securities. We will explore these latter themes in future posts.\nThe idea that algebras (and their \\(\\sigma\\)-algebra generalizations) can represent “information states” is familiar to probability theorists. For example, these ideas appear throughout (Billingsley 1995), which also discusses gambling strategies, while (Björk 2020) provides a perspective from mathematical finance. Information-theoretic measures like entropy are applied to \\(\\sigma\\)-algebras in ergodic theory and other areas; for an introduction, see (Walters 1982).\nIf you’d like to follow along with the code examples in this post, please see the dropdown code block below for the usual imports and setup.\n\n\nCode\n# Load required packages\nlibrary(\"ggplot2\") # For plotting\nlibrary(\"knitr\") # For table formatting (used with kable())\nlibrary(\"dplyr\") # For data manipulation\nlibrary(\"latex2exp\") # For LaTeX rendering in plots\n\n# Source custom theme file\n# Note: Replace this path with your own theme file location\nsource(\"../../aux-files/custom-theme.R\")\n\n# Extract custom colors from theme\n# Note: These color definitions depend on your custom theme file\nyellow &lt;- custom_colors[[\"yellow\"]]\nblue &lt;- custom_colors[[\"blue\"]]\n\n# Apply custom ggplot2 theme\ntheme_set(custom_theme())"
  },
  {
    "objectID": "posts/info-2/index.html#introduction",
    "href": "posts/info-2/index.html#introduction",
    "title": "Algebras & information",
    "section": "",
    "text": "How do we mathematically represent what someone knows? This question sits at the heart of probability theory, information theory, and decision-making under uncertainty. When an experiment produces an outcome but we don’t observe it directly, our knowledge is partial: we might know some facts about the outcome while remaining uncertain about others. This partial knowledge needs a precise mathematical framework.\nConsider a simple scenario: three coin flips produce a binary sequence like \\(011\\) or \\(101\\) where a \\(0\\) indicates that a tail was produced and \\(1\\) a head, but you don’t see the full result. If someone tells you only the outcome of the first flip, you’ve gained information, but how much? If they then reveal the second flip, you learn more, but again, how much? These questions about information content and uncertainty reduction require both a qualitative structure (what can you distinguish?) and a quantitative measure (how uncertain are you?).\nThe qualitative structure is captured by algebras of sets, which formalize the collection of events an observer can decide about. If you know the first flip, you can decide whether the full sequence starts with \\(0\\) or \\(1\\), partitioning the eight possible outcomes into two groups. Knowing the first two flips refines this partition into four groups, and so on. Each partition represents a distinct “information state,” and the nested structure\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3\n\\]\ncaptures progressive learning over time.\nBut algebras alone don’t quantify uncertainty; they only describe logical structure. To measure how much information each partition contains, we need entropy, along with the related notions of conditional entropy and mutual information. As partitions refine, conditional entropy decreases (less residual uncertainty) while mutual information increases (more shared information between algebras).\nIn a previous post, we introduced entropy, conditional entropy, and mutual information for probability distributions on finite sets. Here, we develop the same ideas from a measure-theoretic perspective, showing how information naturally emerges from the algebraic structure of events. This dual perspective (probabilistic and set-theoretic) provides both conceptual clarity and the foundation for extensions to infinite spaces, stochastic processes, and dynamical systems.\nWe’ll work primarily with a concrete example: three coin flips and the nested algebras they generate. Through explicit computations in R, we’ll see how entropy quantifies information at each stage of learning. The mathematical machinery we develop (algebras, partitions, and their correspondence) forms the backbone of modern probability theory. These ideas have powerful applications in gambling problems and games of chance, where players accumulate information over time, and in options pricing in mathematical finance, where nested filtrations model traders’ evolving information and determine fair prices for derivative securities. We will explore these latter themes in future posts.\nThe idea that algebras (and their \\(\\sigma\\)-algebra generalizations) can represent “information states” is familiar to probability theorists. For example, these ideas appear throughout (Billingsley 1995), which also discusses gambling strategies, while (Björk 2020) provides a perspective from mathematical finance. Information-theoretic measures like entropy are applied to \\(\\sigma\\)-algebras in ergodic theory and other areas; for an introduction, see (Walters 1982).\nIf you’d like to follow along with the code examples in this post, please see the dropdown code block below for the usual imports and setup.\n\n\nCode\n# Load required packages\nlibrary(\"ggplot2\") # For plotting\nlibrary(\"knitr\") # For table formatting (used with kable())\nlibrary(\"dplyr\") # For data manipulation\nlibrary(\"latex2exp\") # For LaTeX rendering in plots\n\n# Source custom theme file\n# Note: Replace this path with your own theme file location\nsource(\"../../aux-files/custom-theme.R\")\n\n# Extract custom colors from theme\n# Note: These color definitions depend on your custom theme file\nyellow &lt;- custom_colors[[\"yellow\"]]\nblue &lt;- custom_colors[[\"blue\"]]\n\n# Apply custom ggplot2 theme\ntheme_set(custom_theme())"
  },
  {
    "objectID": "posts/info-2/index.html#algebras-as-information-carriers",
    "href": "posts/info-2/index.html#algebras-as-information-carriers",
    "title": "Algebras & information",
    "section": "Algebras as information carriers",
    "text": "Algebras as information carriers\nAn observer monitors the result of some experiment. The set of all possible results is collected into a sample space \\(\\Omega\\), whose elements \\(\\omega\\) are called outcomes or sample points. Suppose that the experiment is finished, so that a definite \\(\\omega_0\\) in \\(\\Omega\\) is produced, but that the exact identity of \\(\\omega_0\\) remains unknown to the observer. However, suppose that the observer has access to a set \\(\\mathcal{F}\\) of subsets of \\(\\Omega\\) with the following property:\n\n\n\n\n\n\nFor each \\(A\\in \\mathcal{F}\\), the observer can decide whether \\(\\omega_0\\in A\\) or not.\n\n\n\nThe observer’s decisions reflect the information they possess, so we can treat the class \\(\\mathcal{F}\\) as a proxy for that information. Sets in \\(\\mathcal{F}\\) are called (decidable) events. If the observer confirms that \\(\\omega_0\\in A\\) for some \\(A\\in \\mathcal{F}\\), then we say that “the event \\(A\\) has occurred”.\nIt is never in question that \\(\\omega_0\\in \\Omega\\), since the same space \\(\\Omega\\) contains all possible outcomes, so it is natural to include \\(\\Omega\\) in the class \\(\\mathcal{F}\\) of decidable events. But what other properties must \\(\\mathcal{F}\\) have?\n\nIf the observer can determine whether an event \\(A\\) has occurred, then they obviously can determine whether \\(A\\) did not occur. This means that if \\(A\\in \\mathcal{F}\\), then \\(A^c \\in \\mathcal{F}\\) too. \nIf the observer can determine separately whether events \\(A\\) and \\(B\\) have occurred, then they can determine whether \\(A\\) or \\(B\\) has occurred. Since \\(A\\cup B\\) consists of those outcomes that are either in \\(A\\) or \\(B\\), this means that \\(A\\cup B\\) must be in \\(\\mathcal{F}\\) when both \\(A\\) and \\(B\\) are.\nSimilarly, if the observer can determine separately whether events \\(A\\) and \\(B\\) have occurred, then they can determine whether \\(A\\) and \\(B\\) has occurred. This implies that \\(\\mathcal{F}\\) must contain the intersection \\(A\\cap B\\) whenever it contains both \\(A\\) and \\(B\\).\n\nHere, \\(A^c\\) is the absolute complement of \\(A\\), consisting of those \\(\\omega\\in \\Omega\\) that are not in \\(A\\).These three properties capture what it means for the observer’s collection of events \\(\\mathcal{F}\\) to represent a logically coherent state of information. They lead us naturally to the formal definition of an algebra of sets. Actually, we need not explicitly require that \\(\\mathcal{F}\\) contains all (binary) intersections of events, since this follows from properties (1) and (2). Note also that \\(\\mathcal{F}\\) must contain the empty set, since it contains \\(\\Omega\\) and is closed under complements.\n\n\n\n\n\n\n\nDefinition 1 Let \\(\\mathcal{F}\\) be a set of subsets of a set \\(\\Omega\\). Then \\(\\mathcal{F}\\) is called an algebra of sets (in \\(\\Omega\\)) if it has the following properties:\n\nIt contains \\(\\Omega\\).\nIt is closed under (absolute) complementation:\n\n\\[\n  A\\in \\mathcal{F}\\quad \\Rightarrow \\quad A^c\\in \\mathcal{F}.\n  \\]\n\nIt is closed under binary unions:\n\n\\[\n  A_1, A_2\\in \\mathcal{F}\\quad\\Rightarrow \\quad A_1 \\cup A_2 \\in \\mathcal{F}.\n  \\]\n\n\n\n\nWe have thus argued that our class \\(\\mathcal{F}\\) of decidable events—encoding our observer’s information—must be an algebra of sets in the sample space \\(\\Omega\\).\nAs we indicated in the margin note above, algebras of sets are also closed under binary intersections. Moreover, an easy inductive proof shows that algebras are also closed under all finite unions and intersections, not just binary ones. In this post we’ll stick to finite sample spaces, but in the infinite case one usually strengthens these closure properties to include countable unions, yielding \\(\\sigma\\)-algebras of sets."
  },
  {
    "objectID": "posts/info-2/index.html#example-coin-tosses",
    "href": "posts/info-2/index.html#example-coin-tosses",
    "title": "Algebras & information",
    "section": "Example: coin tosses",
    "text": "Example: coin tosses\nIn practice, the information encoded in \\(\\mathcal{F}\\) is often most naturally described by a partition that generates it. A partition is a collection of sets\n\\[\n\\{A_1,A_2,\\ldots,A_n\\}\n\\]\nwhere each \\(A_k \\subset \\Omega\\), the sets \\(A_k\\)​ are mutually disjoint, and their union exhausts the sample space: \\(\\Omega = \\bigcup_{k=1}^n A_k\\)​.\nThe intuition is simple: an observer’s information typically comes from knowing the answer to some question. Each possible answer groups together certain outcomes, and these groups form a partition of \\(\\Omega\\). The algebra \\(\\mathcal{F}\\) is then the smallest algebra containing all sets in this partition. We will study partitions in more detail in the next section, but for now we see how it works in a simple example.\nWe consider the sample space \\(\\Omega\\) consisting of all binary sequences of length 3:\n\\[\n\\Omega = \\{000, 001, 010, 011, 100, 101, 110, 111\\}.\n\\]\nWe would visualize this sample space as follows:\n\n\n\n\n\nWe imagine that a binary sequence \\(\\omega\\) in \\(\\Omega\\) represents the result of flipping a coin three times in succession, with \\(0\\) when a tail occurs, and \\(1\\) when a head is shown. We let \\(X_i(\\omega)\\) denote the result of the \\(i\\)-th flip in \\(\\omega\\) (so \\(X_i(\\omega)=0\\) or \\(1\\)), and we write\n\\[\nY = X_1 + X_2 + X_3.\n\\]\nThe four functions \\(X_1\\), \\(X_2\\), \\(X_3\\), and \\(Y\\) are, of course, random variables on \\(\\Omega\\). If the coin has probability \\(\\theta\\) of showing heads, then \\(X_i \\sim \\mathcal{B}er(\\theta)\\) for each \\(i\\), and \\(Y\\sim \\mathcal{B}in(3,\\theta)\\).\nWe will be carrying out some basic computations later, so let’s implement our scenario in the machine, by coding a data frame in which each row represents an outcome \\(\\omega\\):\n\n\nCode\n# Create the sample space Omega as a data frame\n# Each row represents one of the 8 possible outcomes (binary sequences of length 3)\nOmega &lt;- data.frame(\n  X1 = c(0, 0, 0, 0, 1, 1, 1, 1), # First coin flip: 0 = tail, 1 = head\n  X2 = c(0, 0, 1, 1, 0, 0, 1, 1), # Second coin flip\n  X3 = c(0, 1, 0, 1, 0, 1, 0, 1) # Third coin flip\n)\n\n# Add random variable Y: total number of heads across all three flips\nOmega &lt;- Omega %&gt;%\n  mutate(Y = X1 + X2 + X3)\n\n# Display the sample space as a formatted table\nkable(\n  Omega,\n  caption = \"sample space $\\\\Omega$ and random variables $X_1$, $X_2$, $X_3$, and $Y$\"\n)\n\n\n\nsample space \\(\\Omega\\) and random variables \\(X_1\\), \\(X_2\\), \\(X_3\\), and \\(Y\\)\n\n\nX1\nX2\nX3\nY\n\n\n\n\n0\n0\n0\n0\n\n\n0\n0\n1\n1\n\n\n0\n1\n0\n1\n\n\n0\n1\n1\n2\n\n\n1\n0\n0\n1\n\n\n1\n0\n1\n2\n\n\n1\n1\n0\n2\n\n\n1\n1\n1\n3\n\n\n\n\n\n\nNow, suppose that the coin was flipped three times, producing a definite outcome \\(\\omega_0\\) in \\(\\Omega\\), but that its exact identity is unknown to our observer. The only information available is the answer to the question: “What is the outcome of the first flip?”. This information allows the observer to partition the sample space \\(\\Omega\\) and generate an algebra \\(\\mathcal{F}_1\\), just as we described at the beginning of this section.\nThis partition consists of the sets\n\\[\nA_1 = \\{000,001,010,011\\} \\quad \\text{and} \\quad A_2 = \\{100,101,110,111\\},\n\\]\nwhere \\(A_1\\) contains outcomes corresponding to the answer “tail” and \\(A_2\\) those corresponding to “head.” As you can easily check, the set\n\\[\n\\mathcal{F}_1 = \\{\\emptyset, A_1, A_2, \\Omega\\}\n\\]\nis an algebra of sets in \\(\\Omega\\). We would visualize this partition as follows:\n\n\n\n\n\nLet’s insert a column in our data frame tracking the membership of each outcome \\(\\omega\\) (i.e., row) in \\(A_1\\) or \\(A_2\\):\n\n\nCode\n# Group outcomes by the result of the first flip (X1)\n# and assign a group ID to track which partition set (A1 or A2) each outcome belongs to\nOmega &lt;- Omega %&gt;%\n  group_by(X1) %&gt;%\n  mutate(Ai = cur_group_id()) %&gt;% # Ai = 1 for A1 (X1=0), Ai = 2 for A2 (X1=1)\n  ungroup() %&gt;%\n  as.data.frame()\n\n# Display the updated sample space showing partition membership\nkable(Omega, caption = \"column Ai records the index $i$ of $A_i$\")\n\n\n\ncolumn Ai records the index \\(i\\) of \\(A_i\\)\n\n\nX1\nX2\nX3\nY\nAi\n\n\n\n\n0\n0\n0\n0\n1\n\n\n0\n0\n1\n1\n1\n\n\n0\n1\n0\n1\n1\n\n\n0\n1\n1\n2\n1\n\n\n1\n0\n0\n1\n2\n\n\n1\n0\n1\n2\n2\n\n\n1\n1\n0\n2\n2\n\n\n1\n1\n1\n3\n2\n\n\n\n\n\n\nNow, suppose the observer is also given the answer to the question: “What is the outcome of the second flip?” Combining this with the information from the first flip, the observer can now distinguish outcomes based on the results of the first two flips. This additional information generates a finer partition of \\(\\Omega\\) and produces a second algebra \\(\\mathcal{F}_2\\), which contains the first:\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2.\n\\]\nInclusions of algebras represent refinements of information.\nThe new partition consists of the sets\n\\[\nB_1 = \\{000,001\\}, \\quad B_2 = \\{010,011\\}, \\quad B_3 = \\{100,101\\}, \\quad B_4 = \\{110,111\\},\n\\]\nwhere \\(B_1\\) corresponds to the answer “first and second flips are both tails”, \\(B_2\\) to “first flip tail, second flip head”, and so on. The algebra that is generated by this partition is\n\\[\n\\mathcal{F}_2 = \\{\\emptyset, B_1, B_2, B_3, B_4, B_1 \\cup B_2, B_3 \\cup B_4, B_1\\cup B_3, B_1\\cup B_4, B_2\\cup B_3, B_2\\cup B_4, \\Omega \\}.\n\\tag{1}\\]\nAs expected, \\(\\mathcal{F}_2\\) contains \\(\\mathcal{F}_1\\) since \\(B_1\\cup B_2 = A_1\\) and \\(B_3\\cup B_4 = A_2\\), reflecting that the observer’s information has been refined. Along with the first partition induced by \\(\\mathcal{F}_1\\), we would visualize this finer partition as follows:\n\n\n\n\n\nLet’s insert our new algebra into the machine:\n\n\nCode\n# Group outcomes by the results of the first two flips (X1 and X2)\n# and assign a group ID to track which partition set (B1, B2, B3, or B4) each outcome belongs to\nOmega &lt;- Omega %&gt;%\n  group_by(X1, X2) %&gt;%\n  mutate(Bi = cur_group_id()) %&gt;% # Bi = 1 for B1, Bi = 2 for B2, etc.\n  ungroup() %&gt;%\n  as.data.frame()\n\n# Display the updated sample space showing partition membership for the refined algebra F2\nkable(Omega, caption = \"column Bi records the index $i$ of $B_i$\")\n\n\n\ncolumn Bi records the index \\(i\\) of \\(B_i\\)\n\n\nX1\nX2\nX3\nY\nAi\nBi\n\n\n\n\n0\n0\n0\n0\n1\n1\n\n\n0\n0\n1\n1\n1\n1\n\n\n0\n1\n0\n1\n1\n2\n\n\n0\n1\n1\n2\n1\n2\n\n\n1\n0\n0\n1\n2\n3\n\n\n1\n0\n1\n2\n2\n3\n\n\n1\n1\n0\n2\n2\n4\n\n\n1\n1\n1\n3\n2\n4\n\n\n\n\n\n\nFinally, suppose the observer is given the answer to the question: “What is the outcome of the third flip?” With this information, the observer can now distinguish outcomes based on all three flips, completely specifying \\(\\omega_0\\). This generates the finest partition of \\(\\Omega\\) and produces a third algebra \\(\\mathcal{F}_3\\), which refines the information in \\(\\mathcal{F}_2\\) (and thus also \\(\\mathcal{F}_1\\)):\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3.\n\\]\nThe new partition must contain the eight singletons\n\\[\n\\begin{gather*}\nC_1 = \\{000\\}, \\ C_2 = \\{001\\}, \\ C_3 = \\{010\\}, \\ C_4 = \\{011\\}, \\\\\nC_5 = \\{100\\}, \\ C_6 = \\{101\\}, \\ C_7 = \\{110\\}, \\ C_8 = \\{111\\},\n\\end{gather*}\n\\]\nwhere each \\(C_i\\) corresponds to the answer to the question, “Which sequence of heads and tails occurred in all three flips?”. Knowing which singleton contains \\(\\omega_0\\) completely identifies the outcome. To form an algebra, \\(\\mathcal{F}_3\\) must include not only these singletons but also all possible unions of them; in other words, \\(\\mathcal{F}_3\\) is the power set of \\(\\Omega\\), containing all subsets of the sample space. Along with the first two partitions induced by \\(\\mathcal{F}_1\\) and \\(\\mathcal{F}_2\\), we would visualize this finest partition as follows:\n\n\n\n\n\nWe insert one last column in our data frame for the \\(C_i\\)’s:\n\n\nCode\n# Group outcomes by all three flips (X1, X2, and X3)\n# Each combination of (X1, X2, X3) uniquely identifies one outcome\nOmega &lt;- Omega %&gt;%\n  group_by(X1, X2, X3) %&gt;%\n  mutate(Ci = cur_group_id()) %&gt;% # Ci = 1 for C1, Ci = 2 for C2, etc.\n  ungroup() %&gt;%\n  as.data.frame()\n\n# Display the updated sample space showing the finest partition (singletons)\n# At this level, each outcome is completely distinguishable\nkable(Omega, caption = \"column Ci records the index $i$ of $C_i$\")\n\n\n\ncolumn Ci records the index \\(i\\) of \\(C_i\\)\n\n\nX1\nX2\nX3\nY\nAi\nBi\nCi\n\n\n\n\n0\n0\n0\n0\n1\n1\n1\n\n\n0\n0\n1\n1\n1\n1\n2\n\n\n0\n1\n0\n1\n1\n2\n3\n\n\n0\n1\n1\n2\n1\n2\n4\n\n\n1\n0\n0\n1\n2\n3\n5\n\n\n1\n0\n1\n2\n2\n3\n6\n\n\n1\n1\n0\n2\n2\n4\n7\n\n\n1\n1\n1\n3\n2\n4\n8\n\n\n\n\n\n\nTo recap, the sets \\(A_k\\), \\(B_k\\), and \\(C_k\\) form partitions of \\(\\Omega\\), with each successive partition being finer than the last: each \\(A_k\\) is a union of \\(B_k\\)’s, and each \\(B_k\\) is a union of \\(C_k\\)’s. These sets are also the atoms of their respective algebras, meaning they do not properly contain any nonempty subset in the corresponding algebra.\nTwo outcomes \\(\\omega_1\\) and \\(\\omega_2\\) in the same \\(A_k\\) are indistinguishable using only the information in \\(\\mathcal{F}_1\\). Likewise, outcomes in the same \\(B_k\\) are indistinguishable with respect to \\(\\mathcal{F}_2\\). The \\(C_k\\), by contrast, correspond to complete information in \\(\\mathcal{F}_3\\), so no ambiguity remains.\nEach algebra in the chain of inclusions\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3\n\\]\ncaptures a distinct level of information about the outcome \\(\\omega_0\\). This nested structure provides a simple model of learning over time: as the experiment unfolds and the observer acquires new data, their algebra of decidable events expands, splitting sets of previously indistinguishable outcomes into finer partitions—until, at last, \\(\\mathcal{F}_3\\) represents complete knowledge of \\(\\omega_0\\)."
  },
  {
    "objectID": "posts/info-2/index.html#algebras-and-partitions",
    "href": "posts/info-2/index.html#algebras-and-partitions",
    "title": "Algebras & information",
    "section": "Algebras and partitions",
    "text": "Algebras and partitions\nIn this section, we nail down the ideas from the previous section on partitions and algebras and highlight four useful theorems connecting them. Some proofs get a bit technical, so feel free to skim and focus on the statements. For simplicity, we stick to finite sets \\(\\Omega\\), though many results hold for infinite sets too.\nSo, let \\(\\Omega\\) be a finite set and \\(\\mathcal{F}\\) an algebra of sets in \\(\\Omega\\). Indistinguishability of two elements \\(\\omega\\) and \\(\\omega'\\) of \\(\\Omega\\) (relative to \\(\\mathcal{F}\\)) occurs exactly when we have\n\\[\n\\omega \\in A \\quad \\Leftrightarrow \\quad \\omega' \\in A, \\quad \\forall A\\in \\mathcal{F}.\n\\tag{2}\\]\nElements \\(\\omega\\) and \\(\\omega'\\) related in this way will be called \\(\\mathcal{F}\\)-equivalent.\nThis notion of equivalence is evidently an equivalence relation in the technical sense, and thus we obtain a partition of \\(\\Omega\\) into the distinct equivalence classes. These equivalence classes are actually themselves contained in \\(\\mathcal{F}\\), since \\(\\mathcal{F}\\) is closed under intersections. In fact, if \\(E\\) is an equivalence class containing an outcome \\(\\omega\\in \\Omega\\), then \\(E\\) is the intersection of all \\(A\\in \\mathcal{F}\\) that contain \\(\\omega\\):\n\\[\nE = \\bigcap_{\\omega \\in A \\in \\mathcal{F}} A.\n\\]\n(Verification of this equality is left to the reader.) But even more than being elements of \\(\\mathcal{F}\\), these equivalence classes are atomic in the sense of the following definition.\n\n\n\n\n\n\n\nDefinition 2 Let \\(\\mathcal{F}\\) be an algebra of sets in a finite set \\(\\Omega\\). A set \\(A\\in \\mathcal{F}\\) is called an atom if it is non-reducible, in the sense that \\(A\\) contains no nonempty subset of \\(\\mathcal{F}\\) other than itself.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1 (Atomic theorm) Let \\(\\mathcal{F}\\) be an algebra of sets in a finite set \\(\\Omega\\).\n\nThere exists a partition \\(\\mathcal{A}\\) of \\(\\Omega\\) consisting entirely of atoms in \\(\\mathcal{F}\\). In fact, \\(\\mathcal{A}\\) may be chosen to consist of the distinct \\(\\mathcal{F}\\)-equivalence classes.\nIf \\(\\mathcal{A}\\) is a partition of \\(\\Omega\\) consisting of atoms in \\(\\mathcal{F}\\), then every nonempty set in \\(\\mathcal{F}\\) is a union of atoms in \\(\\mathcal{A}\\).\nLet \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) be two partitions of \\(\\Omega\\) consisting of atoms in \\(\\mathcal{F}\\). Then \\(\\mathcal{A}= \\mathcal{B}\\).\n\n\n\n\n\n\n\n\n\n\n\nNoteProof.\n\n\n\n\n\n(1): Since the distinct \\(\\mathcal{F}\\)-equivalence classes partition \\(\\Omega\\), we just need to show that a given equivalence class is non-reducible. So, let \\(E\\) be an \\(\\mathcal{F}\\)-equivalence class, and suppose that \\(A\\in \\mathcal{F}\\) is a nonempty subset of \\(E\\). Choose \\(\\omega'\\in A\\) and \\(\\omega \\in E\\). Then \\(\\omega'\\in E\\) as well, and so \\(\\omega\\) and \\(\\omega'\\) are \\(\\mathcal{F}\\)-equivalent. But then \\(\\omega'\\in A\\) implies \\(\\omega\\in A\\), and since \\(\\omega\\) was an arbitrarily chosen element of \\(E\\), we must have \\(E\\subset A\\). But \\(A\\) began as a subset of \\(E\\), and so \\(E=A\\).\n(2): Since the atoms in \\(\\mathcal{A}\\) partition \\(\\Omega\\), it will suffice to show that every nonempty set \\(S\\in \\mathcal{F}\\) is either disjoint from a given atom in \\(A\\in\\mathcal{A}\\), or \\(S\\) contains \\(A\\) as a subset. But if \\(S\\cap A\\) is nonempty, then we must have \\(S\\cap A = A\\) since \\(S\\cap A\\in \\mathcal{F}\\) and \\(A\\) is non-reducible. But then \\(A\\subset S\\), as desired.\n(3): First, choose an atom \\(A\\in \\mathcal{A}\\). Since \\(\\mathcal{B}\\) is a partition of \\(\\Omega\\), there must exist a \\(B\\in \\mathcal{B}\\) such that \\(A\\cap B\\) is nonempty. But then\n\\[\nA = A\\cap B =B,\n\\]\nwhere the first equalities follow from the fact that \\(A\\cap B\\) is in \\(\\mathcal{F}\\) and non-reducibility of \\(A\\) and \\(B\\). Thus \\(A\\in \\mathcal{B}\\), which proves \\(\\mathcal{A}\\subset \\mathcal{B}\\). A symmetric argument proves the opposite containment, and hence \\(\\mathcal{A}= \\mathcal{B}\\).\n\n\n\nSo, every algebra of sets \\(\\mathcal{F}\\) in a finite set \\(\\Omega\\) naturally gives rise to a unique partition \\(\\mathcal{A}\\) of \\(\\Omega\\) consisting of atoms in \\(\\mathcal{F}\\). Conversely, as we saw in the coin-flip example, a given partition \\(\\mathcal{A}\\) of \\(\\Omega\\) can be used to generate an algebra, which we denote by \\(\\sigma(\\mathcal{A})\\). By construction, \\(\\sigma(\\mathcal{A})\\) is the smallest algebra in \\(\\Omega\\) that contains all sets in \\(\\mathcal{A}\\), i.e., \\(\\mathcal{A}\\subset \\sigma(\\mathcal{A})\\).\n\n\n\n\n\n\n\nTheorem 2 (Generating algebras via atoms) Let \\(\\mathcal{A}\\) be a partition of a finite set \\(\\Omega\\).\n\nEvery set in \\(\\sigma(\\mathcal{A})\\) is a union of sets in \\(\\mathcal{A}\\).\nThe sets in \\(\\mathcal{A}\\) are atoms in the algebra \\(\\sigma(\\mathcal{A})\\).\n\n\n\n\n\nTo see this theorem in action, I encourage you to scroll back and look at the algebra \\(\\mathcal{F}_2\\) in (1) in the previous section, generated by the partition \\(\\{B_1,B_2,B_3,B_4\\}\\). Notice that \\(\\mathcal{F}_2\\) does indeed consist of all unions of the \\(B_k\\)’s.\n\n\n\n\n\nNoteProof.\n\n\n\n\n\n(1): Since \\(\\sigma(\\mathcal{A})\\) is an algebra, it must contain the set \\(\\mathcal{F}\\) of all unions of sets in \\(\\mathcal{A}\\), so we have\n\\[\n\\mathcal{A}\\subset \\mathcal{F}\\subset \\sigma(\\mathcal{A}).\n\\]\nThe point is that \\(\\mathcal{F}\\) is already an algebra, and hence \\(\\mathcal{F}= \\sigma(\\mathcal{A})\\). To prove this, note that \\(\\Omega \\in \\mathcal{F}\\) since the union of all sets in \\(\\mathcal{A}\\) is \\(\\Omega\\). Clearly \\(\\mathcal{F}\\) is closed under unions, so we need only show that it is closed under absolute complements. But because the sets in \\(\\mathcal{A}\\) are pairwise disjoint and their union is \\(\\Omega\\), the complement of a union of sets in \\(\\mathcal{A}\\) is another set of the same form, and hence \\(\\mathcal{F}\\) is indeed closed under unions. This proves the claim \\(\\mathcal{F}= \\sigma(\\mathcal{A})\\), and thereby establishes the first statement.\n(2): Let \\(A\\in \\mathcal{A}\\) and suppose \\(S\\in \\mathcal{F}\\) is a nonempty subset of \\(A\\). But then \\(S\\) is a union of sets in \\(\\mathcal{A}\\), and since the sets in \\(\\mathcal{A}\\) are pairwise disjoint, we must have \\(S=A\\).\n\n\n\nThe mapping \\(\\mathcal{A}\\mapsto \\sigma(\\mathcal{A})\\) associating every partition \\(\\mathcal{A}\\) of a finite set \\(\\Omega\\) with the algebra \\(\\sigma(\\mathcal{A})\\) is injective. Indeed, if \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) are two partitions such that \\(\\sigma(\\mathcal{A}) = \\sigma(\\mathcal{B})\\), then Theorem 2 shows that \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) are both partitions of the same algebra and both consist of atoms; then Theorem 1 shows \\(\\mathcal{A}= \\mathcal{B}\\). On the other hand, if \\(\\mathcal{F}\\) is an algebra of sets in \\(\\Omega\\), then a partition \\(\\mathcal{A}\\) of \\(\\Omega\\) consisting of atoms in \\(\\mathcal{F}\\) exists, by Theorem 1, and all sets in \\(\\mathcal{F}\\) are unions of atoms. But then \\(\\mathcal{F}= \\sigma(\\mathcal{A})\\) by Theorem 2. We have thus proved:\n\n\n\n\n\n\n\nTheorem 3 (Algebras \\(=\\) Partitions) Let \\(\\Omega\\) be a finite set. The mapping \\(\\mathcal{A}\\mapsto \\sigma(\\mathcal{A})\\) is a bijection from the class of all partitions of \\(\\Omega\\) onto the class of all algebras of sets in \\(\\Omega\\).\n\n\n\n\nFinally, if \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\) are two algebras of sets in \\(\\Omega\\), we say \\(\\mathcal{F}\\) refines \\(\\mathcal{G}\\) if \\(\\mathcal{G}\\subset \\mathcal{F}\\). As we saw in the coin-flip example, refinements of algebras corresponds to refinement of information.\n\n\n\n\n\n\n\nTheorem 4 (Refinement Theorem) Let \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\) be two algebras of sets in a finite set \\(\\Omega\\) with associated partitions \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) consisting of atoms, respectively. Then \\(\\mathcal{F}\\) refines \\(\\mathcal{G}\\) if and only if every set \\(B\\in \\mathcal{B}\\) is a union of sets in \\(\\mathcal{A}\\).\n\n\n\n\n\n\n\n\n\n\nNoteProof.\n\n\n\n\n\nFirst, supposing that \\(\\mathcal{F}\\) refines \\(\\mathcal{G}\\), we have\n\\[\n\\mathcal{B}\\subset \\sigma(\\mathcal{B}) = \\mathcal{G}\\subset \\mathcal{F}= \\sigma(\\mathcal{A}),\n\\]\nwhere the two equalities follow from Theorem 3. The desired conclusion then follows from part (1) of Theorem 2. Conversely, if every \\(B\\in \\mathcal{B}\\) is a union of sets in \\(\\mathcal{A}\\), then we get the subset containment in\n\\[\n\\mathcal{B}\\subset \\sigma(\\mathcal{A}) = \\mathcal{F},\n\\]\nwhile the equality again follows from Theorem 3. But then\n\\[\n\\mathcal{G}= \\sigma(\\mathcal{B}) \\subset \\mathcal{F},\n\\]\nwhere the equality follows (one more time) from Theorem 3 and the subset containment from the fact \\(\\mathcal{F}\\) is an algebra containing \\(\\mathcal{B}\\). Thus \\(\\mathcal{F}\\) refines \\(\\mathcal{G}\\).\n\n\n\nThe correspondence between algebras and partitions gives us two complementary perspectives on information. The algebraic view emphasizes logical operations: we can decide membership in events, take complements, form unions and intersections. The partition view emphasizes distinguishability: outcomes in the same partition set are indistinguishable, while outcomes in different sets can be told apart. Theorem 3 shows these perspectives are equivalent—every algebra uniquely determines a partition of atoms, and every partition uniquely generates an algebra. Moreover, Theorem 4 shows that refinement of information has a natural interpretation in both frameworks: \\(\\mathcal{F}\\) refines \\(\\mathcal{G}\\) (as algebras) precisely when the atoms of \\(\\mathcal{F}\\) subdivide the atoms of \\(\\mathcal{G}\\) (as partitions). This duality between set-theoretic structure and equivalence classes will prove essential when we introduce entropy, where the partition view naturally connects to probability distributions over distinguishable outcomes."
  },
  {
    "objectID": "posts/info-2/index.html#algebras-and-entropy",
    "href": "posts/info-2/index.html#algebras-and-entropy",
    "title": "Algebras & information",
    "section": "Algebras and entropy",
    "text": "Algebras and entropy\nHaving established the correspondence between algebras and partitions, we now turn to quantifying the information content they represent. While the nested structure\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3\n\\]\nthat we explored in our coin-flip example shows that information is refined—that uncertainty decreases—we have not yet measured how much uncertainty remains at each stage, or how much uncertainty is removed by each refinement. To do this, we introduce the concept of entropy associated with an algebra of sets, which is a natural extension of the Shannon entropy of a random variable that we studied in a previous post.\nWe continue to work with a finite sample space \\(\\Omega\\) and an algebra \\(\\mathcal{F}\\) of sets in \\(\\Omega\\). To quantify uncertainty, we need a probability measure \\(P\\) on \\(\\Omega\\), which assigns a probability \\(P(\\{\\omega\\})\\) to each outcome \\(\\omega\\in \\Omega\\). The probability of an event \\(A\\in \\mathcal{F}\\) is then given by\n\\[\nP(A) = \\sum_{\\omega \\in A} P(\\{\\omega\\}).\n\\]\nTogether, the triple \\((\\Omega,\\mathcal{F},P)\\) is called a finite probability space. An algebra of sets \\(\\mathcal{G}\\) in \\(\\Omega\\) is called a sub-\\(\\sigma\\)-algebra of \\(\\mathcal{F}\\) if \\(\\mathcal{G}\\subset \\mathcal{F}\\).\nIn our coin-flip example, we can define a probability measure \\(P\\) on \\(\\Omega\\) by assuming that the coin has probability \\(\\theta\\) of showing heads on any given flip, independently of previous flips. Then the probability of an outcome \\(\\omega\\) with \\(Y(\\omega)\\) heads is\n\\[\nP(\\{\\omega\\}) = \\theta^{Y(\\omega)} (1-\\theta)^{3-Y(\\omega)}.\n\\]\nIf we suppose that \\(\\theta = 0.6\\), then the probabilties on \\(\\Omega\\) are as follows:\n\n\nCode\ntheta &lt;- 0.6 # Probability of heads (coin bias parameter)\n\n# Calculate the probabilities of each outcome in Omega\n# Under independence, P({ω}) = θ^(# heads) × (1-θ)^(# tails)\nOmega &lt;- Omega %&gt;%\n  mutate(\n    p = (theta^Y) * ((1 - theta)^(3 - Y)) # Probability based on binomial model\n  )\n\n# Display the sample space with probabilities as a formatted table\n# Each row shows an outcome ω and its probability P({ω})\nkable(\n  Omega,\n  caption = \"sample space $\\\\Omega$ with probabilities $P(\\\\{\\\\omega\\\\})$ for each outcome $\\\\omega$\"\n)\n\n\n\nsample space \\(\\Omega\\) with probabilities \\(P(\\{\\omega\\})\\) for each outcome \\(\\omega\\)\n\n\nX1\nX2\nX3\nY\nAi\nBi\nCi\np\n\n\n\n\n0\n0\n0\n0\n1\n1\n1\n0.064\n\n\n0\n0\n1\n1\n1\n1\n2\n0.096\n\n\n0\n1\n0\n1\n1\n2\n3\n0.096\n\n\n0\n1\n1\n2\n1\n2\n4\n0.144\n\n\n1\n0\n0\n1\n2\n3\n5\n0.096\n\n\n1\n0\n1\n2\n2\n3\n6\n0.144\n\n\n1\n1\n0\n2\n2\n4\n7\n0.144\n\n\n1\n1\n1\n3\n2\n4\n8\n0.216\n\n\n\n\n\n\nWith finite probability spaces defined, we can now introduce the entropy of an algebra of sets, which measures the uncertainty associated with the information it encodes. The definition is a straightforward extension of Shannon entropy, applied to the partition of \\(\\Omega\\) formed by the atoms of the algebra.\n\n\n\n\n\n\n\nDefinition 3 Let \\((\\Omega,\\mathcal{F},P)\\) be a finite probability space, let \\(\\mathcal{G}\\) be a sub-\\(\\sigma\\)-algebra of \\(\\mathcal{F}\\), and let \\(\\{B_1,B_2,\\ldots,B_n\\}\\) be the partition of \\(\\Omega\\) by atoms of \\(\\mathcal{G}\\). We then define the entropy of \\(\\mathcal{G}\\) to be the quantity\n\\[\nH(\\mathcal{G}) = -\\sum_{i=1}^n P(B_i) \\log{P(B_i)}.\n\\tag{3}\\]\n\n\n\n\nFor those readers who are familiar with abstract integration theory, we note that the formula (3) can be expressed as a Lebesgue integral. Indeed, if we write \\(\\pi: \\Omega \\to \\Omega/\\mathcal{G}\\) for the natural projection map onto the quotient space \\(\\Omega/\\mathcal{G}\\) whose points are the atoms of \\(\\mathcal{G}\\), then the entropy of \\(\\mathcal{G}\\) is given by the Lebesgue integral\n\\[\nH(\\mathcal{G}) = \\int_{\\Omega/\\mathcal{G}} s(B) \\ \\pi_\\ast P(dB),\n\\]\nwhere \\(s:\\Omega/\\mathcal{G}\\to \\mathbb{R}\\) is the surprisal function given by\n\\[\ns(B) = -\\log{P(B)},\n\\]\nand \\(\\pi_\\ast P\\) is the pushforward measure of \\(P\\) under \\(\\pi\\) defined on the power set of the quotient \\(\\Omega/\\mathcal{G}\\). By the substitution rule, we also have\n\\[\nH(\\mathcal{G}) = \\int_\\Omega (s\\circ \\pi)(\\omega) \\ P(d\\omega),\n\\]\nso the entropy is nothing but the expected value \\(E(s\\circ \\pi)\\) of the random variable \\(s\\circ \\pi\\) on \\(\\Omega\\). This abstraction becomes essential for infinite sample spaces, but we won’t need it here.\nReturning to our coin-flip example, we can now compute the entropies of the three algebras \\(\\mathcal{F}_1\\), \\(\\mathcal{F}_2\\), and \\(\\mathcal{F}_3\\) that we constructed. First, we summarize the probabilities of the atoms in each algebra:\n\n\nCode\n# Calculate probabilities on the atoms of F1\n# For algebra F1 with partition {A1, A2}, sum the probabilities of outcomes in each atom\nF1_probs &lt;- Omega %&gt;%\n  group_by(Ai) %&gt;% # Group by atom membership (Ai = 1 or 2)\n  summarise(p = sum(p)) # Sum probabilities of all outcomes in each atom\n\n# Display the probabilities for atoms of F1 as a formatted table\nkable(\n  F1_probs,\n  caption = \"probabilities $P(A_i)$ for the atoms $A_i$ of algebra $\\\\mathcal{F}_1$\"\n)\n\n\n\nprobabilities \\(P(A_i)\\) for the atoms \\(A_i\\) of algebra \\(\\mathcal{F}_1\\)\n\n\nAi\np\n\n\n\n\n1\n0.4\n\n\n2\n0.6\n\n\n\n\n\nCode\n# Calculate probabilities on the atoms of F2\n# For algebra F2 with partition {B1, B2, B3, B4}, sum probabilities for each atom\nF2_probs &lt;- Omega %&gt;%\n  group_by(Bi) %&gt;% # Group by atom membership (Bi = 1, 2, 3, or 4)\n  summarise(p = sum(p)) # Sum probabilities of all outcomes in each atom\n\n# Display the probabilities for atoms of F2 as a formatted table\nkable(\n  F2_probs,\n  caption = \"probabilities $P(B_i)$ for the atoms $B_i$ of algebra $\\\\mathcal{F}_2$\"\n)\n\n\n\nprobabilities \\(P(B_i)\\) for the atoms \\(B_i\\) of algebra \\(\\mathcal{F}_2\\)\n\n\nBi\np\n\n\n\n\n1\n0.16\n\n\n2\n0.24\n\n\n3\n0.24\n\n\n4\n0.36\n\n\n\n\n\nCode\n# Calculate probabilities on the atoms of F3\n# For algebra F3 with partition {C1, ..., C8} (singletons), extract individual outcome probabilities\nF3_probs &lt;- Omega %&gt;%\n  group_by(Ci) %&gt;% # Group by atom membership (Ci = 1 through 8)\n  summarise(p = sum(p)) # Sum probabilities (each atom contains exactly one outcome)\n\n# Display the probabilities for atoms of F3 as a formatted table\n# Since F3 is the power set, each atom Ci corresponds to a single outcome ω\nkable(\n  F3_probs,\n  caption = \"probabilities $P(C_i)$ for the atoms $C_i$ of algebra $\\\\mathcal{F}_3$\"\n)\n\n\n\nprobabilities \\(P(C_i)\\) for the atoms \\(C_i\\) of algebra \\(\\mathcal{F}_3\\)\n\n\nCi\np\n\n\n\n\n1\n0.064\n\n\n2\n0.096\n\n\n3\n0.096\n\n\n4\n0.144\n\n\n5\n0.096\n\n\n6\n0.144\n\n\n7\n0.144\n\n\n8\n0.216\n\n\n\n\n\n\nSince the algebra \\(\\mathcal{F}_3\\) is the power set on \\(\\Omega\\), notice that the probabilties of the atoms in \\(\\mathcal{F}_3\\) are just the probabilities of the individual outcomes in \\(\\Omega\\).\nWe now compute the entropies of the three algebras:\n\n\nCode\n# Calculate entropy using H = -sum(p * log(p))\n# This formula computes Shannon entropy, measuring uncertainty in the distribution\n\n# Calculate marginal entropy H(F1)\n# For algebra F1 with atoms {A1, A2}, compute entropy over the 2-element partition\nH_F1 &lt;- F1_probs %&gt;%\n  mutate(surprisal = -log(p)) %&gt;% # Surprisal: -log(p) for each atom's probability\n  summarise(H = sum(p * surprisal)) %&gt;% # Entropy: expected surprisal\n  pull(H) # Extract the scalar value\n\n# Calculate marginal entropy H(F2)\n# For algebra F2 with atoms {B1, B2, B3, B4}, compute entropy over the 4-element partition\nH_F2 &lt;- F2_probs %&gt;%\n  mutate(surprisal = -log(p)) %&gt;%\n  summarise(H = sum(p * surprisal)) %&gt;%\n  pull(H)\n\n# Calculate marginal entropy H(F3)\n# For algebra F3 with atoms {C1, ..., C8}, compute entropy over the 8-element partition\n# F3 corresponds to complete information (the power set of Omega)\nH_F3 &lt;- F3_probs %&gt;%\n  mutate(surprisal = -log(p)) %&gt;%\n  summarise(H = sum(p * surprisal)) %&gt;%\n  pull(H)\n\n# Create a tibble containing the three entropy values\n# Shows how entropy increases as algebras refine: H(F1) &lt; H(F2) &lt; H(F3)\nmarginal_entropies &lt;- tibble(\n  entropy = c(\"H(F1)\", \"H(F2)\", \"H(F3)\"), # Labels for each algebra\n  value = c(H_F1, H_F2, H_F3) # Corresponding entropy values in nats\n)\n\n# Display the marginal entropies as a formatted table\nkable(\n  marginal_entropies,\n  caption = \"marginal entropies for algebras $\\\\mathcal{F}_1$, $\\\\mathcal{F}_2$, and $\\\\mathcal{F}_3$\"\n)\n\n\n\nmarginal entropies for algebras \\(\\mathcal{F}_1\\), \\(\\mathcal{F}_2\\), and \\(\\mathcal{F}_3\\)\n\n\nentropy\nvalue\n\n\n\n\nH(F1)\n0.6730117\n\n\nH(F2)\n1.3460233\n\n\nH(F3)\n2.0190350\n\n\n\n\n\n\nThus, we see that the entropy increases as we refine our information. This means that uncertainty increases as our information becomes more detailed, which may seem counterintuitive at first glance. However, this is a natural consequence of the way entropy is defined: it measures the expected uncertainty over all possible outcomes, and as we refine our information, we are effectively considering a larger number of possible outcomes (the atoms of the finer algebra), each with its own associated probability. The increased granularity leads to a higher overall uncertainty when averaged across all outcomes.\nWe may visualize this increase in entropy as follows, where reading the plot from left to right corresponds to refining our information from \\(\\mathcal{F}_1\\) to \\(\\mathcal{F}_2\\) to \\(\\mathcal{F}_3\\):\n\n\nCode\n# Create a bar chart visualizing how entropy increases across the filtration\n# Each bar represents the entropy H(Fi) for i = 1, 2, 3\nggplot(marginal_entropies, aes(x = entropy, y = value)) +\n  geom_col(fill = yellow, width = 0.4) + # Yellow bars with reduced width for clarity\n  labs(\n    title = \"entropy increases under refinement\", # Title describes the key pattern\n    x = NULL, # Remove x-axis label (entropy names are self-explanatory)\n    y = \"entropy\" # Label y-axis with the quantity being measured\n  )\n\n\n\n\n\n\n\n\n\nHaving defined the entropy of an algebra, we can now introduce two related concepts: conditional entropy and mutual information. These quantities measure, respectively, the uncertainty remaining in one algebra given knowledge of another, and the amount of information shared between two algebras.\nTo define the conditional entropy, we need two sub-\\(\\sigma\\)-algebras \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\) of a finite probability space \\((\\Omega,\\mathcal{F},P)\\). If the decompositions of \\(\\Omega\\) into the atoms of \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\) are\n\\[\n\\{B_1,B_2,\\ldots,B_n\\} \\quad \\text{and} \\quad \\{C_1,C_2,\\ldots,C_m\\},\n\\]\nrespectively, then the decomposition of \\(\\Omega\\) into the atoms of the algebra \\(\\mathcal{G}\\vee \\mathcal{H}\\), the smallest algebra containing both \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\), is given by the nonempty sets among \\(B_i \\cap C_j\\) for each of \\(i=1,2,\\ldots,n\\) and \\(j=1,2,\\ldots,m\\).\n\n\n\n\n\n\n\nDefinition 4 Let \\((\\Omega,\\mathcal{F},P)\\) be a finite probability space, and let \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\) be two sub-\\(\\sigma\\)-algebras of \\(\\mathcal{F}\\). The conditional entropy of \\(\\mathcal{G}\\) given \\(\\mathcal{H}\\) is the quantity\n\\[\nH(\\mathcal{G}\\mid \\mathcal{H}) = H(\\mathcal{G}\\vee \\mathcal{H}) - H(\\mathcal{H}).\n\\]\n\n\n\n\nThe intuition for this definition is as follows. The entropy \\(H(\\mathcal{H})\\) measures the uncertainty in \\(\\mathcal{H}\\), while \\(H(\\mathcal{G}\\vee \\mathcal{H})\\) measures the uncertainty in the combined information of \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\). The difference between these two quantities, \\(H(\\mathcal{G}\\mid \\mathcal{H})\\), thus represents the uncertainty that remains in \\(\\mathcal{G}\\) once we know everything in \\(\\mathcal{H}\\).\nIn our coin-flip example, three conditional entropies are of interest: \\(H(\\mathcal{F}_3\\mid \\mathcal{F}_i)\\), for each of \\(i=1,2,3\\). Since we have the nested sequence\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3,\n\\]\nnote that\n\\[\nH(\\mathcal{F}_3 \\mid \\mathcal{F}_1) = H(\\mathcal{F}_1 \\vee \\mathcal{F}_3) - H(\\mathcal{F}_1) = H(\\mathcal{F}_3) - H(\\mathcal{F}_1),\n\\tag{4}\\]\nand similarly for \\(H(\\mathcal{F}_3 \\mid \\mathcal{F}_2)\\) and \\(H(\\mathcal{F}_3 \\mid \\mathcal{F}_3)\\). In particular, we have that \\(H(\\mathcal{F}_3 \\mid \\mathcal{F}_3) =0\\), which squares with our intuition that, given the information in \\(\\mathcal{F}_3\\), the uncertainty remaining in \\(\\mathcal{F}_3\\) is zero.\nWe compute all three conditional entropies:\n\n\nCode\n# Create a tibble containing conditional entropy values for the filtration\n# For nested algebras F1 ⊂ F2 ⊂ F3, conditional entropy H(F3 | Fi) = H(F3) - H(Fi)\n# This measures the residual uncertainty in F3 given knowledge of Fi\nconditional_entropies &lt;- tibble(\n  entropy = c(\"H(F3 | F1)\", \"H(F3 | F2)\", \"H(F3 | F3)\"), # Labels for each conditional entropy\n  value = c(H_F3 - H_F1, H_F3 - H_F2, H_F3 - H_F3) # Values: H(F3 | Fi) for each i\n)\n\n# Display the conditional entropies as a formatted table\n# Shows how residual uncertainty in F3 decreases as we refine from F1 to F2 to F3\nkable(\n  conditional_entropies,\n  caption = \"conditional entropies for the filtration $\\\\mathcal{F}_1 \\\\subset \\\\mathcal{F}_2 \\\\subset \\\\mathcal{F}_3$\"\n)\n\n\n\nconditional entropies for the filtration \\(\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3\\)\n\n\nentropy\nvalue\n\n\n\n\nH(F3 | F1)\n1.3460233\n\n\nH(F3 | F2)\n0.6730117\n\n\nH(F3 | F3)\n0.0000000\n\n\n\n\n\n\nThese values show that as we compute \\(H(\\mathcal{F}_3 \\mid \\mathcal{F}_i)\\) along the sequence\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3,\n\\]\nthe conditional entropies decrease. This reflects the fact that as we refine our information (moving from \\(\\mathcal{F}_1\\) to \\(\\mathcal{F}_2\\) to \\(\\mathcal{F}_3\\)), the uncertainty remaining in \\(\\mathcal{F}_3\\) given knowledge of the other algebras diminishes. This aligns perfectly with our intuition: more information leads to less uncertainty.\nFinally, we define the mutual information between two sub-\\(\\sigma\\)-algebras \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\) of a finite probability space \\((\\Omega,\\mathcal{F},P)\\). This quantity measures the amount of information shared between \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\), quantifying how much knowing one reduces uncertainty about the other.\n\n\n\n\n\n\n\nDefinition 5 Let \\((\\Omega,\\mathcal{F},P)\\) be a finite probability space, and let \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\) be two sub-\\(\\sigma\\)-algebras of \\(\\mathcal{F}\\). The mutual information between \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\) is the quantity\n\\[\nI(\\mathcal{G},\\mathcal{H}) = H(\\mathcal{G}) -H(\\mathcal{G}\\mid \\mathcal{H}).\n\\tag{5}\\]\n\n\n\n\nThe intuition for this definition is as follows. The entropy \\(H(\\mathcal{G})\\) measures the uncertainty in \\(\\mathcal{G}\\). The conditional entropy \\(H(\\mathcal{G}\\mid \\mathcal{H})\\) measures the uncertainty remaining in \\(\\mathcal{G}\\) once we know everything in \\(\\mathcal{H}\\). The difference between these two quantities, \\(I(\\mathcal{G},\\mathcal{H})\\), thus represents the reduction in uncertainty about \\(\\mathcal{G}\\) due to knowledge of \\(\\mathcal{H}\\)—in other words, the information that \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\) share.\nReturning once more to our coin-flip example, we compute the three mutual informations \\(I(\\mathcal{F}_3,\\mathcal{F}_i)\\) for \\(i=1,2,3\\). From (4), we have\n\\[\nI(\\mathcal{F}_3, \\mathcal{F}_1) = H(\\mathcal{F}_3) - H(\\mathcal{F}_3 \\mid \\mathcal{F}_1) = H(\\mathcal{F}_3) - (H(\\mathcal{F}_3) - H(\\mathcal{F}_1)) = H(\\mathcal{F}_1),\n\\]\nand similarly for \\(I(\\mathcal{F}_3,\\mathcal{F}_2)\\) and \\(I(\\mathcal{F}_3,\\mathcal{F}_3)\\). We compute all three mutual informations:\n\n\nCode\n# Create a tibble containing mutual information values for each algebra in the filtration\n# For nested algebras F1 ⊂ F2 ⊂ F3, the mutual information I(F3, Fi) equals H(Fi)\n# because Fi contains all the information about F3 that Fi itself possesses\nmutual_infos &lt;- tibble(\n  entropy = c(\"I(F3, F1)\", \"I(F3, F2)\", \"I(F3, F3)\"),  # Labels for each mutual information\n  value = c(H_F1, H_F2, H_F3)  # Values: I(F3, Fi) = H(Fi) for nested algebras\n)\n\n# Display the mutual informations as a formatted table\n# Shows how shared information between F3 and each Fi increases with refinement\nkable(\n  mutual_infos,\n  caption = \"mutual informations for the filtration $\\\\mathcal{F}_1 \\\\subset \\\\mathcal{F}_2 \\\\subset \\\\mathcal{F}_3$\"\n)\n\n\n\nmutual informations for the filtration \\(\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3\\)\n\n\nentropy\nvalue\n\n\n\n\nI(F3, F1)\n0.6730117\n\n\nI(F3, F2)\n1.3460233\n\n\nI(F3, F3)\n2.0190350\n\n\n\n\n\n\nThus, as we compute \\(I(\\mathcal{F}_3,\\mathcal{F}_i)\\) along the sequence\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3,\n\\]\nwe see that the mutual informations increase. This reflects the fact that as we refine our information (moving from \\(\\mathcal{F}_1\\) to \\(\\mathcal{F}_2\\) to \\(\\mathcal{F}_3\\)), the amount of information shared between \\(\\mathcal{F}_3\\) and the other algebras grows. This again aligns perfectly with our intuition: more information leads to greater shared knowledge.\nFrom (5), we have\n\\[\nH(\\mathcal{F}_3) = I(\\mathcal{F}_3,\\mathcal{F}_i) + H(\\mathcal{F}_3 \\mid \\mathcal{F}_i),\n\\]\nfor each \\(i=1,2,3\\), and so the total entropy in \\(\\mathcal{F}_3\\) decomposes into the sum of mutual information and conditional entropy. We visualize this decomposition for each of the three algebras in our filtration:\n\n\nCode\n# Create a data frame decomposing the entropy H(F3) into mutual information and conditional entropy\n# For each algebra F_i (i=1,2,3), we have: H(F3) = I(F3, F_i) + H(F3 | F_i)\ndecomposition &lt;- tibble(\n  algebra = rep(c(\"i=1\", \"i=2\", \"i=3\"), each = 2), # Three algebras, each with two components\n  component = rep(c(\"mutual information\", \"conditional entropy\"), 3), # Two components per algebra\n  entropy = c(H_F1, H_F3 - H_F1, H_F2, H_F3 - H_F2, H_F3, 0) # Values for each component\n)\n\n# Create a stacked bar chart showing the decomposition\n# Each bar represents H(F3) for a different conditioning algebra F_i\nggplot(decomposition, aes(x = algebra, y = entropy, fill = component)) +\n  geom_col(width = 0.4) + # Stacked column chart\n  scale_fill_manual(\n    name = NULL,\n    values = c(\"mutual information\" = blue, \"conditional entropy\" = yellow)\n  ) +\n  labs(\n    title = TeX(\"entropy decomposition $H(F_3) = I(F_3, F_i) + H(F_3 | F_i)$\"),\n    x = NULL\n  )\n\n\n\n\n\n\n\n\n\nThe decomposition \\(H(\\mathcal{F}_3) = I(\\mathcal{F}_3, \\mathcal{F}_i) + H(\\mathcal{F}_3 \\mid \\mathcal{F}_i)\\) reveals a fundamental trade-off: as we refine our information from \\(\\mathcal{F}_1\\) to \\(\\mathcal{F}_2\\) to \\(\\mathcal{F}_3\\), mutual information increases (capturing more shared knowledge with the complete outcome) while conditional entropy decreases (leaving less residual uncertainty). The total entropy \\(H(\\mathcal{F}_3)\\) remains constant, but its composition shifts as information accumulates."
  },
  {
    "objectID": "posts/info-2/index.html#conclusion",
    "href": "posts/info-2/index.html#conclusion",
    "title": "Algebras & information",
    "section": "Conclusion",
    "text": "Conclusion\nWe began this post by asking how to represent an observer’s information mathematically. The answer emerged through two complementary perspectives: algebras of sets, which capture the logical operations available to the observer, and partitions, which describe which outcomes can be distinguished from one another. Theorem 3 showed these views are equivalent—every algebra corresponds to a unique partition of atoms, and vice versa.\nTo quantify information, we introduced entropy as a measure of uncertainty associated with an algebra. The entropy \\(H(\\mathcal{G})\\) tells us how much uncertainty remains when the observer’s knowledge is captured by \\(\\mathcal{G}\\). Our coin-flip example illustrated how entropy increases as partitions refine: knowing the first flip yields \\(H(\\mathcal{F}_1) \\approx 0.67\\), knowing the first two flips yields \\(H(\\mathcal{F}_2) \\approx 1.35\\), and complete knowledge yields \\(H(\\mathcal{F}_3) \\approx 2.02\\).\nWe also introduced conditional entropy \\(H(\\mathcal{G} \\mid \\mathcal{H})\\), measuring residual uncertainty in \\(\\mathcal{G}\\) given knowledge of \\(\\mathcal{H}\\), and mutual information \\(I(\\mathcal{G}, \\mathcal{H})\\), measuring information shared between algebras. These quantities satisfy the fundamental relationship\n\\[\nH(\\mathcal{G}) = I(\\mathcal{G}, \\mathcal{H}) + H(\\mathcal{G} \\mid \\mathcal{H}),\n\\]\ndecomposing total uncertainty into shared information and residual uncertainty.\nThe framework of nested algebras representing evolving information has powerful applications beyond theoretical probability. In future posts, we’ll explore how these ideas illuminate gambling strategies and games of chance, where players accumulate information over time and make decisions under uncertainty. We’ll also see how the same mathematical structure underpins options pricing in mathematical finance, where nested filtrations model the information available to traders at different times, and conditional expectations with respect to these algebras determine fair prices for derivative securities. The interplay between information, uncertainty, and value in these contexts provides a rich testing ground for the concepts developed here."
  },
  {
    "objectID": "posts/info-1/index.html",
    "href": "posts/info-1/index.html",
    "title": "Entropy & information",
    "section": "",
    "text": "Large language models (LLMs) like ChatGPT have brought probabilistic models into mainstream conversation. Unlike deterministic models that always produce the same output for a given input, these models generate a probability distribution over many possible outputs. Information theory provides the mathematical framework for understanding these distributions, and how sensitive they are to the inputs of the model.\nTo understand this more concretely, consider what happens when you prompt an LLM. You might ask “What’s the capital of France?” and reliably get “Paris” as the answer. The model is highly confident: the output probability distribution is concentrated over the one-word output “Paris”. But if you ask “Write me a poem about autumn,” you’ll get a different response each time. Here, the probability distribution is spread across countless possible poems, each with some small probability of being generated. In the first case, the model is certain, reflected in the strongly peaked output distribution, while in the latter case it is uncertain, reflected in the highly dispersed output distribution. The input to the model (i.e., the prompt) thus has a strong effect on the certainty in the output distribution.\nThe gap between certainty and uncertainty lies at the heart of information theory. When Claude Shannon founded the field (see Shannon 1948), he was grappling with a fundamental question in telecommunications: how much data can you reliably transmit through a noisy communication channel? Telephone wires introduce random distortions like static and interference, scrambling the signal in unpredictable ways. Shannon realized he could model this mathematically using probability theory, and what began as a framework for analyzing communication over literal telephone wires evolved into an abstract mathematical theory with remarkably broad applications. Indeed, Shannon has been credited with laying the foundations for our modern information-based society.\nInterestingly, Shannon’s notion of entropy has deep connections to the concept of entropy in statistical mechanics and thermodynamics. In 1957, E. T. Jaynes famously formalized this connection in his influential paper (Jaynes 1957), where he wrote:\n\n“The mere fact that the same mathematical expression \\(-\\sum p_i \\log{p_i}\\) [for entropy] occurs both in statistical mechanics and in information theory does not in itself establish any connection between these fields. This can be done only by finding new viewpoints from which thermodynamic entropy and information-theory entropy appear as the same concept. In this paper we suggest a reinterpretation of statistical mechanics which accomplishes this, so that information theory can be applied to the problem of justification of statistical mechanics.”\n\nEven my undergraduate thermodynamics textbook devoted an entire chapter to Shannon’s information theory, emphasizing how these mathematical ideas provide a unifying perspective across seemingly different domains.\nWe will begin by surveying the most basic quantities of information theory: surprisal, entropy, Kullback–Leibler (KL) divergence, and mutual information. Surprisal is a quantity attached to a random outcome that is inversely linked to its probability: a likely outcome is not surprising, while an unlikely one is highly surprising. When we average surprisal over all possible random outcomes, we obtain the entropy, which quantifies the overall uncertainty in a probability distribution. The KL divergence measures how one probability distribution differs from another, capturing the “distance” between them. Mutual information can be viewed as a special kind of KL divergence applied to two random variables, \\(X\\) and \\(Y\\): it measures how much observing \\(X\\) reduces the uncertainty (entropy) in \\(Y\\) on average. This last quantity connects directly to our discussion of LLMs: it captures precisely how much the input prompt narrows down the output distribution.\nThis post is the first in a series on information. In future posts, we will explore other ways the concept of information appears—for example, through \\(\\sigma\\)-algebras—and apply these ideas to a range of problems, from gambling strategies and games of chance (the historical origin of mathematical probability theory), to options pricing in mathematical finance, and to probabilistic models in machine learning. I have discussed information theory previously in a chapter of my book; while some material overlaps with that chapter, this series also introduces many new perspectives and examples.\nIf you’d like to follow along with the code examples in this post, please see the dropdown code block below for the usual imports and setup.\n\n\nCode\n# Import probability distributions, integration, and plotting libraries\nfrom scipy.stats import norm, multivariate_normal, beta, poisson, binom, entropy\nfrom scipy.integrate import quad\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib.gridspec as gridspec\n\n\n# Set custom matplotlib style (user must use their own style file)\nplt.style.use(\"../../aux-files/custom-theme.mplstyle\")\n\n# Define color palette for plots\nyellow = \"#FFC300\"\nblue = \"#3399FF\"\npurple = \"#AA77CC\"\n\n\nclass RV:\n    \"\"\"\n    A class representing a random variable (discrete or continuous), with optional support for conditional densities.\n\n    Attributes\n    ----------\n    support : array-like or None\n        The support of the random variable (e.g., possible values for discrete, grid for continuous).\n    density : callable\n        The marginal density or mass function.\n    cond_density : callable or None\n        The conditional density function, if provided.\n    cond_support : array-like or None\n        The support for the conditional variable, if applicable.\n    density_array : np.ndarray or None\n        The marginal density evaluated on the support, if support is array-like.\n    _cond_density_array : dict or None\n        Precomputed conditional densities, if available.\n    \"\"\"\n\n    def __init__(\n        self,\n        support=None,\n        density=None,\n        cond_density=None,\n        cond_support=None,\n    ):\n        \"\"\"\n        Initialize an RV object.\n\n        Parameters\n        ----------\n        support : array-like or None\n            The support of the random variable (e.g., possible values for discrete, grid for continuous).\n        density : callable\n            The marginal density or mass function. Should accept a value (or array of values) and return the density/mass.\n        cond_density : callable, optional\n            The conditional density function f(x|y). Should accept (x, y) and return the density of x given y.\n        cond_support : array-like or None, optional\n            The support for the conditional variable, if applicable.\n        \"\"\"\n        self.support = support\n        self.density = density\n        self.cond_density = cond_density\n        self.cond_support = cond_support\n\n        # Precompute the marginal density array if possible\n        if support is not None and density is not None:\n            self.density_array = np.array([density(x) for x in support])\n        else:\n            self.density_array = None\n\n        # Precompute the conditional density array as a dictionary, if possible\n        if (\n            support is not None\n            and density is not None\n            and cond_density is not None\n            and cond_support is not None\n        ):\n            self._cond_density_array = {\n                y: cond_density(support, y) for y in cond_support\n            }\n        else:\n            self._cond_density_array = None\n\n    def pdf(self, x):\n        \"\"\"\n        Evaluate the marginal density or mass function at x.\n        \"\"\"\n        return self.density(x)\n\n    def pmf(self, x):\n        \"\"\"\n        Alias for pdf, for discrete random variables.\n        \"\"\"\n        return self.pdf(x)\n\n    def set_cond_density(self, cond_density):\n        \"\"\"\n        Set the conditional density function f(x|y).\n        \"\"\"\n        self.cond_density = cond_density\n\n    def cond_pdf(self, x, y):\n        \"\"\"\n        Evaluate the conditional density f(x|y).\n        Raises a ValueError if the conditional density function is not set.\n        \"\"\"\n        if self.cond_density is None:\n            raise ValueError(\"Conditional density function not set.\")\n        return self.cond_density(x, y)\n\n    def cond_density_array(self, y):\n        \"\"\"\n        Get the conditional density array f(x|y) for a fixed y.\n        Raises a ValueError if the conditional density array is not precomputed.\n        \"\"\"\n        if self._cond_density_array is None:\n            raise ValueError(\"Conditional density array not precomputed.\")\n        return self._cond_density_array[y]"
  },
  {
    "objectID": "posts/info-1/index.html#introduction",
    "href": "posts/info-1/index.html#introduction",
    "title": "Entropy & information",
    "section": "",
    "text": "Large language models (LLMs) like ChatGPT have brought probabilistic models into mainstream conversation. Unlike deterministic models that always produce the same output for a given input, these models generate a probability distribution over many possible outputs. Information theory provides the mathematical framework for understanding these distributions, and how sensitive they are to the inputs of the model.\nTo understand this more concretely, consider what happens when you prompt an LLM. You might ask “What’s the capital of France?” and reliably get “Paris” as the answer. The model is highly confident: the output probability distribution is concentrated over the one-word output “Paris”. But if you ask “Write me a poem about autumn,” you’ll get a different response each time. Here, the probability distribution is spread across countless possible poems, each with some small probability of being generated. In the first case, the model is certain, reflected in the strongly peaked output distribution, while in the latter case it is uncertain, reflected in the highly dispersed output distribution. The input to the model (i.e., the prompt) thus has a strong effect on the certainty in the output distribution.\nThe gap between certainty and uncertainty lies at the heart of information theory. When Claude Shannon founded the field (see Shannon 1948), he was grappling with a fundamental question in telecommunications: how much data can you reliably transmit through a noisy communication channel? Telephone wires introduce random distortions like static and interference, scrambling the signal in unpredictable ways. Shannon realized he could model this mathematically using probability theory, and what began as a framework for analyzing communication over literal telephone wires evolved into an abstract mathematical theory with remarkably broad applications. Indeed, Shannon has been credited with laying the foundations for our modern information-based society.\nInterestingly, Shannon’s notion of entropy has deep connections to the concept of entropy in statistical mechanics and thermodynamics. In 1957, E. T. Jaynes famously formalized this connection in his influential paper (Jaynes 1957), where he wrote:\n\n“The mere fact that the same mathematical expression \\(-\\sum p_i \\log{p_i}\\) [for entropy] occurs both in statistical mechanics and in information theory does not in itself establish any connection between these fields. This can be done only by finding new viewpoints from which thermodynamic entropy and information-theory entropy appear as the same concept. In this paper we suggest a reinterpretation of statistical mechanics which accomplishes this, so that information theory can be applied to the problem of justification of statistical mechanics.”\n\nEven my undergraduate thermodynamics textbook devoted an entire chapter to Shannon’s information theory, emphasizing how these mathematical ideas provide a unifying perspective across seemingly different domains.\nWe will begin by surveying the most basic quantities of information theory: surprisal, entropy, Kullback–Leibler (KL) divergence, and mutual information. Surprisal is a quantity attached to a random outcome that is inversely linked to its probability: a likely outcome is not surprising, while an unlikely one is highly surprising. When we average surprisal over all possible random outcomes, we obtain the entropy, which quantifies the overall uncertainty in a probability distribution. The KL divergence measures how one probability distribution differs from another, capturing the “distance” between them. Mutual information can be viewed as a special kind of KL divergence applied to two random variables, \\(X\\) and \\(Y\\): it measures how much observing \\(X\\) reduces the uncertainty (entropy) in \\(Y\\) on average. This last quantity connects directly to our discussion of LLMs: it captures precisely how much the input prompt narrows down the output distribution.\nThis post is the first in a series on information. In future posts, we will explore other ways the concept of information appears—for example, through \\(\\sigma\\)-algebras—and apply these ideas to a range of problems, from gambling strategies and games of chance (the historical origin of mathematical probability theory), to options pricing in mathematical finance, and to probabilistic models in machine learning. I have discussed information theory previously in a chapter of my book; while some material overlaps with that chapter, this series also introduces many new perspectives and examples.\nIf you’d like to follow along with the code examples in this post, please see the dropdown code block below for the usual imports and setup.\n\n\nCode\n# Import probability distributions, integration, and plotting libraries\nfrom scipy.stats import norm, multivariate_normal, beta, poisson, binom, entropy\nfrom scipy.integrate import quad\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib.gridspec as gridspec\n\n\n# Set custom matplotlib style (user must use their own style file)\nplt.style.use(\"../../aux-files/custom-theme.mplstyle\")\n\n# Define color palette for plots\nyellow = \"#FFC300\"\nblue = \"#3399FF\"\npurple = \"#AA77CC\"\n\n\nclass RV:\n    \"\"\"\n    A class representing a random variable (discrete or continuous), with optional support for conditional densities.\n\n    Attributes\n    ----------\n    support : array-like or None\n        The support of the random variable (e.g., possible values for discrete, grid for continuous).\n    density : callable\n        The marginal density or mass function.\n    cond_density : callable or None\n        The conditional density function, if provided.\n    cond_support : array-like or None\n        The support for the conditional variable, if applicable.\n    density_array : np.ndarray or None\n        The marginal density evaluated on the support, if support is array-like.\n    _cond_density_array : dict or None\n        Precomputed conditional densities, if available.\n    \"\"\"\n\n    def __init__(\n        self,\n        support=None,\n        density=None,\n        cond_density=None,\n        cond_support=None,\n    ):\n        \"\"\"\n        Initialize an RV object.\n\n        Parameters\n        ----------\n        support : array-like or None\n            The support of the random variable (e.g., possible values for discrete, grid for continuous).\n        density : callable\n            The marginal density or mass function. Should accept a value (or array of values) and return the density/mass.\n        cond_density : callable, optional\n            The conditional density function f(x|y). Should accept (x, y) and return the density of x given y.\n        cond_support : array-like or None, optional\n            The support for the conditional variable, if applicable.\n        \"\"\"\n        self.support = support\n        self.density = density\n        self.cond_density = cond_density\n        self.cond_support = cond_support\n\n        # Precompute the marginal density array if possible\n        if support is not None and density is not None:\n            self.density_array = np.array([density(x) for x in support])\n        else:\n            self.density_array = None\n\n        # Precompute the conditional density array as a dictionary, if possible\n        if (\n            support is not None\n            and density is not None\n            and cond_density is not None\n            and cond_support is not None\n        ):\n            self._cond_density_array = {\n                y: cond_density(support, y) for y in cond_support\n            }\n        else:\n            self._cond_density_array = None\n\n    def pdf(self, x):\n        \"\"\"\n        Evaluate the marginal density or mass function at x.\n        \"\"\"\n        return self.density(x)\n\n    def pmf(self, x):\n        \"\"\"\n        Alias for pdf, for discrete random variables.\n        \"\"\"\n        return self.pdf(x)\n\n    def set_cond_density(self, cond_density):\n        \"\"\"\n        Set the conditional density function f(x|y).\n        \"\"\"\n        self.cond_density = cond_density\n\n    def cond_pdf(self, x, y):\n        \"\"\"\n        Evaluate the conditional density f(x|y).\n        Raises a ValueError if the conditional density function is not set.\n        \"\"\"\n        if self.cond_density is None:\n            raise ValueError(\"Conditional density function not set.\")\n        return self.cond_density(x, y)\n\n    def cond_density_array(self, y):\n        \"\"\"\n        Get the conditional density array f(x|y) for a fixed y.\n        Raises a ValueError if the conditional density array is not precomputed.\n        \"\"\"\n        if self._cond_density_array is None:\n            raise ValueError(\"Conditional density array not precomputed.\")\n        return self._cond_density_array[y]"
  },
  {
    "objectID": "posts/info-1/index.html#flows-of-information",
    "href": "posts/info-1/index.html#flows-of-information",
    "title": "Entropy & information",
    "section": "Flows of information",
    "text": "Flows of information\nWe begin by building a mathematical gadget—a kind of probabilistic framework—that models the “flow of information” between two random variables \\(X\\) and \\(Y\\) (or random vectors, or random objects, or …). Such flows are exactly what information theory calls communication channels, and they include many of the predictive probabilistic models in machine learning where information flows from input \\(X\\) to output \\(Y\\). Once these flows have been identified in this section, in the rest of the post we seek to understand how information flowing from \\(X\\) influences and shapes the distribution of \\(Y\\).\nThe simplest flow between \\(X\\) and \\(Y\\) is a functional one, expressed as an equation \\[\ng(X)=Y,\n\\tag{1}\\]\nwhere \\(g\\) is a function. With \\(X\\) as input and \\(Y\\) as output, each \\(X=x\\) produces a unique output \\(y = g(x)\\). Such flows underlie deterministic models. In the case that \\(X\\) and \\(Y\\) take values on the real line \\(\\mathbb{R}\\), we might visualize the situation like this:\n\n\n\n\n\nNote that each value of \\(x\\) along the input (left) axis determines a unique value of \\(y\\) along the output (right) axis.\nOn the other hand, we might suppose that information flows from \\(X\\) to \\(Y\\) in a stochastic fashion, in which \\(X=x\\) no longer determines a single \\(y\\), but instead induces a distribution over possible \\(Y\\) values. This is precisely what a conditional distribution \\(P(Y= y\\mid X=x)\\) captures: given an observed value \\(X=x\\), we have a probability distribution on \\(y\\)’s. We can think of this as a function of the form\n\\[\nx \\mapsto P(Y= y \\mid X=x),\n\\tag{2}\\]\nwhere \\(y\\) plays the role of a variable rather than a fixed quantity, so that \\(P(Y= y \\mid X=x)\\) is a probability distribution and not just a single probability. So this function is rather special: its input is a value \\(x\\), while its output is an entire probability distribution. Mathematicians call such objects Markov kernels. A figure will make this more concrete: in contrast to the deterministic flows drawn above, in the case that \\(X\\) and \\(Y\\) take values on the real line \\(\\mathbb{R}\\), we visualize a stochastic flow as follows, where each value of \\(x\\) is mapped to a probability distribution on \\(y\\)’s:\n\n\n\n\n\nIn our model of a stochastic flow, we therefore use conditional distributions, understood as functions on the conditioning variable.\nIn practice, we often model such flows with a family \\(P(Y=y;\\theta)\\) of distributions parameterized by a vector \\(\\theta\\). The stochastic flow from \\(X\\) to \\(Y\\) is then implemented as a function \\(x\\mapsto \\theta(x)\\) from observations of \\(X\\) to parameters \\(\\theta\\), and the conditional distribution is then defined as\n\\[\nP(Y=y \\mid X=x) = P(Y=y ; \\theta=\\theta(x)).\n\\]\nLinear regression (with known variance \\(\\sigma^2\\)) is a familiar example: here \\(P(Y=y;\\theta)\\) has the normal density\n\\[\nf(y;\\theta) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left[ - \\frac{1}{2\\sigma^2}(y-\\theta)^2 \\right],\n\\]\nand with parameter mapping\n\\[\nx\\mapsto \\theta(x) = \\beta_0 + \\beta_1x,\n\\]\nfor some model coefficients \\(\\beta_0\\) and \\(\\beta_1\\). Concisely, the stochastic flow from \\(X\\) to \\(Y\\) in a linear regression model is completely described by specifying\n\\[\n(Y\\mid X=x) \\sim \\mathcal{N}(\\beta_0+\\beta_1x, \\sigma^2).\n\\]\nWe will return to an information-theoretic treatment of linear regression (and other) models in a later post.\nFor now, let’s see all this in action with real distributions in a real-world context. Suppose that \\(X\\) is the number of hours that a randomly chosen student studies for an upcoming exam, restricted to discrete values \\(X = 1,2,3,4,5,6\\). Let’s say that most students study only 2 or 3 hours, with its full distribution (mass function \\(f(x)\\)) shown below:\n\n\nCode\n# Compute the probability mass function for X (hours studied) using a Poisson distribution with mean 3\nfx_array = poisson.pmf(range(1, 7), mu=3)\nfx_array /= fx_array.sum()  # Normalize so probabilities sum to 1\n\n# Define a function to look up the probability for a given value of X\nfx = lambda x: fx_array[x - 1]\n\n# Create an RV object for X, specifying its support and marginal mass function\nX = RV(support=range(1, 7), density=fx)\n\n# Create a new matplotlib figure and axis with a specified size\n_, ax = plt.subplots(figsize=(6, 4))\n\n# Plot a bar chart of the probability mass function for X\nax.bar(X.support, X.density_array, width=0.4, zorder=2)\n\n# Label the x-axis as \"hours studied (x)\"\nax.set_xlabel(r\"hours studied ($x$)\")\n\n# Label the y-axis as \"probability\"\nax.set_ylabel(\"probability\")\n\n# Set the plot title to indicate this is the marginal mass function f(x)\nax.set_title(r\"marginal mass $f(x)$\")\n\n# Set the x-axis ticks to match the possible values of X\nax.set_xticks(X.support)\n\n# Adjust layout for better appearance and display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe might reasonably believe that \\(X\\) is predictive of \\(Y\\), the exam score of a randomly chosen student, taking continuous values in the interval \\([0,1]\\), understood as percentages. The corresponding marginal density \\(f(y)\\) is shown below:\n\n\nCode\n# Define the conditional density function fy_given_x(y, x) as a Beta(x, 3) distribution\nfy_given_x = lambda y, x: beta.pdf(y, a=x, b=3)\n\n# Define the marginal density function fy(y) as a mixture over x, weighted by fx(x)\nfy = lambda y: sum([fy_given_x(y, x) * fx(x) for x in range(1, 7)])\n\n# Create an RV object for Y, specifying its support, marginal density, and conditional density\nY = RV(\n    support=np.linspace(0, 1, num=250),  # Grid of possible y values (test scores)\n    density=fy,  # Marginal density function for Y\n    cond_density=fy_given_x,  # Conditional density function fy_given_x(y, x)\n    cond_support=range(1, 7),  # Possible values of x (hours studied)\n)\n\n# Create a new matplotlib figure and axis for plotting the marginal density of Y\n_, ax = plt.subplots(figsize=(6, 4))\n\n# Plot the marginal density fy(y) over the grid of y values\nax.plot(Y.support, Y.density_array)\n\n# Shade the area under the density curve for visual emphasis\nax.fill_between(Y.support, Y.density_array, zorder=2, alpha=0.1)\n\n# Format the x-axis labels as percentages (since y is a proportion)\nax.xaxis.set_major_formatter(PercentFormatter(xmax=1))\n\n# Set the plot title and axis labels\nax.set_title(r\"marginal density $f(y)$\")\nax.set_xlabel(\"test score ($y$)\")\nax.set_ylabel(\"probability density\")\n\n# Adjust layout for better appearance and display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nTogether, \\(X\\) and \\(Y\\) have a joint mass/density function \\(f(x,y)\\), visualized in the following ridgeline plot, where each of the horizontal density curves shows \\(f(x,y)\\) as a function of \\(y\\), for fixed \\(x=1,2,3,4,5,6\\).\n\n\nCode\n# Create a new matplotlib figure and axis with a specified size\n_, ax = plt.subplots(figsize=(6, 5))\n\n# Create a custom colormap for conditional distributions\nconditional_stops = [blue, purple, yellow]\nconditional_cmap = LinearSegmentedColormap.from_list(\n    \"conditional_cmap\", conditional_stops\n)\n\n# Generate a list of colors for each value of x using the custom colormap\nconditional_colors = [conditional_cmap(i / 5) for i in range(6)]\n\n# Loop over each possible value of x\nfor x in X.support:\n    # Compute the joint density values for each x, scaled for visualization\n    # This is f(y|x) * f(x), scaled for the ridgeline effect\n    joint_vals = 1.7 * Y.cond_density_array(x) * X.pdf(x)\n\n    # Fill the area between the baseline (x) and the curve (x + joint_vals) for ridgeline effect\n    ax.fill_between(\n        Y.support,\n        x,\n        x + joint_vals,\n        color=conditional_colors[x - 1],\n        zorder=2,\n        alpha=0.1,\n    )\n\n    # Plot the top edge of the density curve for each x\n    ax.plot(Y.support, x + joint_vals, color=conditional_colors[x - 1], zorder=2)\n\n# Label the y-axis as \"hours studied (x)\"\nax.set_ylabel(r\"hours studied ($x$)\")\n\n# Label the x-axis as \"test score (y)\"\nax.set_xlabel(r\"test score ($y$)\")\n\n# Format the x-axis labels as percentages\nax.xaxis.set_major_formatter(PercentFormatter(xmax=1))\n\n# Set the plot title to indicate this is the joint mass/density f(x, y)\nax.set_title(r\"joint mass/density $f(x,y)$\")\n\n# Adjust layout for better appearance and display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nDividing the joint mass/density \\(f(x,y)\\) by the marginal mass \\(f(x)\\) yields the conditional densities \\(f(y|x)\\). These are just the same density curves in the ridgeline plot above, normalized so that they integrate to \\(1\\) over \\([0,1]\\). They are shown in:\n\n\nCode\n# Create a new matplotlib figure and axis with a specified size\n_, ax = plt.subplots(figsize=(6, 4))\n\n# Loop over each possible value of x (hours studied)\nfor x in X.support:\n    # Plot the conditional density f(y|x) for each x as a Beta(x, 3) distribution\n    ax.plot(\n        Y.support,\n        Y.cond_density_array(x),\n        color=conditional_colors[x - 1],\n        label=x\n    )\n\n# Add a legend indicating the value of x for each curve\nax.legend(title=r\"hours studied ($x$)\", loc=\"center left\", bbox_to_anchor=(1, 0.5))\n\n# Format the x-axis labels as percentages (since y is a proportion)\nax.xaxis.set_major_formatter(PercentFormatter(xmax=1))\n\n# Set the plot title to indicate these are conditional densities f(y|x)\nax.set_title(r\"conditional densities $f(y|x)$\")\n\n# Label the axes\nax.set_xlabel(r\"test score ($y$)\")\nax.set_ylabel(\"probability density\")\n\n# Adjust layout for better appearance and display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn our information-theoretic terminology, the conditional density\n\\[\nx\\mapsto f(y|x),\n\\]\nthought of as a function of \\(x\\), models the stochastic flow of information from \\(X\\) to \\(Y\\).\nThe marginal density \\(f(y)\\) shows moderate uncertainty—it’s somewhat peaked, but not sharply. An exam score randomly drawn from \\(Y\\) will be mildly uncertain, mildly surprising. The exact amount of uncertainty in \\(Y\\) will be measured through its entropy, denoted \\(H(Y)\\), introduced in the next section. In contrast, the conditional densities \\(f(y|x)\\) exhibit less uncertainty compared to the marginal, especially for values of \\(x\\) closer to \\(6\\). The uncertainty remaining in \\(Y\\) after observing \\(X=x\\) is measured by the conditional entropy, denoted \\(H(Y\\mid X=x)\\). Averaging this conditional entropy over \\(X\\) yields the quantity\n\\[\nH(Y\\mid X) \\overset{\\text{def}}{=}E_{x\\sim f(x)}(H(Y\\mid X=x)),\n\\]\nthe average amount of uncertainty in \\(Y\\), given \\(X\\). Then, it is a general observation that\n\\[\nH(Y) \\geq H(Y\\mid X)\n\\]\nfor any pair of random variables \\(X\\) and \\(Y\\), reflecting the obvious fact that no additional information will ever increase the uncertainty in \\(Y\\). Thus, the quantity\n\\[\nI(X,Y) \\overset{\\text{def}}{=}H(Y) - H(Y\\mid X)\n\\]\nis a nonnegative proxy for the amount of information transmitted from \\(X\\) to \\(Y\\): if it is large, then the gap between \\(H(Y)\\) and \\(H(Y\\mid X)\\) is wide, indicating that observations of \\(X\\) greatly reduce the uncertainty in \\(Y\\). We understand this as a “large amount of information” is transmitted from \\(X\\) to \\(Y\\). Conversely, when \\(I(X,Y)\\) is small, observations of \\(X\\) reveal little about \\(Y\\); in the extreme case \\(I(X,Y)=0\\), the two are independent. The quantity \\(I(X,Y)\\) is exactly the mutual information between \\(X\\) and \\(Y\\), introduced in the next section."
  },
  {
    "objectID": "posts/info-1/index.html#surprisal-and-entropy",
    "href": "posts/info-1/index.html#surprisal-and-entropy",
    "title": "Entropy & information",
    "section": "Surprisal and entropy",
    "text": "Surprisal and entropy\nAs mentioned in the introduction, entropy measures the uncertainty in the outcome of a random variable. More precisely, it is the average surprisal of an observation. Surprisal varies inversely with probability: large probabilities yield small surprisals, and small probabilities yield large ones.\nThis inverse relationship is given by the function \\(s = -\\log{p}\\), linking a probability \\(p\\in [0,1]\\) with a surprisal \\(s\\in [0,\\infty)\\). The graph of this relationship is shown in:We write \\(\\log\\) for the base-\\(e\\) logarithm.\n\n\nCode\n# Create a grid of probability values from 0.01 to 1 (avoiding 0 to prevent log(0))\nmesh = np.linspace(0.01, 1, num=100)\n\n# Create a new matplotlib figure and axis with a specified size\n_, ax = plt.subplots(figsize=(4, 3))\n\n# Plot the surprisal function s = -log(p) as a function of probability p\nax.plot(mesh, -np.log(mesh), color=yellow)\n\n# Label the x-axis as probability (p)\nax.set_xlabel(r\"probability ($p$)\")\n\n# Label the y-axis as surprisal (s)\nax.set_ylabel(r\"surprisal ($s$)\")\n\n# Adjust layout for better appearance and display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAt first glance, many functions might seem equally capable of expressing this inverse relationship between probability and surprisal—so why the choice of base-\\(e\\) logarithm? It turns out that if one starts from a few natural axioms that any reasonable notion of surprisal should satisfy, then you can prove all such surprisal functions must be proportional to negative logarithms (see, for example, the discussion in Section 9 in Rioul 2021). The choice of base \\(e\\) is then somewhat arbitrary, akin to choosing units. Another popular choice is base \\(2\\), which aligns naturally with bit strings in coding theory. In base \\(e\\), information content is measured in so-called natural units, or nats; in base \\(2\\), it is measured in binary units, or bits. (See Section 10 in the aforementioned reference Rioul 2021 for more on units.)\nThis link between surprisals and probabilities may be extended to a link between surprisal and probability densities in the case that the probabilities are continuous. Since it is inconvenient to continually distinguish between mass and density functions in all definitions and theorems, we will follow the convention in measure-theoretic probability theory and refer to all probability mass and density functions as densities and denote them all by \\(f\\). In this scheme, a probability mass function really is a density function relative to the counting measure.\nWith this convention in mind, the following definition applies to both discrete and continuous random variables:\n\n\n\n\n\n\n\nDefinition 1 Let \\(X\\) and \\(Y\\) be two random variables with density functions \\(f(x)\\) and \\(f(y)\\), respectively.\n\nThe surprisal of an observed value \\(X=x\\) is the quantity \\[\n  s(x) = -\\log{f(x)}.\n  \\]\nThe conditional surprisal of an observed value \\(Y=y\\), given \\(X=x\\), is the quantity \\[\n  s(y|x) = -\\log{f(y|x)},\n  \\] where \\(f(y|x)\\) is the conditional density of \\(Y\\) given \\(X\\).\n\n\n\n\n\nFor a simple example of the relationship between discrete probabilities and surprisals, let’s bring back our random variable \\(X\\) from the previous section, which tallied the number of hours a randomly chosen student studied for the upcoming exam:\n\n\nCode\n# Create a figure with two subplots side by side\nfig, axes = plt.subplots(ncols=2, figsize=(6, 3))\n\n# Plot the marginal probability mass function f(x) as a bar chart on the first subplot\naxes[0].bar(X.support, X.density_array, width=0.4, zorder=2)\naxes[0].set_ylabel(r\"probability mass\")  # Label y-axis\naxes[0].set_title(r\"marginal density $f(x)$\")  # Set subplot title\naxes[0].set_xticks(X.support)  # Set x-ticks to match possible values of x\n\n# Plot the marginal surprisal s(x) = -log(f(x)) as a bar chart on the second subplot\naxes[1].bar(X.support, -np.log(X.density_array), width=0.4, zorder=2)\naxes[1].set_ylabel(r\"surprisal mass\")  # Label y-axis\naxes[1].set_title(r\"marginal surprisal $s(x)$\")  # Set subplot title\naxes[1].set_xticks(X.support)  # Set x-ticks to match possible values of x\n\n# Add a shared x-axis label for both subplots\nfig.supxlabel(r\"hours studied ($x$)\")\n\n# Adjust layout for better appearance and display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nBecause a probability density (mass) function of a discrete random variable must take values in \\([0,1]\\), its surprisal function is never negative. However, the probability density function of a continuous random variable may take on values larger than \\(1\\), which means that the associated surprisal density function can be negative. This can be seen for the continuous random variable \\(Y\\) from the previous section, whose density can exceed \\(1\\) in regions of high concentration—hence its surprisal can dip below zero.\n\n\nCode\n# Create a figure with two subplots side by side\nfig, axes = plt.subplots(ncols=2, figsize=(6, 3))\n\n# Plot the marginal probability density function f(y) on the first subplot\naxes[0].plot(Y.support, Y.density_array)\naxes[0].xaxis.set_major_formatter(\n    PercentFormatter(xmax=1)\n)  # Format x-axis as percentages\naxes[0].set_title(r\"marginal density $f(y)$\")  # Set subplot title\naxes[0].set_ylabel(\"probability density\")  # Label y-axis\n\n# Plot the marginal surprisal s(y) = -log(f(y)) on the second subplot\naxes[1].plot(Y.support[:-1], -np.log(Y.density_array[:-1]))\naxes[1].xaxis.set_major_formatter(\n    PercentFormatter(xmax=1)\n)  # Format x-axis as percentages\naxes[1].set_title(r\"marginal surprisal $s(y)$\")  # Set subplot title\naxes[1].set_ylabel(\"surprisal density\")  # Label y-axis\n\n# Add a shared x-axis label for both subplots\nfig.supxlabel(\"test score ($y$)\")\n\n# Adjust layout for better appearance and display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nHaving defined surprisal for individual outcomes, entropy emerges naturally as its average—capturing the typical “surprise” we can expect.\n\n\n\n\n\n\n\nDefinition 2 Let \\(X\\) and \\(Y\\) be two random variables with density functions \\(f(x)\\) and \\(f(y)\\), respectively.\n\nThe entropy of \\(X\\) is the quantity \\[\n  H(X) = E_{x\\sim f(x)}(s(x)).\n  \\]\nThe conditional entropy of \\(Y\\), given an observed value \\(X=x\\), is the quantity \\[\n  H(Y\\mid X=x) = E_{y\\sim f(y|x)}(s(y\\mid x)),\n  \\] where \\(f(y|x)\\) is the conditional density of \\(Y\\) given \\(X\\).\nThe conditional entropy of \\(Y\\), given \\(X\\), is the quantity\n\n\\[\nH(Y\\mid X) = E_{x\\sim f(x)}(H(Y\\mid X=x)).\n\\]\n\n\n\n\nIn the case that \\(X\\) is discrete, then the entropy \\(H(X)\\) is a sum of either a finite or countably infinite number of terms:\n\\[\nH(X) = \\sum_{x\\in \\mathbb{R}} f(x)s(x) = - \\sum_{x\\in \\mathbb{R}} f(x) \\log{f(x)}.\n\\]\nIf \\(X\\) is continuous, then the entropy is an integral:\n\\[\nH(X) = \\int f(x) s(x) \\ dx = - \\int f(x) \\log{f(x)} \\ dx,\n\\]\nwhere, by convention, \\(\\int\\) denotes integration over \\(\\mathbb{R}\\). In the literature, the entropy of a continuous random variable is often called differential entropy.\nThe stats submodule of the SciPy library contains a convenient method called entropy for computing entropies of discrete random variables. We use it to compute the entropy \\(H(X)\\), where \\(X\\) is the “hours studied” random variable:\n\nprint(f\"The probability mass function f(x) of X is:\\n\")\nfor x in X.support:\n    # Print the probability mass for each possible value of X (1 through n)\n    print(f\"    f({x}) =\", round(X.pdf(x), 3))\n\n# Compute and print the entropy H(X) using scipy's entropy function\nprint(f\"\\nThe entropy H(X) is {entropy(X.density_array):.3f}.\")\n\nThe probability mass function f(x) of X is:\n\n    f(1) = 0.163\n    f(2) = 0.244\n    f(3) = 0.244\n    f(4) = 0.183\n    f(5) = 0.11\n    f(6) = 0.055\n\nThe entropy H(X) is 1.698.\n\n\nWe can use the quad method in the integrate submodule of SciPy to compute differential entropies. For the “exam score” random variable \\(Y\\), we compute:\n\n# Compute the differential entropy H(Y) by integrating -f(y) * log(f(y)) over [0, 1]\ndiff_entropy, _ = quad(func=lambda y: -Y.pdf(y) * np.log(Y.pdf(y)), a=0, b=1)\n\n# Print the computed differential entropy value\nprint(f\"The differential entropy H(Y) is {diff_entropy:.3f}.\")\n\nThe differential entropy H(Y) is -0.131.\n\n\nNotice that \\(H(Y)\\) turns out to be negative—a reminder that differential entropy behaves quite differently from its discrete cousin."
  },
  {
    "objectID": "posts/info-1/index.html#kullbackleibler-divergence-and-mutual-information",
    "href": "posts/info-1/index.html#kullbackleibler-divergence-and-mutual-information",
    "title": "Entropy & information",
    "section": "Kullback–Leibler divergence and mutual information",
    "text": "Kullback–Leibler divergence and mutual information\nIn this section, we develop an information-theoretic way to measure how “far apart” two probability distributions are. By way of motivation, we consider two probability measures on a single finite probability space \\(\\Omega\\), so that the two measures have mass functions \\(f(\\omega)\\) and \\(g(\\omega)\\). The metric we’ll use is the mean logarithmic relative magnitude, a measure that captures not the absolute difference between probabilities, but how one probability scales relative to another. To define it, we first define the absolute relative magnitude of the probability \\(f(\\omega)\\) to the probability \\(g(\\omega)\\) as the ratio \\(f(\\omega)/g(\\omega)\\). Then, logarithmic relative magnitude refers to the base-\\(e\\) logarithm of the absolute relative magnitude:\n\\[\n\\log\\left( \\frac{f(\\omega)}{g(\\omega)} \\right).\n\\]\nIntuitively, this logarithm tells us the “order of magnitude” difference between \\(f(\\omega)\\) and \\(g(\\omega)\\). If \\(f(\\omega)\\approx e^k\\) and \\(g(\\omega)\\approx e^l\\), then the log ratio is roughly \\(k-l\\).\nPerhaps the most obvious immediate benefit of introducing the logarithm is that it yields a workable number when \\(f(\\omega)\\) and \\(g(\\omega)\\) each have widely different magnitudes. For example, let’s suppose that the mass functions \\(f(\\omega)\\) and \\(g(\\omega)\\) are given by\n\\[\nf(\\omega) = \\binom{10}{\\omega} (0.4)^\\omega(0.6)^{10-\\omega} \\quad \\text{and} \\quad g(\\omega) = \\binom{10}{\\omega} (0.9)^\\omega(0.1)^{10-\\omega}\n\\]\nfor \\(\\omega\\in \\{0,1,\\ldots,10\\}\\). These are the mass functions of a \\(\\mathcal{B}in(10,0.4)\\) and \\(\\mathcal{B}in(10,0.9)\\) random variable, respectively. We then plot histograms for these mass functions, along with histograms of the absolute and logarithmic relative magnitudes:\n\n\nCode\n# Define the possible values of omega (0 through 10)\nomegas = np.arange(0, 11)\n\n# Compute the probability mass functions for two Binomial distributions:\n# p: Binomial(n=10, p=0.4)\n# q: Binomial(n=10, p=0.9)\np = binom(n=10, p=0.4).pmf(omegas)\nq = binom(n=10, p=0.9).pmf(omegas)\n\n# Titles for each subplot\ntitles = [\n    \"$f(\\\\omega)$\",  # PMF of first distribution\n    \"$g(\\\\omega)$\",  # PMF of second distribution\n    \"$\\\\frac{f(\\\\omega)}{g(\\\\omega)}$\",  # Ratio of PMFs\n    \"$\\\\log\\\\left(\\\\frac{f(\\\\omega)}{g(\\\\omega)}\\\\right)$\",  # Log-ratio of PMFs\n]\n\n# Data to plot in each subplot\nprobs = [p, q, p / q, np.log(p / q)]\n\n# Y-axis limits for each subplot for better visualization\nylims = [(0, 0.4), (0, 0.4), (-50, 0.75e8), (-10, 20)]\n\n# Create a 2x2 grid of subplots\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 5), sharex=True)\n\n# Loop over each subplot, plotting the corresponding data\nfor title, prob, ylim, axis in zip(titles, probs, ylims, axes.flatten()):\n    axis.bar(omegas, prob, width=0.4, zorder=2)  # Bar plot for each omega\n    axis.set_xticks(ticks=omegas)  # Set x-ticks to omega values\n    axis.set_ylim(ylim)  # Set y-axis limits\n    axis.set_title(title)  # Set subplot title\n\n# Add a shared x-axis label for all subplots\nfig.supxlabel(\"$\\\\omega$\")\n\n# Adjust layout for better appearance and spacing\nplt.tight_layout()\nplt.subplots_adjust(hspace=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe second row makes the point vividly: the absolute relative magnitudes span such wildly different scales that the plot is almost useless, and numerical computations would be unstable. The logarithmic version, by contrast, stays well-behaved and informative.\nWe obtain a single-number summary of the logarithmic relative magnitudes by taking their mean with respect to the mass function \\(f(\\omega)\\), giving us the number\n\\[\nE_{\\omega\\sim f(\\omega)} \\left[\\log\\left( \\frac{f(\\omega)}{g(\\omega)} \\right)\\right] = \\sum_{\\omega\\in \\Omega} f(\\omega) \\log\\left( \\frac{f(\\omega)}{g(\\omega)} \\right).\n\\tag{3}\\]\nObserve that we could have instead computed the mean relative to \\(g(\\omega)\\), giving us the number\n\\[\nE_{\\omega\\sim g(\\omega)} \\left[\\log\\left( \\frac{f(\\omega)}{g(\\omega)} \\right)\\right] = \\sum_{\\omega\\in \\Omega} g(\\omega) \\log\\left( \\frac{f(\\omega)}{g(\\omega)} \\right).\n\\tag{4}\\]\nBut notice that\n\\[\nE_{\\omega\\sim g(\\omega)} \\left[\\log\\left( \\frac{f(\\omega)}{g(\\omega)} \\right)\\right] = - E_{\\omega\\sim g(\\omega)} \\left[\\log\\left( \\frac{g(\\omega)}{f(\\omega)} \\right)\\right],\n\\]\nwhere the right-hand side is the negative of a number of the form (3). So, at least up to sign, it doesn’t really matter which of the two numbers (3) or (4) that we use to develop our theory. Our choice of (3) has the benefit of making the KL divergence nonnegative when the distributions are discrete.\nThese considerations lead us to:\n\n\n\n\n\n\n\nDefinition 3 Let \\(X\\) and \\(Y\\) be two random variables with density functions \\(f(x)\\) and \\(g(x)\\), respectively. The Kullback–Leibler divergence (or just KL divergence) from \\(X\\) to \\(Y\\), denoted \\(D(X \\parallel Y)\\), is the mean logarithmic relative magnitude:\n\\[\nD(X \\parallel Y) = E_{x\\sim f(x)} \\left[ \\log \\left( \\frac{f(x)}{g(x)}\\right) \\right].\n\\]\n\n\n\n\nTechnically, \\(D(X\\parallel Y)\\) is defined only when \\(f(x)=0\\) implies \\(g(x)=0\\) for all \\(x\\)—a condition known as absolute continuity in measure theory. If \\(X\\) and \\(Y\\) are continuous, then \\(D(X\\parallel Y)\\) is often called the differential KL divergence.\nFor some examples of differential KL divergences, let’s consider the conditional random variables \\(Y\\mid X=x\\) from the previous section, which give the exam score \\(Y\\) of a randomly chosen student if they had studied \\(X=x\\) hours (for \\(x=1,2,\\ldots,6\\)). In the figure below, we plot the densities \\(f(y\\mid x)\\) of the conditional distributions and compute the five differential KL divergences\n\\[\nD\\left( (Y\\mid X=1) \\parallel (Y\\mid X=x) \\right)\n\\]\nfor \\(x=2,3,4,5,6\\).\n\n\nCode\n# Define the integrand for KL divergence between two Beta distributions:\ndef integrand(y, x1, x2):\n    return Y.cond_pdf(y, x1) * np.log(Y.cond_pdf(y, x1) / Y.cond_pdf(y, x2))\n\n\n# Compute KL divergence D((Y|X=1) || (Y|X=x)) for x = 2, 3, 4, 5, 6\nKL_div = {x: quad(func=integrand, args=(1, x), a=0, b=1)[0] for x in X.support[1:]}\n\n# Set up a 2x6 grid for custom subplot arrangement\nfig = plt.figure(figsize=(8, 5))\ngs = gridspec.GridSpec(2, 6, figure=fig)\nax1 = fig.add_subplot(gs[0, 0:2])\nax2 = fig.add_subplot(gs[0, 2:4])\nax3 = fig.add_subplot(gs[0, 4:6])\nax4 = fig.add_subplot(gs[1, 1:3])\nax5 = fig.add_subplot(gs[1, 3:5])\naxes = [ax1, ax2, ax3, ax4, ax5]\n\n# For each subplot, plot the two conditional densities and annotate with KL divergence\nfor x, ax in zip(X.support[1:], axes):\n    # Plot f(y|x=1) in blue\n    ax.plot(Y.support, Y.cond_density_array(1), color=blue, zorder=2, label=\"x = 1\")\n    ax.fill_between(Y.support, Y.cond_density_array(1), zorder=2, color=blue, alpha=0.1)\n\n    # Plot f(y|x) in yellow\n    ax.plot(\n        Y.support, Y.cond_density_array(x), color=yellow, zorder=2, label=f\"x = {x}\"\n    )\n    ax.fill_between(\n        Y.support, Y.cond_density_array(x), zorder=2, color=yellow, alpha=0.1\n    )\n\n    # Annotate with the computed KL divergence\n    ax.set_title(f\"KL div. = {KL_div[x]:.3f}\")\n    ax.set_ylim(0, 4)\n    ax.legend(loc=\"upper right\")\n\n# Adjust layout and spacing for better appearance\nplt.tight_layout()\nplt.subplots_adjust(hspace=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nEach subplot contains the blue density curve \\(f(y\\mid x=1)\\), along with a yellow density curve \\(f(y\\mid x)\\) for \\(x=2,3,4,5,6\\). As \\(x\\) gets larger, we can see visually that the densities become more unalike; this increasing dissimilarity is reflected in larger KL divergences as \\(x\\) gets larger.\nAt the other end of the spectrum, we have \\(D(X \\parallel Y) = 0\\) when \\(X\\) and \\(Y\\) are identically distributed. And, at least when the variables are discrete, it is a basic but important fact that we always have \\(D(X \\parallel Y)\\geq 0\\), with equality if and only if \\(X\\) and \\(Y\\) are identically distributed; this is referred to as Gibbs’ inequality (see here for a proof). So, the KL divergence has several properties that make it a good measure for the “distance” between two probability distributions. However, note that this distance is not symmetric, in the sense that we have\n\\[\nD(X\\parallel Y) \\neq D(Y \\parallel X)\n\\]\nin general.\nKL divergence measures how one distribution differs from another. To study relationships between random variables, we apply it to their joint and marginal distributions. If \\(X\\) and \\(Y\\) are independent, their joint density factors as the product of their marginals, \\(f(x,y)=f(x)f(y)\\). Thus, a measure of the “information flow” between \\(X\\) and \\(Y\\) is the distance—in the sense of KL divergence—from the true joint density \\(f(x,y)\\) to the product densities \\(f(x)f(y)\\). This leads us to:\n\n\n\n\n\n\n\nDefinition 4 Let \\(X\\) and \\(Y\\) be two random variables with density functions \\(f(x)\\) and \\(f(y)\\). The mutual information shared between \\(X\\) and \\(Y\\), denoted \\(I(X,Y)\\), is the quantity\n\\[\nI(X,Y) = E_{(x,y)\\sim f(x,y)} \\left[ \\log \\left( \\frac{f(x,y)}{f(x)f(y)} \\right) \\right].\n\\tag{5}\\]\n\n\n\n\nThe product \\(f(x)f(y)\\) is the density of some probability distribution on \\(\\mathbb{R}^2\\), which would coincide with the true joint probability distribution (with density \\(f(x,y)\\)) if the variables were independent. So, the mutual information is the KL divergence between two probability distributions on \\(\\mathbb{R}^2\\), and we have \\(I(X,Y)=0\\) when \\(X\\) and \\(Y\\) are independent.\nAs an example, we return once more to our random variables \\(X\\) and \\(Y\\), the “hours studied” discrete variable and the “exam score” continuous variable. In this case, the joint distribution of \\(X\\) and \\(Y\\) is a mixed discrete-continuous one, so the formula (5) gives\n\\[\nI(X,Y) = \\sum_{x=1}^6 \\int_0^1 f(x,y) \\log\\left(\\frac{f(x,y)}{f(x)f(y)} \\right) \\ dy,\n\\]\nwhere \\(f(x,y)\\) is the true joint mass-density function and \\(f(x)\\) and \\(f(y)\\) are the marginal mass and densities, respectively. We implement this formula directly in Python, using the quad method in the integrate submodule of SciPy for integration:\n\n# Compute the true joint density f(x, y) = f(x) * f(y|x)\ndef fxy(x, y):\n    return X.pdf(x) * Y.cond_pdf(y, x)\n\n\n# Define the integrand for mutual information:\n# f(x, y) * log(f(x, y) / (f(x) * f(y)))\ndef integrand(x, y):\n    return fxy(x, y) * np.log(fxy(x, y) / (X.pdf(x) * Y.pdf(y)))\n\n\n# For each x, create a function of y for integration over y in [0, 1]\nfuncs = [lambda y, x=x: integrand(x, y) for x in range(1, 7)]\n\n# Compute the mutual information by summing the integrals over y for each x\nmutual_info = sum([quad(func, a=0, b=1)[0] for func in funcs])\n\n# Print the computed mutual information I(X, Y)\nprint(f\"The mutual information I(X,Y) is {mutual_info:.3f}.\")\n\nThe mutual information I(X,Y) is 0.201.\n\n\nUsing the definitions of marginal and conditional entropies given in Definition 2, one easily proves that the mutual information \\(I(X,Y)\\) may be computed as described in:\n\n\n\n\n\n\n\nTheorem 1 (Mututal information is entropy) Let \\(X\\) and \\(Y\\) be two random variables. Then\n\\[\nI(X,Y) = H(Y) - H(Y\\mid X).\n\\tag{6}\\]\n\n\n\n\nThus, the mutual information measures the amount of entropy in \\(Y\\) that is “leftover” after having observed \\(X\\). In other words, it quantifies how much knowing \\(X\\) reduces uncertainty about \\(Y\\).\nWe end this section by using formula (6) to re-do our computation of the mutual information \\(I(X,Y)\\) from above. We get the same answer:\n\n# the differential entropy H(Y) is stored in `diff_entropy`\n\n# For each x, define a function of y for the conditional entropy integrand: -f(y|x) * log(f(y|x))\nfuncs = [lambda y, x=x: -Y.cond_pdf(y, x) * np.log(Y.cond_pdf(y, x)) for x in X.support]\n\n# Compute the conditional entropy H(Y|X=x) for each x by integrating over y in [0, 1]\ncond_entropies = [quad(func, a=0, b=1)[0] for func in funcs]\n\n# Compute H(Y) - sum_x f(x) * H(Y|X=x), which equals the mutual information I(X, Y)\ndiff_entropy - sum([cond_entropies[x - 1] * X.pdf(x) for x in X.support])\n\n# Print the previously computed mutual information for comparison\nprint(f\"The mutual information I(X,Y) is {mutual_info:.3f}.\")\n\nThe mutual information I(X,Y) is 0.201."
  },
  {
    "objectID": "posts/info-1/index.html#mutual-information-of-jointly-normal-random-variables",
    "href": "posts/info-1/index.html#mutual-information-of-jointly-normal-random-variables",
    "title": "Entropy & information",
    "section": "Mutual information of jointly normal random variables",
    "text": "Mutual information of jointly normal random variables\nUseful intuition for the mutual information \\(I(X,Y)\\) arises from the simple case of jointly normal variables, where a closed-form expression can be obtained. As a first step toward this formula, we compute the differential entropy of a single normal random variable:For background on normal random vectors, see here.\n\n\n\n\n\n\n\nTheorem 2 (Entropy of a normal random variable) If \\(X\\sim \\mathcal{N}(\\mu,\\sigma^2)\\), then\n\\[\nH(X) = \\frac{1}{2}\\log(2\\pi e \\sigma^2),\n\\]\nwhere \\(e\\) is the base of the natural logarithm.\n\n\n\n\n\n\n\n\n\n\nNoteProof.\n\n\n\n\n\nLetting \\(f(x)\\) be the density of \\(X\\), we compute:\n\\[\n\\begin{align*}\nH(X) &= -\\int f(x) \\log{f(x)} \\ dx \\\\\n&= - \\int f(x) \\log\\left\\{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left[ -\\frac{1}{2\\sigma^2}(x-\\mu)^2 \\right]\\right\\} \\ dx \\\\\n&= - \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) \\int f(x) \\ dx + \\frac{1}{2\\sigma^2}\\int f(x)(x-\\mu)^2 \\ dx \\\\\n&= \\frac{1}{2}\\log(2\\pi \\sigma^2) + \\frac{1}{2} \\\\\n&= \\frac{1}{2}\\log(2\\pi e \\sigma^2)\n\\end{align*}\n\\]\nwhere we’ve used \\(\\int f(x) \\ dx =1\\) and \\(\\int f(x)(x-\\mu)^2 \\ dx = \\sigma^2\\).\n\n\n\nIt is well known that the conditional distributions of a normal random vector are themselves normal; we include here the result for a \\(2\\)-dimensional normal random vector. The proof follows directly from standard properties of multivariate normal distributions and is omitted for brevity.\n\n\n\n\n\n\n\nTheorem 3 (Conditional distributions of normal vectors are normal) Let \\((X,Y) \\sim \\mathcal{N}_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) be a \\(2\\)-dimensional normal random vector with\n\\[\n\\boldsymbol{\\mu}= \\begin{bmatrix} \\mu_X \\\\ \\mu_Y \\end{bmatrix} \\quad \\text{and} \\quad \\boldsymbol{\\Sigma}= \\begin{bmatrix}\n\\sigma_X^2 & \\rho \\sigma_X \\sigma_Y \\\\\n\\rho \\sigma_X \\sigma_Y & \\sigma_Y^2\n\\end{bmatrix},\n\\]\nwhere \\(X \\sim \\mathcal{N}(\\mu_X,\\sigma_X^2)\\), \\(Y\\sim \\mathcal{N}(\\mu_Y,\\sigma_Y^2)\\), and \\(\\rho\\) is the correlation of \\(X\\) and \\(Y\\). Then\n\\[\n(Y \\mid X=x) \\sim \\mathcal{N}\\left(\\mu_Y + (x-\\mu_X) \\frac{\\rho \\sigma_Y}{\\sigma_X}, \\ \\sigma_Y^2(1-\\rho^2) \\right)\n\\]\nfor all \\(x\\).\n\n\n\n\nThe next result contains the formula for the mutual information of two jointly normal random variables; its proof is an easy application of our previous results. Notice the mutual information only depends on the correlation between the variables, as intuition might suggest.\n\n\n\n\n\n\n\nTheorem 4 (Mutual information of jointly normal variables) Let \\((X,Y) \\sim \\mathcal{N}_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) be a \\(2\\)-dimensional normal random vector. Then\n\\[\nI(X,Y) = -\\frac{1}{2} \\log \\left(1-\\rho^2 \\right),\n\\]\nwhere \\(\\rho\\) is the correlation of \\(X\\) and \\(Y\\).\n\n\n\n\n\n\n\n\n\n\nNoteProof.\n\n\n\n\n\nFrom Theorem 2 and Theorem 3, we get that\n\\[\nH(Y\\mid X=x) = \\frac{1}{2}\\log\\left(2\\pi e \\sigma_Y^2(1-\\rho^2)\\right),\n\\]\nwhere \\(\\sigma_Y\\) is the standard deviation of \\(Y\\). Since this does not depend on \\(x\\), we have\n\\[\nH(Y\\mid X) = E_{x\\sim f(x)}\\left(H(Y\\mid X=x) \\right) = \\frac{1}{2}\\log\\left(2\\pi e \\sigma_Y^2(1-\\rho^2)\\right),\n\\]\nwhere \\(f(x)\\) is the marginal density of \\(X\\). But another application of Theorem 2 gives\n\\[\nH(Y) = \\frac{1}{2}\\log(2\\pi e \\sigma_Y^2),\n\\]\nand so by Theorem 1 we have\n\\[\n\\begin{align*}\nI(X,Y) &= H(Y) - H(Y\\mid X) \\\\\n&= \\frac{1}{2}\\log(2\\pi e \\sigma_Y^2) - \\frac{1}{2}\\log\\left(2\\pi e \\sigma_Y^2(1-\\rho^2)\\right) \\\\\n&= -\\frac{1}{2} \\log \\left(1-\\rho^2 \\right).\n\\end{align*}\n\\]\n\n\n\nHence, the larger the correlation between the marginal normals, the larger the mutual information. To see this in action with a concrete example, let’s suppose that we have \\((X,Y) \\sim \\mathcal{N}_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), where\n\\[\n\\boldsymbol{\\mu}= \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\quad \\text{and} \\quad \\boldsymbol{\\Sigma}= \\begin{bmatrix}\n1 & 2\\rho  \\\\\n2\\rho  & 4\n\\end{bmatrix},\n\\]\nand \\(\\rho\\) is the correlation of \\(X\\) and \\(Y\\) (hence the marginal standard deviations are \\(\\sigma_X = 1\\) and \\(\\sigma_Y=2\\)). In the second and third plots below, we have selected two correlations \\(\\rho=0.5,0.85\\) and computed the corresponding mutual information \\(I(X,Y)\\). The isoprobability contours of the joint normal density \\(f(x,y)\\) are shown in yellow, while the conditional normal densities \\(f(y|x)\\) are shown in blue for each of \\(x=-1, 0, 1\\). For comparison, the marginal density \\(f(y)\\) has been shown in the first plot.\n\n\nCode\n# Define a function to plot contours of a bivariate normal distribution\ndef plot_multivar_norm(ax, muX, muY, sigmaX, sigmaY, x, y, labels=False):\n    # Construct the covariance matrix using the specified correlation rho\n    Sigma = np.array(\n        [[sigmaX**2, rho * sigmaX * sigmaY], [rho * sigmaX * sigmaY, sigmaY**2]]\n    )\n    Mu = np.array([muX, muY])  # Mean vector\n    U = multivariate_normal(mean=Mu, cov=Sigma)  # Multivariate normal object\n    grid = np.dstack((x, y))  # Create a grid for evaluation\n    z = U.pdf(grid)  # Evaluate the PDF on the grid\n    contour = ax.contour(x, y, z, colors=yellow, alpha=0.3)  # Plot contours\n    if labels:\n        ax.clabel(contour, inline=True, fontsize=8)  # Optionally label contours\n\n\n# Define a function to plot the conditional density f(y|x) for a given x_obs\ndef plot_conditional(\n    ax, muX, muY, sigmaX, sigmaY, rho, y_mesh, x_obs, magnification_factor=1\n):\n    # Compute conditional mean and standard deviation for Y|X=x_obs\n    mu = muY + (x_obs - muX) * rho * sigmaY / sigmaX\n    sigma = sigmaY * np.sqrt(1 - rho**2)\n    # Compute and scale the conditional normal density\n    x = magnification_factor * norm(loc=mu, scale=sigma).pdf(y_mesh)\n    # Plot the conditional density horizontally, shifted to align with x_obs\n    ax.plot(-x + x_obs, y_mesh, color=blue)\n    ax.fill_betweenx(y_mesh, -x + x_obs, x_obs, color=blue, alpha=0.4)\n\n\n# Set parameters for the bivariate normal distribution\nmuX = 0\nmuY = 0\nsigmaX = 1\nsigmaY = 2\nrhos = [0.5, 0.85]  # Correlation values to illustrate\nx_obs = [-1, 0, 1]  # Observed x values for conditional plots\nx, y = np.mgrid[-2.1:2.1:0.01, -5:5:0.01]  # Grid for contour plot\ny_mesh = np.linspace(-5, 5, num=250)  # Grid for conditional densities\n\n# Create a figure with three subplots: one for the marginal, two for different correlations\nfig, axes = plt.subplots(\n    ncols=3, figsize=(8, 4), sharey=True, gridspec_kw={\"width_ratios\": [1, 4, 4]}\n)\n\n# Plot the marginal density of Y on the first subplot (as a horizontal density)\nmagnification_factor = 2.5\nx_marginal = magnification_factor * norm(scale=sigmaY).pdf(y_mesh)\naxes[0].plot(-x_marginal, y_mesh, color=blue)\naxes[0].set_xlim(-1, 0)\naxes[0].fill_betweenx(y_mesh, -x_marginal, 0, color=blue, alpha=0.4)\naxes[0].yaxis.tick_right()\naxes[0].spines[\"left\"].set_visible(False)\naxes[0].spines[\"right\"].set_visible(True)\naxes[0].spines[\"bottom\"].set_visible(False)\naxes[0].set_xticks([])\n\n# For each correlation value, plot the joint contours and conditional densities\nfor rho, ax in zip(rhos, axes[1:]):\n    plot_multivar_norm(ax, muX, muY, sigmaX, sigmaY, x, y)\n    for x_ob in x_obs:\n        plot_conditional(\n            ax,\n            muX,\n            muY,\n            sigmaX,\n            sigmaY,\n            rho,\n            y_mesh,\n            x_obs=x_ob,\n            magnification_factor=3,\n        )\n    # Compute and display the mutual information for this correlation\n    info = -(1 / 2) * np.log(1 - rho**2)\n    ax.set_title(rf\"$\\rho ={rho}$, $I(X,Y)= {info:0.3f}$\")\n    ax.set_xlabel(r\"$x$\")\n    ax.set_xlim(-2.2, 2.2)\n    ax.set_xticks(range(-2, 3))\n\n# Label the y-axis on the first subplot\naxes[0].set_ylabel(r\"$y$\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nCompared to the marginal distribution of \\(Y\\), the conditional distributions become increasingly concentrated as \\(\\rho\\) increases. This stronger concentration reflects the reduced uncertainty in \\(Y\\) when \\(X\\) is observed. This example illustrates the intuition behind mutual information: greater correlation implies stronger dependence, smaller conditional entropy, and thus higher mutual information."
  },
  {
    "objectID": "posts/info-1/index.html#conclusion",
    "href": "posts/info-1/index.html#conclusion",
    "title": "Entropy & information",
    "section": "Conclusion",
    "text": "Conclusion\nWe began by examining how information flows between random variables, distinguishing deterministic flows (where input uniquely determines output) from stochastic flows (where input induces a probability distribution over outputs). This distinction captures the fundamental difference between classical predictive models and modern probabilistic ones like large language models.\nTo quantify information and uncertainty, we introduced the core concepts of information theory: surprisal, entropy, KL divergence, and mutual information. Surprisal measures how unexpected an outcome is, inversely related to its probability through the logarithm. Entropy emerges as the average surprisal—capturing the overall uncertainty in a probability distribution. For our exam score example, we computed \\(H(X) \\approx 1.698\\) for hours studied and \\(H(Y) \\approx -0.131\\) for exam scores, illustrating how differential entropy can be negative for continuous variables.\nThe KL divergence \\(D(X \\parallel Y)\\) quantifies how one distribution differs from another by measuring the mean logarithmic relative magnitude between their densities. While not a true metric (it’s asymmetric), it provides a principled way to measure distributional distance. This led naturally to mutual information \\(I(X,Y)\\), which applies KL divergence to the joint and marginal distributions, measuring how much knowing one variable reduces uncertainty about the other.\nThe relationship \\(I(X,Y) = H(Y) - H(Y \\mid X)\\) reveals mutual information as the reduction in entropy: the gap between uncertainty before and after observing additional information. For our student example, we found \\(I(X,Y) \\approx 0.201\\), indicating that knowing study hours provides modest but measurable information about exam performance. In the jointly normal case, we obtained the elegant formula \\(I(X,Y) = -\\frac{1}{2}\\log(1-\\rho^2)\\), showing mutual information depends only on correlation.\nThese information-theoretic quantities provide a unified mathematical framework for understanding uncertainty, dependence, and information flow in probabilistic systems. In future posts, we’ll extend these ideas through the lens of \\(\\sigma\\)-algebras and apply them to gambling strategies, options pricing in mathematical finance, and the analysis of probabilistic models in machine learning. The concepts developed here—viewing information as reduction in entropy—will prove essential for understanding learning over time and decision-making under uncertainty."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "john myers, ph.d.",
    "section": "",
    "text": "I’m a Ph.D. mathematician and university professor with a background that spans both theoretical and applied mathematics. My undergraduate education was in applied and computational mathematics and in physics. In graduate school and through the first few years of my research career, I specialized in homological algebra and commutative ring theory, focusing on the bridge between modern algebra and geometry and topology. However, my interests have gradually shifted toward more practical domains, where I now work with probability theory, modeling, and machine learning, with particular interest in applications to finance and risk. You can find my early mathematical research on my arXiv page.\n\n\n\n\n\nIn addition to my role as a mathematician, I am also an educator who has taught 13 distinct college mathematics courses ranging from introductory calculus, to applied engineering mathematics, to upper-division theoretical courses, and have been recognized with teaching awards for my classroom work. Notable among these was a novel course in my dissertation research areas of commutative ring theory and algebraic geometry—topics not often taught at the undergraduate level—and a year-long course in probabilistic machine learning for which I wrote the textbook and developed the supporting infrastructure, all available in the navigation bar at the top.\n\n\n\n\n\nMathematics has a unique duality: it’s both deeply theoretical and remarkably practical. The abstract concepts that fascinate pure mathematicians frequently evolve into the foundations of the algorithms and technological systems that shape our daily lives. This website will explore that arc from theory to application, sharing writings on mathematics, probability, machine learning, and their real-world intersections. Writing helps me clarify my own understanding as I continue learning across these fields. Whether you’re a student, researcher, or practitioner, I hope you’ll find ideas and resources here that inform and inspire."
  },
  {
    "objectID": "teaching/analysis-fa-25.html",
    "href": "teaching/analysis-fa-25.html",
    "title": "mat347 analysis, fall 2025",
    "section": "",
    "text": "instructor:\n\n\njohn myers\n\n\n\n\noffice:\n\n\nmarano 175\n\n\n\n\noffice hours:\n\n\n12-12:30 MWF\n\n\n\n\nsyllabus:\n\n\nlink\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndate\ntopics\ninfo + due dates\n\n\n\n\n14\n12.05 fri\n\nweek 14 hw due\n\n\n\n12.03 wed\n\n\n\n\n\n12.01 mon\n7.2 The definition of the Riemann integral, part 1  7.2 The definition of the Riemann integral, part 2 \n\n\n\n13\n11.21 fri\n5.2 derivatives, part 1     5.2 derivatives, part 2 \nweek 13 hw due\n\n\n\n11.19 wed\n4.4 continuous functions on compact sets, part 1     5.2 derivatives, part 1   \n\n\n\n\n11.17 mon\n4.3 continuous functions     4.4 continuous functions on compact sets, part 1   \n\n\n\n12\n11.14 fri\n4.2 functional limits, part 2     4.3 continuous functions   \nweek 12 hw due\n\n\n\n11.12 wed\nno class\n\n\n\n\n11.10 mon\n4.2 functional limits, part 1     4.2 functional limits, part 2     4.3 continuous functions   \n\n\n\n11\n11.07 fri\n4.2 functional limits, part 1   \nweek 11 hw due\n\n\n\n11.05 wed\n3.3 compact sets, part 2     4.2 functional limits, part 1   \n\n\n\n\n11.03 mon\n3.3 compact sets, part 1     3.3 compact sets, part 2   \n\n\n\n10\n10.31 fri\n3.3 compact sets, part 1   \nweek 10 hw due\n\n\n\n10.29 wed\n3.2 open and closed sets   \n\n\n\n\n10.27 mon\nno class\n\n\n\n9\n10.24 fri\n3.2 open and closed sets   \nweek 9 hw due\n\n\n\n10.22 wed\n3.2 open and closed sets   \n\n\n\n\n10.20 mon\n2.6 the Cauchy criterion   \n\n\n\n8\n10.17 fri\nexam 1 on sections 1.2-2.5\n\n\n\n\n10.15 wed\n2.4-2.5 Monotone Conv. and B-W theorems, part 2   2.6 the Cauchy criterion   \n\n\n\n\n10.13 mon\n2.4-2.5 Monotone Conv. and B-W theorems, part 1     2.4-2.5 Monotone Conv. and B-W theorems, part 2 \n\n\n\n7\n10.10 fri\nno class - fall break\n\n\n\n\n10.08 wed\n2.4-2.5 Monotone Conv. and B-W theorems, part 1   \n\n\n\n\n10.06 mon\n2.3 the algebraic and order limit theorems, part 2   \n\n\n\n6\n10.03 fri\nno class\nweek 6 hw due\n\n\n\n10.01 wed\n2.3 the algebraic and order limit theorems, part 1     2.3 the algebraic and order limit theorems, part 2   \n\n\n\n\n09.29 mon\n2.3 the algebraic and order limit theorems, part 1   \n\n\n\n5\n09.26 fri\n2.2 the limit of a sequence, part 2   \nweek 5 hw due\n\n\n\n09.24 wed\n2.2 the limit of a sequence, part 1   \n\n\n\n\n09.22 mon\n1.5 cardinality, part 2   2.2 the limit of a sequence, part 1   \n\n\n\n4\n09.19 fri\n1.5 cardinality, part 2   \nweek 4 hw due\n\n\n\n09.17 wed\n1.5 cardinality, part 1   \n\n\n\n\n09.15 mon\n1.5 cardinality, part 1   \n\n\n\n3\n09.12 fri\n1.4 consequences of completeness   \nweek 2 & 3 homework due\n\n\n\n09.10 wed\n1.4 consequences of completeness   \n\n\n\n\n09.08 mon\n1.3 axiom of completeness   \n\n\n\n2\n09.05 fri\n1.2 some preliminaries1.3 axiom of completeness   \n\n\n\n\n09.03 wed\n1.2 some preliminaries   \n\n\n\n1\n08.29 fri\nno class\n\n\n\n\n08.27 wed\n1.1 introduction   1.2 some preliminaries   \n\n\n\n\n08.25 mon\n1.1 introduction"
  }
]