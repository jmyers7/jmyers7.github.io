[
  {
    "objectID": "writings.html",
    "href": "writings.html",
    "title": "john myers, ph.d.",
    "section": "",
    "text": "The posts below (as they are written) will range from technical deep-dives in machine learning and pure mathematics, to practical applications in finance and risk, and anything else that catches my interests. Some posts will draw from my expertise as a mathematician, while others will reflect my journey as a beginner exploring and learning new fields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEntropy, surprise, and information\n\n\n\nInformation\n\nPython\n\nSciPy\n\nEntropy\n\nSurprisal\n\nMutual information\n\n\n\n\n\n\n\n\n\nSep 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian mixture models: a first look\n\n\n\nPython\n\nProbability\n\nStatistics\n\nMachine learning\n\nProbabilistic graphical models\n\nMixture models\n\nGaussian mixture models\n\n\n\n\n\n\n\n\n\nSep 26, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/analysis-fa-25.html",
    "href": "teaching/analysis-fa-25.html",
    "title": "mat347 analysis, fall 2025",
    "section": "",
    "text": "instructor:\n\n\njohn myers\n\n\n\n\noffice:\n\n\nmarano 175\n\n\n\n\noffice hours:\n\n\n12-12:30 MWF\n\n\n\n\nsyllabus:\n\n\nlink\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndate\ntopics\ninfo + due dates\n\n\n\n\n7\n10.10 fri\nno class - fall break\n\n\n\n\n10.08 wed\n\nweek 7 hw due\n\n\n\n10.06 mon\n\n\n\n\n6\n10.03 fri\n2.3 the algebraic and order limit theorems, part 2    2.4-2.5 Monotone Conv. and B-W theorems, part 1   \nweek 6 hw due\n\n\n\n10.01 wed\n2.3 the algebraic and order limit theorems, part 1     2.3 the algebraic and order limit theorems, part 2   \n\n\n\n\n09.29 mon\n2.3 the algebraic and order limit theorems, part 1   \n\n\n\n5\n09.26 fri\n2.2 the limit of a sequence, part 2   \nweek 5 hw due\n\n\n\n09.24 wed\n2.2 the limit of a sequence, part 1   \n\n\n\n\n09.22 mon\n1.5 cardinality, part 2   2.2 the limit of a sequence, part 1   \n\n\n\n4\n09.19 fri\n1.5 cardinality, part 2   \nweek 4 hw due\n\n\n\n09.17 wed\n1.5 cardinality, part 1   \n\n\n\n\n09.15 mon\n1.5 cardinality, part 1   \n\n\n\n3\n09.12 fri\n1.4 consequences of completeness   \nweek 2 & 3 homework due\n\n\n\n09.10 wed\n1.4 consequences of completeness   \n\n\n\n\n09.08 mon\n1.3 axiom of completeness   \n\n\n\n2\n09.05 fri\n1.2 some preliminaries1.3 axiom of completeness   \n\n\n\n\n09.03 wed\n1.2 some preliminaries   \n\n\n\n1\n08.29 fri\nno class\n\n\n\n\n08.27 wed\n1.1 introduction   1.2 some preliminaries   \n\n\n\n\n08.25 mon\n1.1 introduction"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "john myers, ph.d.",
    "section": "",
    "text": "I’m a Ph.D. mathematician and university professor with a background that spans both theoretical and applied mathematics. My undergraduate education was in applied and computational mathematics with a minor in physics. In graduate school and through the first few years of my research career, I specialized in homological algebra and commutative ring theory, focusing on the bridge between modern algebra and geometry and topology. However, my interests have gradually shifted toward more practical domains, where I now work with machine learning, AI, statistics, and probability, with particular interest in applications to finance and risk. You can find my early mathematical research on my arXiv page.\n\n\n\n\n\nIn addition to my role as a mathematician, I am also an educator who has taught 13 distinct college mathematics courses ranging from introductory calculus, to applied engineering mathematics, to upper-division theoretical courses, and have been recognized with teaching awards for my classroom work. Notable among these was a novel course in my dissertation research areas of commutative ring theory and algebraic geometry—topics not often taught at the undergraduate level—and a year-long course in probabilistic machine learning for which I wrote the textbook and developed the supporting infrastructure, all available in the navigation bar at the top.\n\n\n\n\n\nMathematics has a unique duality: it’s both deeply theoretical and remarkably practical. The abstract concepts that fascinate pure mathematicians frequently evolve into the foundations of the algorithms and technological systems that shape our daily lives. This website will explore that arc from theory to application, sharing writings on mathematics, probability, machine learning, and their real-world intersections. Writing helps me clarify my own understanding as I continue learning across these fields. Whether you’re a student, researcher, or practitioner, I hope you’ll find ideas and resources here that inform and inspire."
  },
  {
    "objectID": "posts/info/index.html",
    "href": "posts/info/index.html",
    "title": "Entropy, surprise, and information",
    "section": "",
    "text": "Large language models (LLMs) like ChatGPT have brought probabilistic models into mainstream conversation. Unlike deterministic models that always produce the same output for a given input, these models generate a probability distribution over many possible outputs. Information theory provides the mathematical framework for understanding these distributions, and how sensitive they are to the inputs of the model.\nTo understand this more concretely, consider what happens when you prompt an LLM. You might ask “What’s the capital of France?” and reliably get “Paris” as the answer. The model is highly confident: the output probability distribution is concentrated over the one-word output “Paris”. But if you ask “Write me a poem about autumn,” you’ll get a different response each time. Here, the probability distribution is spread across countless possible poems, each with some small probability of being generated. In the first case, the model is certain, reflected in the strongly peaked output distribution, while in the latter case it is uncertain, reflected in the highly dispersed output distribution. The input to the model (i.e., the prompt) thus has a strong effect on the certainty in the output distribution.\nThe gap between certainty and uncertainty lies at the heart of information theory. When Claude Shannon founded the field in 1948, he was grappling with a fundamental question in telecommunications: how much data can you reliably transmit through a noisy communication channel? Telephone wires introduce random distortions like static and interference, scrambling the signal in unpredictable ways. Shannon realized he could model this mathematically using probability theory, and what began as a framework for analyzing communication over literal telephone wires evolved into an abstract mathematical theory with remarkably broad applications. Indeed, Shannon has been credited with laying the foundations for our modern information-based society.\nThis post will survey the most basic quantities of information theory: surprisal, entropy, and mutual information. Surprisal is a quantity attached to a random outcome that is inversely linked to its probability: a likely outcome is not surprising, while an unlikely one is highly surprising. When we average surprisal over all possible random outcomes, we obtain the entropy, which quantifies the overall uncertainty in a probability distribution. Finally, mutual information applies to two random variables \\(X\\) and \\(Y\\) with a joint distribution. It measures how much observing \\(X\\) reduces the uncertainty (entropy) in \\(Y\\) on average. This last quantity connects directly to our discussion of LLMs: it captures precisely how much the input prompt narrows down the output distribution.\nWhile this post focuses on the foundational concepts, future posts will explore how these ideas apply to the probabilistic models used in machine learning."
  },
  {
    "objectID": "posts/info/index.html#introduction",
    "href": "posts/info/index.html#introduction",
    "title": "Entropy, surprise, and information",
    "section": "",
    "text": "Large language models (LLMs) like ChatGPT have brought probabilistic models into mainstream conversation. Unlike deterministic models that always produce the same output for a given input, these models generate a probability distribution over many possible outputs. Information theory provides the mathematical framework for understanding these distributions, and how sensitive they are to the inputs of the model.\nTo understand this more concretely, consider what happens when you prompt an LLM. You might ask “What’s the capital of France?” and reliably get “Paris” as the answer. The model is highly confident: the output probability distribution is concentrated over the one-word output “Paris”. But if you ask “Write me a poem about autumn,” you’ll get a different response each time. Here, the probability distribution is spread across countless possible poems, each with some small probability of being generated. In the first case, the model is certain, reflected in the strongly peaked output distribution, while in the latter case it is uncertain, reflected in the highly dispersed output distribution. The input to the model (i.e., the prompt) thus has a strong effect on the certainty in the output distribution.\nThe gap between certainty and uncertainty lies at the heart of information theory. When Claude Shannon founded the field in 1948, he was grappling with a fundamental question in telecommunications: how much data can you reliably transmit through a noisy communication channel? Telephone wires introduce random distortions like static and interference, scrambling the signal in unpredictable ways. Shannon realized he could model this mathematically using probability theory, and what began as a framework for analyzing communication over literal telephone wires evolved into an abstract mathematical theory with remarkably broad applications. Indeed, Shannon has been credited with laying the foundations for our modern information-based society.\nThis post will survey the most basic quantities of information theory: surprisal, entropy, and mutual information. Surprisal is a quantity attached to a random outcome that is inversely linked to its probability: a likely outcome is not surprising, while an unlikely one is highly surprising. When we average surprisal over all possible random outcomes, we obtain the entropy, which quantifies the overall uncertainty in a probability distribution. Finally, mutual information applies to two random variables \\(X\\) and \\(Y\\) with a joint distribution. It measures how much observing \\(X\\) reduces the uncertainty (entropy) in \\(Y\\) on average. This last quantity connects directly to our discussion of LLMs: it captures precisely how much the input prompt narrows down the output distribution.\nWhile this post focuses on the foundational concepts, future posts will explore how these ideas apply to the probabilistic models used in machine learning."
  },
  {
    "objectID": "posts/info/index.html#flows-of-information",
    "href": "posts/info/index.html#flows-of-information",
    "title": "Entropy, surprise, and information",
    "section": "Flows of information",
    "text": "Flows of information\nWe begin by building a mathematical gadget that models the “flow of information” between two random variables \\(X\\) and \\(Y\\) (or random vectors, or random objects, or …). Such flows are exactly what information theory calls communication channels, and they include many of the predictive probabilistic models in machine learning where information flows from input \\(X\\) to output \\(Y\\). Once these flows have been identified in this section, in the rest of the post we seek to understand how information flowing from \\(X\\) influences and shapes the distribution on \\(Y\\).\nThe simplest flow between \\(X\\) and \\(Y\\) is a functional one, expressed as an equation \\[\ng(X)=Y,\n\\tag{1}\\]\nwhere \\(g\\) is a function. With \\(X\\) as input and \\(Y\\) as output, an observed input \\(X=x\\) produces a unique output \\(y = g(x)\\). Such flows underlie deterministic models. In the case that \\(X\\) and \\(Y\\) take values on the real line \\(\\mathbb{R}\\), we might visualize the situation like this:\n\n\n\n\n\nNote that each value of \\(x\\) along the input (left) axis determines a unique value of \\(y\\) along the output (right) axis.\nOn the other hand, we might suppose that information flows from \\(X\\) to \\(Y\\) in a stochastic fashion, in which an observed input \\(X=x\\) does not uniquely determine an output \\(y\\), but rather a distribution on \\(Y\\). This is precisely what a conditional distribution \\(P(Y= y\\mid X=x)\\) captures: given an observed value \\(X=x\\), we have a probability distribution on \\(y\\)’s. We can think of this as a function of the form\n\\[\nx \\mapsto P(Y= y \\mid X=x),\n\\tag{2}\\]\nwhere the \\(y\\) is intended as a variable and not a fixed quantity, so that \\(P(Y= y \\mid X=x)\\) is a probability distribution and not just a single probability. So, this function is rather special: its input is a value \\(x\\), while its output is an entire probability distribution. Mathematicians call such objects Markov kernels. A figure will make this more concrete: in contrast to the deterministic flows drawn above, in the case that \\(X\\) and \\(Y\\) take values on the real line \\(\\mathbb{R}\\), we visualize a stochastic flow as follows, where each value of \\(x\\) is mapped to a probability distribution on \\(y\\)’s:\n\n\n\n\n\nIn our model of a stochastic flow, we therefore use conditional distributions, understood as functions on the conditioning variable.\nIn practive, very often we have a family \\(P(Y=y; \\theta)\\) of parameterized distributions over \\(y\\)’s, where \\(\\theta\\) is a parameter vector. The stochastic flow from \\(X\\) to \\(Y\\) is then implemented as a function \\(x\\mapsto \\theta(x)\\) from observations of \\(X\\) to parameters \\(\\theta\\), and the conditional distribution is then defined as\n\\[\nP(Y=y \\mid X=x) = P(Y=y ; \\theta=\\theta(x)).\n\\]\nFamiliar models like linear regression (with known variance \\(\\sigma^2\\)) fit this description, in which \\(P(Y=y; \\theta)\\) is given by the normal density\n\\[\nf(y;\\theta) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left[ - \\frac{1}{2\\sigma^2}(y-\\theta)^2 \\right],\n\\]\nand with parameter mapping\n\\[\nx\\mapsto \\theta(x) = \\beta_0 + \\beta_1x,\n\\]\nfor some model coefficients \\(\\beta_0\\) and \\(\\beta_1\\). Concisely, the stochastic flow from \\(X\\) to \\(Y\\) in a linear regression model is completely described by specifying\n\\[\n(Y\\mid X=x) \\sim \\mathcal{N}(\\beta_0+\\beta_1x, \\sigma^2).\n\\]\nWe will return to an information-theoretic treatment of linear regression (and other) models in a later post.\nFor now, let’s see all this in action with real distributions in a real-world context. Suppose that \\(X\\) is the number of hours that a randomly chosen student studies for an upcoming exam, restricted to discrete values \\(X = 1,2,3,4,5,6\\). Let’s say that most students study only 2 or 3 hours, with the full distribution on \\(X\\) (i.e., its mass function \\(f(x)\\)) given in the following plot:\n\n\nCode\nn = 6\nxs = range(1, n+1)\nfxs = poisson.pmf(xs, mu=3)\nfxs /= fxs.sum()\n\n_, ax = plt.subplots(figsize=(6, 4))\n\nax.bar(xs, fxs, width=0.4, zorder=2)\nax.set_xlabel(r\"hours studied ($x$)\")\nax.set_ylabel(\"probability\")\nax.set_title(r\"marginal mass $f(x)$\")\nax.set_xticks(range(1, n+1))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe might reasonably believe that \\(X\\) is predictive of \\(Y\\), the exam score of a randomly chosen student, taking continuous values in the interval \\([0,1]\\), understood as percentages. The plot of the density function \\(f(y)\\) is given in:\n\n\nCode\ndef fy(y):\n  return sum([beta.pdf(y, a=x, b=3) * fx for x, fx in zip(xs, fxs)])\n\n_, ax = plt.subplots(figsize=(6, 4))\n\ngrid = np.linspace(0, 1, num=250)\nax.plot(grid, fy(grid))\nax.fill_between(grid, fy(grid), zorder=2, alpha=0.1)\nax.xaxis.set_major_formatter(PercentFormatter(xmax=1))\nax.set_title(r\"marginal density $f(y)$\")\nax.set_xlabel(\"test score ($y$)\")\nax.set_ylabel(\"probability density\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nTogether, \\(X\\) and \\(Y\\) have a joint mass/density function \\(f(x,y)\\), visualized in the following ridgeline plot, where each of the horizontal density curves shows \\(f(x,y)\\) as a function of \\(y\\), for fixed \\(x=1,2,3,4,5,6\\).\n\n\nCode\n_, ax = plt.subplots(figsize=(6, 5))\n\nconditional_colors = [conditional_cmap(i/(n-1)) for i in range(n)]\nfor x, fx in zip(xs, fxs):\n    joint_vals = 1.7 * beta.pdf(x=grid, a=x, b=3) * fx\n    ax.fill_between(grid, x, x + joint_vals, color=conditional_colors[x-1], zorder=2, alpha=0.1)\n    ax.plot(grid, x + joint_vals, color=conditional_colors[x-1], zorder=2)\nax.set_ylabel(r\"hours studied ($x$)\")\nax.set_xlabel(r\"test score ($y$)\")\nax.xaxis.set_major_formatter(PercentFormatter(xmax=1))\nax.set_title(r\"joint mass/density $f(x,y)$\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nDividing the joint mass/density \\(f(x,y)\\) by the marginal mass \\(f(x)\\) yields the conditional densities \\(f(x|y)\\). These are just the same density curves in the ridgeline plot above, normalized so that they integrate to \\(1\\) over \\([0,1]\\). They are shown in:\n\n\nCode\n_, ax = plt.subplots(figsize=(6, 4))\n\nfor x in xs:\n  ax.plot(grid, beta.pdf(x=grid, a=x, b=3), color=conditional_colors[x-1], label=x)\nax.legend(title=r\"hours studied ($x$)\", loc=\"center left\", bbox_to_anchor=(1, .5))\nax.xaxis.set_major_formatter(PercentFormatter(xmax=1))\nax.set_title(r\"conditional densities $f(y|x)$\")\n\nax.set_xlabel(r\"test score ($y$)\")\nax.set_ylabel(\"probability density\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn our information-theoretic terminology, the conditional density\n\\[\nx\\mapsto f(y|x),\n\\]\nthought of as a function of \\(x\\), models the stochastic flow of information from \\(X\\) to \\(Y\\).\nInspecting the plot of the marginal density \\(f(y)\\) reveals a good amount of uncertainty: the distribution is peaked, but not strongly so. An exam score randomly drawn from \\(Y\\) will be mildly uncertain, mildly surprising. The exact amount of uncertainty in \\(Y\\) will be measured through its entropy, denoted \\(H(Y)\\), introduced in the next section. In contrast, the conditional densities \\(f(y|x)\\) exhibit less uncertainty compared to the marginal, especially for values of \\(x\\) closer to \\(6\\). The exact amount of uncertainty of \\(Y\\), given an observed value \\(X=x\\), will be measured through the conditional entropy, denoted \\(H(Y\\mid X=x)\\). Averaging this conditional entropy over \\(X\\) yields the quantity\n\\[\nH(Y\\mid X) \\overset{\\text{def}}{=}E_{x\\sim f(x)}(H(Y\\mid X=x)),\n\\]\nthe average amount of uncertainty in \\(Y\\), given \\(X\\). Then, it is a general observation that\n\\[\nH(Y) \\geq H(Y\\mid X)\n\\]\nfor any pair of random variables \\(X\\) and \\(Y\\), reflecting the obvious fact that no additional information will ever increase the uncertainty in \\(Y\\). Thus, the quantity\n\\[\nI(X,Y) \\overset{\\text{def}}{=}H(Y) - H(Y\\mid X)\n\\]\nis a nonnegative proxy for the amount of information transmitted from \\(X\\) to \\(Y\\): if it is large, then the gap between \\(H(Y)\\) and \\(H(Y\\mid X)\\) is wide, indicating that observations of \\(X\\) greatly reduce the uncertainty in \\(Y\\). We understand this as a “large amount of information” is transmitted from \\(X\\) to \\(Y\\). Conversely, if \\(I(X,Y)\\) is small, then observations of \\(X\\) do not tell us much about \\(Y\\); in fact, in the extreme case that \\(I(X,Y)=0\\), the variables \\(X\\) and \\(Y\\) are independent. The quantity \\(I(X,Y)\\) is exactly the mutual information between \\(X\\) and \\(Y\\), introduced in the next section."
  },
  {
    "objectID": "posts/info/index.html#entropy-surprisal-and-mutual-information",
    "href": "posts/info/index.html#entropy-surprisal-and-mutual-information",
    "title": "Entropy, surprise, and information",
    "section": "Entropy, surprisal, and mutual information",
    "text": "Entropy, surprisal, and mutual information\n\n\n\n\n\n\n\nDefinition 1 Let \\(X\\) and \\(Y\\) be two random variables with density functions \\(f(x)\\) and \\(f(y)\\), respectively.\n\nThe surprisal of an observed value \\(X=x\\) is the quantity \\[\n  I(x) = -\\log{f(x)},\n  \\] where the logarithm is the natural one.\nThe conditional surprisal of an observed value \\(Y=y\\), given \\(X=x\\), is the quantity \\[\n  I(y|x) = -\\log{f(y|x)}.\n  \\]\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2 Let \\(X\\) and \\(Y\\) be two random variables with density functions \\(f(x)\\) and \\(f(y)\\), respectively.\n\nThe entropy of \\(X\\) is the quantity \\[\n  H(X) = E_{x\\sim f(x)}(I(x)).\n  \\]\nThe conditional entropy of \\(Y\\), given an observed value \\(X=x\\), is the quantity\n\n\\[\nH(Y\\mid X=x) = E_{y\\sim f(y|x)}(I(y\\mid x)).\n\\]\n\nThe conditional entropy of \\(Y\\), given \\(X\\), is the quantity\n\n\\[\nH(Y\\mid X) = E_{x\\sim f(x)}(H(Y\\mid X=x)).\n\\]"
  },
  {
    "objectID": "posts/info/index.html#mutual-information-of-jointly-discrete-random-variables",
    "href": "posts/info/index.html#mutual-information-of-jointly-discrete-random-variables",
    "title": "Entropy, surprise, and information",
    "section": "Mutual information of jointly discrete random variables",
    "text": "Mutual information of jointly discrete random variables\n\n\nCode\nn = 6\nfxy = np.random.rand(n ** 2)\nfxy /= fxy.sum()\nfxy = fxy.reshape(n, n)\n\nfx = fxy.sum(axis=1)\nfy = fxy.sum(axis=0)\n\nfig = plt.figure(figsize=(8, 8))\ngs = gridspec.GridSpec(2, 2, height_ratios=[1, 1.5])\n\nax1 = fig.add_subplot(gs[0, 0])\nax1.bar(range(n), fx, width=0.4, zorder=2)\nax1.set_xlabel(r\"$x$\")\nax1.set_ylabel(\"probability\")\nax1.set_title(r\"marginal distribution $f(x)$\")\n\nax2 = fig.add_subplot(gs[0, 1], sharex=ax1, sharey=ax1)\nax2.bar(range(n), fy, width=0.4, zorder=2)\nax2.set_xlabel(r\"$y$\")\nax2.set_ylabel(\"probability\")\nax2.set_title(r\"marginal distribution $f(y)$\")\n\nax3 = fig.add_subplot(gs[1,:])\nsns.heatmap(fxy.T, annot=True, fmt=\".3f\", cmap=heatmap_cmap, linewidth=8, linecolor=grey, zorder=2, cbar_kws={'label': 'probability'}, ax=ax3)\nax3.invert_yaxis()\nax3.set_xlabel(r\"$x$\")\nax3.set_ylabel(r\"$y$\")\nax3.set_title(r\"joint distribution $f(x,y)$\")\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.4, hspace=0.4)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, axes = plt.subplots(nrows=2, ncols=3, sharey=True, sharex=True)\nconditionals = []\n\nfor x, ax in enumerate(axes.flatten()):\n  f_y_given_x = fxy[x, :] / fxy[x, :].sum()\n  conditionals.append(f_y_given_x)\n  ax.bar(range(n), f_y_given_x, width=0.4, zorder=2)\n  ax.set_xticks(range(n))  \n  ax.set_xticklabels(range(n))\n  ax.set_title(rf\"$x={x}$\")\n  \nfig.supxlabel(r\"$y$\")\nfig.supylabel(\"probability\")\nfig.suptitle(r\"conditional distributions $f(y\\mid x)$\")\n\nplt.tight_layout()\nplt.subplots_adjust(hspace=0.8)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ninfo = entropy(fy) - sum([entropy(f_y_given_x) * fx[x] for x, f_y_given_x in enumerate(conditionals)])\nprint(f\"The mutual information is I(X,Y) = {info:.4f}.\")\n\n\nThe mutual information is I(X,Y) = 0.2032."
  },
  {
    "objectID": "posts/info/index.html#mutual-information-of-jointly-normal-random-variables",
    "href": "posts/info/index.html#mutual-information-of-jointly-normal-random-variables",
    "title": "Entropy, surprise, and information",
    "section": "Mutual information of jointly normal random variables",
    "text": "Mutual information of jointly normal random variables\n\n\n\n\n\n\n\nTheorem 1 Let \\((X,Y) \\sim \\mathcal{N}_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) be a \\(2\\)-dimensional normal random vector with\n\\[\n\\boldsymbol{\\mu}^\\intercal = \\begin{bmatrix} \\mu_X & \\mu_Y \\end{bmatrix} \\quad \\text{and} \\quad \\boldsymbol{\\Sigma}= \\begin{bmatrix}\n\\sigma_X^2 & \\rho \\sigma_X \\sigma_Y \\\\\n\\rho \\sigma_X \\sigma_Y & \\sigma_Y^2\n\\end{bmatrix},\n\\]\nwhere \\(X \\sim \\mathcal{N}(\\mu_X,\\sigma_X^2)\\), \\(Y\\sim \\mathcal{N}(\\mu_Y,\\sigma_Y^2)\\), and \\(\\rho\\) is the correlation between \\(X\\) and \\(Y\\). Then\n\\[\n(Y \\mid X=x) \\sim \\mathcal{N}\\left(\\mu_Y + (x-\\mu_X) \\frac{\\rho \\sigma_Y}{\\sigma_X}, \\ \\sigma_Y^2(1-\\rho^2) \\right)\n\\]\nfor all \\(x\\).\n\n\n\n\n\n\nCode\ndef plot_multivar(ax, muX, muY, sigmaX, sigmaY, x, y, labels=False):\n  Sigma = np.array([[sigmaX ** 2, rho * sigmaX * sigmaY], [rho * sigmaX * sigmaY, sigmaY ** 2]])\n  Mu = np.array([muX, muY])\n  U = multivariate_normal(mean=Mu, cov=Sigma)\n  grid = np.dstack((x, y))\n  z = U.pdf(grid)\n  contour = ax.contour(x, y, z, colors=yellow, alpha=0.5)\n  if labels:\n    ax.clabel(contour, inline=True, fontsize=8)\n  \ndef plot_conditional(ax, muX, muY, sigmaX, sigmaY, rho, y, x_obs):\n  mu = muY + (x_obs - muX) * rho * sigmaY / sigmaX\n  sigma = sigmaY * np.sqrt(1 - rho ** 2)\n  x = norm(loc=mu, scale=sigma).pdf(y)\n  ax.plot(-x + x_obs, y, color=blue)\n  ax.fill_betweenx(y, -x + x_obs, x_obs, color=blue, alpha=0.4)\n\ndef plot_combined(ax, muX, muY, sigmaX, sigmaY, rho, x, y, x_obs, labels=False):\n  plot_multivar(ax, muX, muY, sigmaX, sigmaY, x, y, labels)\n  y = np.linspace(np.min(y), np.max(y), num=250)\n  plot_conditional(ax, muX, muY, sigmaX, sigmaY, rho, y, x_obs[0])\n  plot_conditional(ax, muX, muY, sigmaX, sigmaY, rho, y, x_obs[1])\n  plot_conditional(ax, muX, muY, sigmaX, sigmaY, rho, y, x_obs[2])\n  ax.set_title(rf\"$\\rho ={rho}$\")\n  ax.set_xlabel(r\"$x$\")\n  ax.set_ylabel(r\"$y$\")\n  fig = plt.gcf()  # Get current figure\n  fig.set_size_inches(6, 4)\n  plt.tight_layout()\n  plt.show()\n\n_, ax = plt.subplots()\nx, y = np.mgrid[-1:3:0.01, -4:6:0.01]\n\nmuX = 1\nmuY = 1\nsigmaX = 1\nsigmaY = 2\nrho = 0.15\n\nplot_combined(ax, muX, muY, sigmaX, sigmaY, rho, x, y, x_obs=[0, 1, 2], labels=True)\n\n\n\n\n\n\n\n\n\n\n\nCode\n_, ax = plt.subplots()\nrho = 0.5\nplot_combined(ax, muX, muY, sigmaX, sigmaY, rho, x, y, x_obs=[0, 1, 2], labels=True)\n\n\n\n\n\n\n\n\n\n\n\nCode\n_, ax = plt.subplots()\nrho = 0.85\nplot_combined(ax, muX, muY, sigmaX, sigmaY, rho, x, y, x_obs=[0, 1, 2], labels=True)"
  },
  {
    "objectID": "posts/info/index.html#conclusion",
    "href": "posts/info/index.html#conclusion",
    "title": "Entropy, surprise, and information",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "posts/gmm-1/index.html",
    "href": "posts/gmm-1/index.html",
    "title": "Gaussian mixture models: a first look",
    "section": "",
    "text": "A mixture of gaussians represents one of the most elegant and practically useful concepts in probability theory and machine learning. At its core, this is a probabilistic model that assumes observed data arises from a combination of several underlying normal distributions, each contributing to the overall distribution with different weights and parameters. Rather than forcing all data points to conform to a single bell curve, a gaussian mixture allows for multiple clusters, each governed by its own mean, variance, and relative importance in the mixture. This creates a framework for modeling complex, multi-modal distributions that frequently appear in real-world scenarios. Consider, for example, the heights of adults in a population that includes both men and women, or the distribution of pixel intensities in an image containing multiple distinct objects. A single gaussian would poorly capture the true underlying structure, while a mixture model can naturally accommodate the multiple peaks and varying spreads that characterize such data. The beauty of this approach lies not only in its flexibility but also in its mathematical tractability, making it both theoretically sound and computationally feasible for a wide range of applications.\nFor example, suppose we were presented with the following data distribution, displayed here as a histogram:\n\n\n\n\n\n\n\n\n\nThree clusters are immediately evident: the first with a peak around \\(x=10\\), the second around \\(x=15\\), and the third near \\(x=21\\). The presence of three clusters with well-defined peaks suggests that the data might be modeled well by a mixture of three gaussians.\nIndeed, this synthetic dataset contains 1,000 observations sampled from a three-component mixture of gaussians, also called a gaussian mixture model or GMM. In this introductory post, I’ll explain the mathematical and graphical structure of the GMM that generated this data. Since I fixed the GMM parameters before sampling, we know the true underlying model. In a follow-up post, I’ll tackle the inverse problem: given only the dataset, how can we estimate the parameters and recover the original GMM?"
  },
  {
    "objectID": "posts/gmm-1/index.html#introduction",
    "href": "posts/gmm-1/index.html#introduction",
    "title": "Gaussian mixture models: a first look",
    "section": "",
    "text": "A mixture of gaussians represents one of the most elegant and practically useful concepts in probability theory and machine learning. At its core, this is a probabilistic model that assumes observed data arises from a combination of several underlying normal distributions, each contributing to the overall distribution with different weights and parameters. Rather than forcing all data points to conform to a single bell curve, a gaussian mixture allows for multiple clusters, each governed by its own mean, variance, and relative importance in the mixture. This creates a framework for modeling complex, multi-modal distributions that frequently appear in real-world scenarios. Consider, for example, the heights of adults in a population that includes both men and women, or the distribution of pixel intensities in an image containing multiple distinct objects. A single gaussian would poorly capture the true underlying structure, while a mixture model can naturally accommodate the multiple peaks and varying spreads that characterize such data. The beauty of this approach lies not only in its flexibility but also in its mathematical tractability, making it both theoretically sound and computationally feasible for a wide range of applications.\nFor example, suppose we were presented with the following data distribution, displayed here as a histogram:\n\n\n\n\n\n\n\n\n\nThree clusters are immediately evident: the first with a peak around \\(x=10\\), the second around \\(x=15\\), and the third near \\(x=21\\). The presence of three clusters with well-defined peaks suggests that the data might be modeled well by a mixture of three gaussians.\nIndeed, this synthetic dataset contains 1,000 observations sampled from a three-component mixture of gaussians, also called a gaussian mixture model or GMM. In this introductory post, I’ll explain the mathematical and graphical structure of the GMM that generated this data. Since I fixed the GMM parameters before sampling, we know the true underlying model. In a follow-up post, I’ll tackle the inverse problem: given only the dataset, how can we estimate the parameters and recover the original GMM?"
  },
  {
    "objectID": "posts/gmm-1/index.html#conditional-pdfs-and-graphical-structures",
    "href": "posts/gmm-1/index.html#conditional-pdfs-and-graphical-structures",
    "title": "Gaussian mixture models: a first look",
    "section": "Conditional PDFs and graphical structures",
    "text": "Conditional PDFs and graphical structures\nAs mentioned above, a GMM combines several individual gaussians, each called a component of the mixture. Generating a sample \\(x\\) from a GMM involves two steps:\n\nSelect a component from which to generate \\(x\\).\nSample \\(x\\) from the chosen component gaussian.\n\nThe key insight is that components need not be selected with equal probability. Each component has an associated weight that determines its selection probability in step 1.\nTo formalize the selection process, we introduce the categorical distribution. Suppose given non-negative real numbers \\(\\phi_1,\\phi_2,\\ldots,\\phi_m\\) that sum to \\(1\\):\n\\[\n\\phi_1 + \\phi_2 + \\cdots + \\phi_m =1.\n\\]\nWe say that a discrete random variable \\(Y\\) has a categorical distribution, written\n\\[\nY \\sim \\mathcal{C}at(\\phi_1,\\phi_2,\\ldots,\\phi_m),\n\\tag{1}\\]\nif its probability mass function has the form\n\\[\np(y) = \\begin{cases}\n\\phi_y & : y=1,2,\\ldots,m, \\\\\n0 & : \\text{otherwise}.\n\\end{cases}\n\\]\nWe can now describe GMM sampling more precisely. For a GMM with \\(m\\) components where the \\(i\\)-th component has weight \\(\\phi_i\\), and \\(Y\\) a categorical random variable as in Equation 1, the sampling scheme above becomes:\n\nSample \\(y\\) from the categorical distribution \\(Y\\).\nGiven \\(y\\), sample \\(x\\) from the \\(y\\)-th component gaussian.\n\nThis natural progression from component selection to data generation is often called forward sampling, for obvious reasons.\nWe can visualize this sampling process with a simple graphical representation:\n\n\n\n\n\nHere, \\(Y\\) represents the categorical random variable that encodes component weights, while \\(X\\) represents the random variable from which we sample our observation \\(x\\). The arrow captures the “forward” direction of the sampling process: first select a component, then generate data from it. This diagram illustrates the GMM as a simple example of a probabilistic graphical model (or PGM)—a topic I’ll explore in depth in future posts.\nThe mathematical relationship between these variables is straightforward. Given that component \\(y\\) has been selected, the conditional distribution of \\(X\\) is gaussian:\n\\[\n(X \\mid Y=y) \\sim \\mathcal{N}(\\mu_y,\\sigma_y^2).\n\\]\nThis conditional distribution corresponds exactly to the \\(y\\)-th component of our GMM. Notice that both the mean \\(\\mu_y\\)​ and standard deviation \\(\\sigma_y\\) are component-specific, allowing each gaussian in the mixture to have its own location and spread.\nA GMM thus has two different types of parameters:\n\nThe component weights \\(\\phi_1,\\phi_2,\\ldots,\\phi_m\\).\nFor each \\(y=1,2,\\ldots,m\\), the guassian parameters \\(\\mu_y\\) and \\(\\sigma_y\\).\n\nAs I mentioned in the introduction, in this post we assume that these parameters are fixed and known. Later, we will learn how to estimate them from data."
  },
  {
    "objectID": "posts/gmm-1/index.html#marginal-pdfs",
    "href": "posts/gmm-1/index.html#marginal-pdfs",
    "title": "Gaussian mixture models: a first look",
    "section": "Marginal PDFs",
    "text": "Marginal PDFs\nThe marginal distribution of \\(X\\) follows directly from the law of total probability: \\[\nf(x) = \\sum_{y=1}^m p(y)f(x|y)  = \\sum_{y=1}^m \\phi_y f(x|y),\n\\tag{2}\\]\nwhere the conditional densities are normal:\n\\[\nf(x|y) = \\frac{1}{\\sigma_y\\sqrt{2\\pi}} \\exp \\left[ - \\frac{1}{2} \\left( \\frac{x-\\mu_y}{\\sigma_y}\\right)^2\\right].\n\\]\nThus, the marginal density \\(f(x)\\) is a convex linear combination of normal densities.\nNow let’s examine the specific \\(3\\)-component GMM that generated our dataset. The component parameters are:\n\\[\n(\\mu_1,\\sigma_1) = (10, 1), \\quad (\\mu_2,\\sigma_2) = (15, 2), \\quad (\\mu_3,\\sigma_3) = (21, 0.5),\n\\]\nwith weights:\n\\[\n(\\phi_1,\\phi_2,\\phi_3) = (0.2, 0.7, 0.1).\n\\]\nNotice that the middle component (centered at 15) dominates with 70% weight, while the outer components contribute 20% and 10% respectively.\nLet’s implement and visualize the component gaussians and the marginal PDF in Python. First, let’s load the parameters:\n\nnorm_params = [\n  {\"loc\": 10, \"scale\": 1},\n  {\"loc\": 15, \"scale\": 2},\n  {\"loc\": 21, \"scale\": 0.5}\n]\nweights = [0.2, 0.7, 0.1]\n\nNow, we plot the individual components, each scaled by its weight:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nmesh = np.linspace(7, 23, num=200)\n\nfor y, (param, weight) in enumerate(zip(norm_params, weights)):\n  X = norm(**param)\n  plt.plot(mesh, weight * X.pdf(mesh), color=colors[y], label=f\"component {y+1}\")\n\nplt.xlabel(\"x\")\nplt.ylabel(\"density\")\nplt.title(\"gaussian components\")\nplt.legend()\nfig = plt.gcf()  # Get current figure\nfig.set_size_inches(6, 4)\nplt.tight_layout()\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\nThe marginal PDF implementation follows Equation 2 directly:\n\ndef marginal_pdf(x):  \n  return sum([weight * norm(**param).pdf(x) for weight, param in zip(weights, norm_params)])\n\nFinally, we plot the theoretical marginal PDF against the empirical histogram:\n\nimport seaborn as sns\n\ndata = np.load(\"../../data/gmm-data.npy\")\n\nsns.histplot(data=data, color=yellow, alpha=0.25, ec=grey, zorder=2, stat=\"density\", label=\"data\")\nplt.plot(mesh, marginal_pdf(mesh), color=blue, label=\"marginal PDF\")\nplt.xlabel(\"x\")\nplt.ylabel(\"density\")\nplt.legend()\nfig = plt.gcf()  # Get current figure\nfig.set_size_inches(6, 4)\nplt.tight_layout()\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\nThe close agreement between our theoretical marginal PDF and the empirical histogram confirms that our synthetic dataset faithfully represents the underlying GMM. Notice how the theoretical curve captures all three modes: the subtle left peak around \\(x=10\\), the dominant central peak near \\(x=15\\), and the sharp right peak at \\(x=21\\). The varying heights and widths of these peaks directly reflect the component weights and standard deviations we specified."
  },
  {
    "objectID": "posts/gmm-1/index.html#sampling",
    "href": "posts/gmm-1/index.html#sampling",
    "title": "Gaussian mixture models: a first look",
    "section": "Sampling",
    "text": "Sampling\nHaving established the theoretical foundation and visualized the marginal PDF, let’s now examine the process behind generating our synthetic dataset.\nBut this is easy. We first sample 1,000 values of \\(y\\) from the categorical random variable\n\\[\nY \\sim \\mathcal{C}at(0.2, 0.7, 0.1),\n\\]\nand then for each \\(y\\) we sample from the gaussian \\(X|Y=y\\). We check that our new generated sample coincides with the original dataset:\n\nnp.random.seed(42) # Needed to replicate the original dataset\n\nn_samples = 1000\ny_samples = np.random.choice([0, 1, 2], size=n_samples, p=weights)\nx_samples = np.array([norm(**norm_params[y]).rvs() for y in y_samples])\n\nprint(\"Are the datasets equal? \" + (\"Yes.\" if np.array_equal(data, x_samples) else \"No.\"))\n\nAre the datasets equal? Yes."
  },
  {
    "objectID": "posts/gmm-1/index.html#conclusion",
    "href": "posts/gmm-1/index.html#conclusion",
    "title": "Gaussian mixture models: a first look",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we’ve built the GMM from the ground up, starting with its definition, exploring its conditional and marginal structure, and even sampling from it directly. But in practice, the real challenge is the inverse problem we mentioned earlier: we are usually given a dataset and want to fit a GMM by estimating its parameters. How do we tease apart the hidden components, estimate their weights, and recover the underlying mixture? That puzzle leads directly to one of the most beautiful iterative methods in statistics and machine learning: the expectation–maximization algorithm, which will be the focus of the next post in this series."
  },
  {
    "objectID": "teaching/calculus-ii-fa-25.html",
    "href": "teaching/calculus-ii-fa-25.html",
    "title": "mat220 calculus II, fall 2025",
    "section": "",
    "text": "instructor:\n\n\njohn myers\n\n\n\n\noffice + office hours:\n\n\nmarano 175, 12-12:30 MWF\n\n\n\n\nhomework:\n\n\nlink\n\n\n\n\nsyllabus:\n\n\nlink\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndate\ntopics\ninfo + due dates\n\n\n\n\n7\n10.10 fri\nno class - fall break\n\n\n\n\n10.09 thu\nno class - fall break\n\n\n\n\n10.08 wed\n\nsection 7.3, part 2, hw due @ 12:30pm\n\n\n\n10.06 mon\n\nsection 7.3, part 1, hw due @ 12:30pm\n\n\n6\n10.03 fri\n7.3 trigonometric substitution, part 2  \nquiz 6 on section 7.1\n\n\n\n10.04 thu\n7.3 trigonometric substitution, part 1    7.3 trigonometric substitution, part 2  \n\n\n\n\n10.01 wed\n7.3 trigonometric substitution, part 1  \nsection 7.1 hw due @ 12:30pm\n\n\n\n09.29 mon\n7.1 integration by parts  \n\n\n\n5\n09.26 fri\n7.1 integration by parts  \nquiz 5 on sections 5.4 and 5.5\n\n\n\n09.25 thu\ngroup exploration 5\n\n\n\n\n09.24 wed\n7.1 integration by parts  \nsection 5.5 hw due @ 12:30pm\n\n\n\n09.22 mon\n5.5 average value of a function  \nsection 5.4 hw due @ 12:30pm\n\n\n4\n09.19 fri\n5.4 work  \nsection 5.3 hw due @ 12:30pmquiz 4 on section 5.3\n\n\n\n09.18 thu\ngroup exploration 4\n\n\n\n\n09.17 wed\n5.4 work  \n\n\n\n\n09.15 mon\n5.3 volumes by cylindrical shells  \nsection 5.2 hw due @ 12:30pm\n\n\n3\n09.12 fri\n5.2 volumes, part 2  \nquiz 3 on section 5.1\n\n\n\n09.11 thu\ngroup exploration 3\n\n\n\n\n09.10 wed\n5.2 volumes, part 1  5.2 volumes, part 2  \n\n\n\n\n09.08 mon\n5.2 volumes, part 1  \nsection 5.1 hw due @ 12:30pm\n\n\n2\n09.05 fri\n5.1 areas between curves  \n\n\n\n\n09.04 thu\ngroup exploration 2\ncalculus I review homework due @ 9:30am\n\n\n\n09.03 wed\ncalculus I review, part 2  \n\n\n\n1\n08.29 fri\nno class\n\n\n\n\n08.28 thu\nno class\n\n\n\n\n08.27 wed\ncalculus I review, part 1  \n\n\n\n\n08.25 mon\ncalculus I review, part 1"
  },
  {
    "objectID": "scripts/lin-reg-1.html",
    "href": "scripts/lin-reg-1.html",
    "title": "john myers, ph.d.",
    "section": "",
    "text": "from scipy.stats import norm, multivariate_normal, entropy\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\n\nnp.random.seed(42)\nplt.style.use(\"../aux-files/custom-theme.mplstyle\")\nyellow = \"#FFC300\"\nblue = \"#3399FF\"\npink = \"#FF3399\"\ngrey = \"#121212\"\nwhite = \"#E5E5E5\"\ncolors = [yellow, blue, pink]\n\ncmap_colors = [grey, yellow]\ncustom_cmap = LinearSegmentedColormap.from_list(\"my_cmap\", cmap_colors)\n\n\nn = 6\njoint = np.random.rand(n ** 2)\njoint = joint / joint.sum()\njoint = joint.reshape(n, n)\n\nax = sns.heatmap(joint.T, annot=True, fmt=\".3f\", cmap=custom_cmap, linewidth=8, linecolor=grey)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nax.invert_yaxis()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=3, sharey=True, sharex=True)\naxes = axes.flatten()\nconditionals = []\n\nfor x, ax in enumerate(axes):\n  conditional = joint[x, :] / joint[x, :].sum()\n  conditionals.append(conditional)\n  ax.bar(range(n), conditional, width=0.4, zorder=2)\n  ax.set_xticks(range(n))  \n  ax.set_xticklabels(range(n))\n  ax.set_title(rf\"$x={x}$\")\n  \nfig.supxlabel(r\"$y$\")\nfig.supylabel(\"probability\")\nfig.suptitle(r\"conditional distributions $f(y\\mid x)$\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nX = joint.sum(axis=1)\nY = joint.sum(axis=0)\n\nfig, axes = plt.subplots(ncols=2, sharey=True, sharex=True, figsize=(7, 3))\n\naxes[0].bar(range(n), X, width=0.4)\naxes[0].set_xlabel(r\"$x$\")\naxes[0].set_title(r\"marginal distribution $f(x)$\")\n\naxes[1].bar(range(n), Y, width=0.4)\naxes[1].set_xlabel(r\"$y$\")\naxes[1].set_title(r\"marginal distribution $f(y)$\")\n\nfig.supylabel(\"probability\")\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.4)\nplt.show()\n\n\n\n\n\n\n\n\n\ninfo = entropy(Y) - sum([entropy(conditional) * X[x] for x, conditional in enumerate(conditionals)])\nprint(f\"The mutual information between X and Y is {info:.4f}.\")\n\nThe mutual information between $X$ and $Y$ is 0.2032.\n\n\n\ndef plot_multivar(ax, muX, muY, sigmaX, sigmaY, x, y, labels=False):\n  Sigma = np.array([[sigmaX ** 2, rho * sigmaX * sigmaY], [rho * sigmaX * sigmaY, sigmaY ** 2]])\n  Mu = np.array([muX, muY])\n  U = multivariate_normal(mean=Mu, cov=Sigma)\n  grid = np.dstack((x, y))\n  z = U.pdf(grid)\n  contour = ax.contour(x, y, z, colors=yellow, alpha=0.5)\n  if labels:\n    ax.clabel(contour, inline=True, fontsize=8)\n  \ndef plot_conditional(ax, muX, muY, sigmaX, sigmaY, rho, y, x_obs):\n  mu = muY + (x_obs - muX) * rho * sigmaY / sigmaX\n  sigma = sigmaY * np.sqrt(1 - rho ** 2)\n  x = norm(loc=mu, scale=sigma).pdf(y)\n  ax.plot(-x + x_obs, y, color=blue)\n  ax.fill_betweenx(y, -x + x_obs, x_obs, color=blue, alpha=0.4)\n\ndef plot_combined(ax, muX, muY, sigmaX, sigmaY, rho, x, y, x_obs, labels=False):\n  plot_multivar(ax, muX, muY, sigmaX, sigmaY, x, y, labels)\n  y = np.linspace(np.min(y), np.max(y), num=250)\n  plot_conditional(ax, muX, muY, sigmaX, sigmaY, rho, y, x_obs[0])\n  plot_conditional(ax, muX, muY, sigmaX, sigmaY, rho, y, x_obs[1])\n  plot_conditional(ax, muX, muY, sigmaX, sigmaY, rho, y, x_obs[2])\n  ax.set_title(rf\"$\\rho ={rho}$\")\n  ax.set_xlabel(r\"$x$\")\n  ax.set_ylabel(r\"$y$\")\n  plt.tight_layout()\n  plt.show()\n\n_, ax = plt.subplots()\nx, y = np.mgrid[-1:3:0.01, -4:6:0.01]\n\nmuX = 1\nmuY = 1\nsigmaX = 1\nsigmaY = 2\nrho = 0.15\n\nplot_combined(ax, muX, muY, sigmaX, sigmaY, rho, x, y, x_obs=[0, 1, 2], labels=False)\n\n\n\n\n\n\n\n\n\n_, ax = plt.subplots()\nrho = 0.50\nplot_combined(ax, muX, muY, sigmaX, sigmaY, rho, x, y, x_obs=[0, 1, 2], labels=False)\n\n\n\n\n\n\n\n\n\n_, ax = plt.subplots()\nrho = 0.85\nplot_combined(ax, muX, muY, sigmaX, sigmaY, rho, x, y, x_obs=[0, 1, 2], labels=False)"
  }
]