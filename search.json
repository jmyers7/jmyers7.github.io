[
  {
    "objectID": "writings.html",
    "href": "writings.html",
    "title": "john myers, ph.d.",
    "section": "",
    "text": "I like to write about technical stuff. I’m an academic, so my writing can sometimes read like a research paper, which is to say: dense. But at least I’m self-aware and working on it. I promise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing SigAlg: Measure-Theoretic Probability in Python\n\n\n\nStochastic process\n\nIID process\n\nRandom walk\n\nProbability theory\n\nSigma algebras\n\nFiltrations\n\nInformation theory\n\nSigAlg\n\nSciPy\n\nPython\n\n\n\n\n\n\n\n\n\nFeb 3, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nAlgebras & information\n\n\n\nInformation theory\n\nProbability theory\n\nMeasure theory\n\nEntropy\n\nMutual information\n\nSigma-algebras\n\nR\n\n\n\n\n\n\n\n\n\nOct 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEntropy & information\n\n\n\nInformation theory\n\nProbability theory\n\nEntropy\n\nSurprisal\n\nKL divergence\n\nMutual information\n\nPython\n\nSciPy\n\n\n\n\n\n\n\n\n\nOct 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian mixture models I: a first look\n\n\n\nMixture models\n\nGaussian mixture models\n\nProbabilistic graphical models\n\nMachine learning\n\nProbability theory\n\nPython\n\n\n\n\n\n\n\n\n\nSep 26, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/04ex-general-pdes-and-boundary-conditions.html",
    "href": "teaching/pde-sp-26/exercises/04ex-general-pdes-and-boundary-conditions.html",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "Consider the wave equation for a vibrating rectangular membrane of dimensions \\(a\\) by \\(b\\), where the displacement is given by \\(u(x,y,t)\\) with \\((x,y) \\in [0,a] \\times [0,b]\\) and \\(t \\geq 0\\).\n\nWhat is the dimension of the domain \\(D\\) for this PDE?\nSketch or describe what a spatial slice at time \\(t = t_0\\) looks like geometrically.\nIf we track the displacement at a single point \\((x_0, y_0)\\) over all time, what geometric object does this trace out in the domain \\(D\\)?\nSuppose instead we consider the steady-state version where \\(u\\) solves Laplace’s equation (so \\(u = u(x,y)\\) with no time dependence). How does the domain change?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe domain is \\[\nD =\\{ (x,y,t) : 0 \\leq x \\leq a, \\, 0 \\leq y \\leq b, \\, t \\geq 0\\},\n\\] which is 3-dimensional (two spatial dimensions plus time).\nA spatial slice at time \\(t = t_0\\) is the rectangle \\([0,a] \\times [0,b]\\) in the \\(xy\\)-plane. Geometrically, this represents the entire membrane at a single frozen moment in time. The value \\(u(x,y,t_0)\\) at each point gives the vertical displacement of the membrane at that location at time \\(t_0\\).\nTracking a single point \\((x_0, y_0)\\) over all time traces out a vertical line (or ray) in the \\(t\\)-direction: the set \\(\\{(x_0, y_0, t) : t \\geq 0\\}\\). This is a 1-dimensional curve in the 3-dimensional domain.\nFor Laplace’s equation, there is no time dependence, so the domain becomes \\[\nD = \\{(x,y) : 0 \\leq x \\leq a, \\, 0 \\leq y \\leq b\\},\n\\] which is 2-dimensional. This is just the rectangle representing the membrane itself.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor each differential operator below, determine whether it is linear or nonlinear. If it is linear, state whether the PDE \\(L(u) = 0\\) is homogeneous or non-homogeneous.\n\n\\(L(u) = \\frac{\\partial u}{\\partial t} - \\frac{\\partial^2 u}{\\partial x^2} + e^x\\)\n\\(L(u) = \\frac{\\partial^2 u}{\\partial x^2} + \\left(\\frac{\\partial u}{\\partial x}\\right)^2\\)\n\\(L(u) = x^2 \\frac{\\partial^2 u}{\\partial x^2} + y \\frac{\\partial u}{\\partial y} - 3u\\)\n\\(L(u) = \\nabla^2 u + e^u\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThis operator is not linear because it includes a term \\(e^x\\). In particular, notice that we don’t have \\(L(0) = 0\\) due to this \\(e^x\\) term. (Every linear transformation must map the zero function to zero.)\nThis operator is not linear because of the \\(\\left(\\frac{\\partial u}{\\partial x}\\right)^2\\) term. To verify, notice, in particular, that \\[\nL(2x^2) = \\frac{\\partial^2 (2x^2)}{\\partial x^2} + \\left(\\frac{\\partial (2x^2)}{\\partial x}\\right)^2 = 4 + 16x^2.\n\\] However, we have \\[\n2L(x^2) = 2\\left(\\frac{\\partial^2 (x^2)}{\\partial x^2} + \\left(\\frac{\\partial (x^2)}{\\partial x}\\right)^2\\right) = 2(2 + 4x^2) = 4 + 8x^2.\n\\] Thus, \\(L(2x^2) \\neq 2L(x^2)\\), so \\(L\\) is nonlinear.\nThis operator is linear. The coefficients \\(x^2\\), \\(y\\), and \\(-3\\) are all functions of the independent variables (not of \\(u\\)), and \\(u\\) and its derivatives appear linearly (i.e., no products or nonlinear functions of \\(u\\) or its derivatives).\nThis operator is not linear because of the \\(e^u\\) term, which depends nonlinearly on \\(u\\) itself. To verify, notice, in particular, that \\[\nL(0) = \\nabla^2 0 + e^0 = 1 \\neq 0,\n\\]\n\nwhile for any linear operator we must have \\(L(0) = 0\\).\n\n\n\n\n\n\n\n\n\n\n\n\nFor each physical scenario below, determine the appropriate type of boundary condition (Dirichlet or Neumann) and write it down mathematically. Explain your reasoning.\n\nA metal wire of length \\(L\\) has its left end held at a constant temperature of \\(100°C\\) and its right end held at \\(0°C\\). We model the temperature as \\(u(x,t)\\) where \\(x \\in [0,L]\\).\nA metal rod of length \\(L\\) is perfectly insulated at both ends, so no heat can flow in or out. The temperature is \\(u(x,t)\\) where \\(x \\in [0,L]\\).\nA vibrating guitar string of length \\(L\\) is pinned down at both ends (displacement is zero there). The displacement is \\(u(x,t)\\) where \\(x \\in [0,L]\\).\nA hot metal plate occupies the square \\([0,1] \\times [0,1]\\), and you hold the temperature constant at \\(50°C\\) along the entire boundary. The temperature is \\(u(x,y,t)\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThis requires Dirichlet boundary conditions because the temperature values are specified directly at the endpoints: \\[\nu(0,t) = 100, \\quad u(L,t) = 0 \\quad \\text{for all } t &gt; 0.\n\\]\nThis requires Neumann boundary conditions. Since the rod is insulated, no heat flows through the endpoints, meaning that \\[\n\\frac{\\partial u}{\\partial x}(0,t) = 0, \\quad \\frac{\\partial u}{\\partial x}(L,t) = 0 \\quad \\text{for all } t &gt; 0.\n\\]\nThis requires Dirichlet boundary conditions because the displacement is specified (as zero) at the endpoints: \\[\nu(0,t) = 0, \\quad u(L,t) = 0 \\quad \\text{for all } t &gt; 0.\n\\]\nThis requires Dirichlet boundary conditions on the entire boundary of the square. If we denote the boundary as \\(\\partial D\\), then: \\[\nu(x,y,t) = 50 \\quad \\text{for all } (x,y) \\in \\partial D, \\, t &gt; 0.\n\\] More explicitly, this means: \\[\n\\begin{align*}\nu(0,y,t) &= 50 \\quad \\text{for } y \\in [0,1], \\\\\nu(1,y,t) &= 50 \\quad \\text{for } y \\in [0,1], \\\\\nu(x,0,t) &= 50 \\quad \\text{for } x \\in [0,1], \\\\\nu(x,1,t) &= 50 \\quad \\text{for } x \\in [0,1].\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsider the wave equation \\(\\frac{\\partial^2 u}{\\partial t^2} = a^2 \\frac{\\partial^2 u}{\\partial x^2}\\) on the interval \\([0,L]\\), modeling a string of length \\(L\\) held fixed at both ends. At \\(t=0\\), the string has the shape \\(u(x,0) = x(L-x)\\) and is released from rest. Write down all the boundary and initial conditions for this problem.\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nBoundary conditions (Dirichlet, since the string is fixed at both ends): \\[\nu(0,t) = 0, \\quad u(L,t) = 0 \\quad \\text{for all } t &gt; 0.\n\\] Initial conditions: \\[\nu(x,0) = x(L-x) \\quad \\text{for all } x \\in [0,L],\n\\] \\[\n\\frac{\\partial u}{\\partial t}(x,0) = 0 \\quad \\text{for all } x \\in [0,L].\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/04ex-general-pdes-and-boundary-conditions.html#exercise-1-visualizing-spatial-slices",
    "href": "teaching/pde-sp-26/exercises/04ex-general-pdes-and-boundary-conditions.html#exercise-1-visualizing-spatial-slices",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "Consider the wave equation for a vibrating rectangular membrane of dimensions \\(a\\) by \\(b\\), where the displacement is given by \\(u(x,y,t)\\) with \\((x,y) \\in [0,a] \\times [0,b]\\) and \\(t \\geq 0\\).\n\nWhat is the dimension of the domain \\(D\\) for this PDE?\nSketch or describe what a spatial slice at time \\(t = t_0\\) looks like geometrically.\nIf we track the displacement at a single point \\((x_0, y_0)\\) over all time, what geometric object does this trace out in the domain \\(D\\)?\nSuppose instead we consider the steady-state version where \\(u\\) solves Laplace’s equation (so \\(u = u(x,y)\\) with no time dependence). How does the domain change?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe domain is \\[\nD =\\{ (x,y,t) : 0 \\leq x \\leq a, \\, 0 \\leq y \\leq b, \\, t \\geq 0\\},\n\\] which is 3-dimensional (two spatial dimensions plus time).\nA spatial slice at time \\(t = t_0\\) is the rectangle \\([0,a] \\times [0,b]\\) in the \\(xy\\)-plane. Geometrically, this represents the entire membrane at a single frozen moment in time. The value \\(u(x,y,t_0)\\) at each point gives the vertical displacement of the membrane at that location at time \\(t_0\\).\nTracking a single point \\((x_0, y_0)\\) over all time traces out a vertical line (or ray) in the \\(t\\)-direction: the set \\(\\{(x_0, y_0, t) : t \\geq 0\\}\\). This is a 1-dimensional curve in the 3-dimensional domain.\nFor Laplace’s equation, there is no time dependence, so the domain becomes \\[\nD = \\{(x,y) : 0 \\leq x \\leq a, \\, 0 \\leq y \\leq b\\},\n\\] which is 2-dimensional. This is just the rectangle representing the membrane itself."
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/04ex-general-pdes-and-boundary-conditions.html#exercise-2-recognizing-linear-vs.-nonlinear-operators",
    "href": "teaching/pde-sp-26/exercises/04ex-general-pdes-and-boundary-conditions.html#exercise-2-recognizing-linear-vs.-nonlinear-operators",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "For each differential operator below, determine whether it is linear or nonlinear. If it is linear, state whether the PDE \\(L(u) = 0\\) is homogeneous or non-homogeneous.\n\n\\(L(u) = \\frac{\\partial u}{\\partial t} - \\frac{\\partial^2 u}{\\partial x^2} + e^x\\)\n\\(L(u) = \\frac{\\partial^2 u}{\\partial x^2} + \\left(\\frac{\\partial u}{\\partial x}\\right)^2\\)\n\\(L(u) = x^2 \\frac{\\partial^2 u}{\\partial x^2} + y \\frac{\\partial u}{\\partial y} - 3u\\)\n\\(L(u) = \\nabla^2 u + e^u\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThis operator is not linear because it includes a term \\(e^x\\). In particular, notice that we don’t have \\(L(0) = 0\\) due to this \\(e^x\\) term. (Every linear transformation must map the zero function to zero.)\nThis operator is not linear because of the \\(\\left(\\frac{\\partial u}{\\partial x}\\right)^2\\) term. To verify, notice, in particular, that \\[\nL(2x^2) = \\frac{\\partial^2 (2x^2)}{\\partial x^2} + \\left(\\frac{\\partial (2x^2)}{\\partial x}\\right)^2 = 4 + 16x^2.\n\\] However, we have \\[\n2L(x^2) = 2\\left(\\frac{\\partial^2 (x^2)}{\\partial x^2} + \\left(\\frac{\\partial (x^2)}{\\partial x}\\right)^2\\right) = 2(2 + 4x^2) = 4 + 8x^2.\n\\] Thus, \\(L(2x^2) \\neq 2L(x^2)\\), so \\(L\\) is nonlinear.\nThis operator is linear. The coefficients \\(x^2\\), \\(y\\), and \\(-3\\) are all functions of the independent variables (not of \\(u\\)), and \\(u\\) and its derivatives appear linearly (i.e., no products or nonlinear functions of \\(u\\) or its derivatives).\nThis operator is not linear because of the \\(e^u\\) term, which depends nonlinearly on \\(u\\) itself. To verify, notice, in particular, that \\[\nL(0) = \\nabla^2 0 + e^0 = 1 \\neq 0,\n\\]\n\nwhile for any linear operator we must have \\(L(0) = 0\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/04ex-general-pdes-and-boundary-conditions.html#exercise-3-boundary-condition-detective-work",
    "href": "teaching/pde-sp-26/exercises/04ex-general-pdes-and-boundary-conditions.html#exercise-3-boundary-condition-detective-work",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "For each physical scenario below, determine the appropriate type of boundary condition (Dirichlet or Neumann) and write it down mathematically. Explain your reasoning.\n\nA metal wire of length \\(L\\) has its left end held at a constant temperature of \\(100°C\\) and its right end held at \\(0°C\\). We model the temperature as \\(u(x,t)\\) where \\(x \\in [0,L]\\).\nA metal rod of length \\(L\\) is perfectly insulated at both ends, so no heat can flow in or out. The temperature is \\(u(x,t)\\) where \\(x \\in [0,L]\\).\nA vibrating guitar string of length \\(L\\) is pinned down at both ends (displacement is zero there). The displacement is \\(u(x,t)\\) where \\(x \\in [0,L]\\).\nA hot metal plate occupies the square \\([0,1] \\times [0,1]\\), and you hold the temperature constant at \\(50°C\\) along the entire boundary. The temperature is \\(u(x,y,t)\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThis requires Dirichlet boundary conditions because the temperature values are specified directly at the endpoints: \\[\nu(0,t) = 100, \\quad u(L,t) = 0 \\quad \\text{for all } t &gt; 0.\n\\]\nThis requires Neumann boundary conditions. Since the rod is insulated, no heat flows through the endpoints, meaning that \\[\n\\frac{\\partial u}{\\partial x}(0,t) = 0, \\quad \\frac{\\partial u}{\\partial x}(L,t) = 0 \\quad \\text{for all } t &gt; 0.\n\\]\nThis requires Dirichlet boundary conditions because the displacement is specified (as zero) at the endpoints: \\[\nu(0,t) = 0, \\quad u(L,t) = 0 \\quad \\text{for all } t &gt; 0.\n\\]\nThis requires Dirichlet boundary conditions on the entire boundary of the square. If we denote the boundary as \\(\\partial D\\), then: \\[\nu(x,y,t) = 50 \\quad \\text{for all } (x,y) \\in \\partial D, \\, t &gt; 0.\n\\] More explicitly, this means: \\[\n\\begin{align*}\nu(0,y,t) &= 50 \\quad \\text{for } y \\in [0,1], \\\\\nu(1,y,t) &= 50 \\quad \\text{for } y \\in [0,1], \\\\\nu(x,0,t) &= 50 \\quad \\text{for } x \\in [0,1], \\\\\nu(x,1,t) &= 50 \\quad \\text{for } x \\in [0,1].\n\\end{align*}\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/04ex-general-pdes-and-boundary-conditions.html#exercise-4-initial-vs.-boundary-value-problems",
    "href": "teaching/pde-sp-26/exercises/04ex-general-pdes-and-boundary-conditions.html#exercise-4-initial-vs.-boundary-value-problems",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "Consider the wave equation \\(\\frac{\\partial^2 u}{\\partial t^2} = a^2 \\frac{\\partial^2 u}{\\partial x^2}\\) on the interval \\([0,L]\\), modeling a string of length \\(L\\) held fixed at both ends. At \\(t=0\\), the string has the shape \\(u(x,0) = x(L-x)\\) and is released from rest. Write down all the boundary and initial conditions for this problem.\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nBoundary conditions (Dirichlet, since the string is fixed at both ends): \\[\nu(0,t) = 0, \\quad u(L,t) = 0 \\quad \\text{for all } t &gt; 0.\n\\] Initial conditions: \\[\nu(x,0) = x(L-x) \\quad \\text{for all } x \\in [0,L],\n\\] \\[\n\\frac{\\partial u}{\\partial t}(x,0) = 0 \\quad \\text{for all } x \\in [0,L].\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/03ex-the-laplacian-and-laplaces-equation.html",
    "href": "teaching/pde-sp-26/exercises/03ex-the-laplacian-and-laplaces-equation.html",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "In \\(n\\) variables, recall that the Laplacian operator is defined as \\[\n\\nabla^2 =\\sum_{i=1}^n \\frac{\\partial^2 }{\\partial x_i^2}.\n\\]\nRecall also that if \\(u(x_1, \\dots, x_n, t)\\) is a time-dependent function, the Laplacian only acts on the spatial variables.\n\n\n\n\n\n\nCompute the Laplacians of the following functions, using the definition above.\n\n\\(u(x,y) = x^2 + y^2\\)\n\\(u(x,y,z) = 2x^2\\sin{y} + e^z\\)\n\\(u(x) = x^2e^x\\)\n\\(u(x,y,t) = x^2yt + y^2t + t^2\\cos{x}\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(4\\)\n\\(-2 x^2 \\sin {y}+4 \\sin {y}+e^z\\)\n\\(e^x x^2+4 e^x x+2 e^x\\)\n\\(-t^2 \\cos {x}+ 2 y t+2 t\\)\n\n\n\n\n\n\n\nLet \\(u(x,y)\\) be a function with continuous second-order partial derivatives. In class, we showed that the Laplacian may be computed as the limit\n\\[\n(\\nabla^2 u)(x_0,y_0) = \\lim_{r\\to 0^+}\\left( \\frac{4}{r^2}\\cdot \\frac{1}{2\\pi r} \\int_C \\big( u(x,y) - u(x_0,y_0)\\big) \\, ds\\right),\n\\]\nwhere \\(C\\) is a circle centered at \\((x_0,y_0)\\) with radius \\(r\\). This formula sheds some light on the intuitive idea that the Laplacian measures how the value of a function at a point compares to the average value around that point.\n\n\n\n\n\n\nSuppose \\(u(x,y) = \\sin(x^2+y^2)\\).\n\nCompute the Laplacian at \\((x_0,y_0) = (0,0)\\) using the limit formula above.\nCheck that your answer is correct by computing the Laplacian from the definition.\nUse your value for the Laplacian to estimate the average value of \\(u\\) around the point \\((0,0)\\) on a circle of radius \\(r=0.1\\). Compare this to the true average value.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nParametrize the circle \\(C\\) as \\(x=r\\cos{t}\\) and \\(y=r\\sin{t}\\) for \\(0\\leq t \\leq 2\\pi\\). Then \\(dx = -r\\sin{t}\\, dt\\) and \\(dy = r\\cos{t}\\, dt\\), so that \\[\nds = \\sqrt{ dx^2 + dy^2} = \\sqrt{r^2\\sin^2{t}+ r^2\\cos^2{t}}\\, dt = r\\, dt\n\\] We then have \\[\n\\int_C \\big( u(x,y) - u(0,0)\\big) \\, ds = \\int_0^{2\\pi} r\\sin(r^2) \\, dt = 2\\pi r \\sin(r^2).\n\\] Inserting this into the limit formula gives \\[\n\\begin{align*}\n(\\nabla^2 u)(0,0) &= \\lim_{r\\to 0^+} \\frac{4}{r^2} \\cdot \\frac{1}{2\\pi r} \\cdot 2\\pi r \\sin(r^2) \\\\\n&= \\lim_{r\\to 0^+} \\frac{4\\sin(r^2)}{r^2}  \\\\\n&= \\lim_{r\\to 0^+} \\frac{ 8r\\cos(r^2)}{2r} \\\\\n&= 4.\n\\end{align*}\n\\]\nWe first compute: \\[\n\\begin{align*}\n\\frac{\\partial u}{\\partial x} &= 2x \\cos(x^2+y^2), \\\\\n\\frac{\\partial u}{\\partial y} &= 2y \\cos(x^2+y^2).\n\\end{align*}\n\\] Then: \\[\n\\begin{align*}\n\\frac{\\partial^2 u}{\\partial x^2} &= 2\\cos(x^2+y^2) - 4x^2\\sin(x^2+y^2), \\\\\n\\frac{\\partial^2 u}{\\partial y^2} &= 2\\cos(x^2+y^2) - 4y^2\\sin(x^2+y^2).\n\\end{align*}\n\\] Hence: \\[\n(\\nabla^2 u)(0,0) = \\frac{\\partial^2 u}{\\partial x^2}(0,0) + \\frac{\\partial^2 u}{\\partial y^2}(0,0) = 2 + 2 = 4.\n\\]\nAs long as \\(r\\) is small, the limit formula for the Laplacian may be rearranged to give \\[\n\\frac{r^2}{4} \\cdot (\\nabla^2 u)(0,0) \\approx \\frac{1}{2\\pi r} \\int_C u(x,y) \\, ds.\n\\] Note that \\(u(0,0)=0\\), so the integral on the right is the true average value of \\(u\\) around the circle of radius \\(r\\) centered at \\((0,0)\\). But from our solution to (a), we know \\[\n\\frac{1}{2\\pi r} \\int_C u(x,y) \\, ds = \\sin(r^2).\n\\] On the other hand, we have \\[\n\\frac{r^2}{4} \\cdot (\\nabla^2 u)(0,0) = r^2.\n\\] Note that \\[\nr^2 = 0.01 \\quad \\text{and} \\quad \\sin(r^2) \\approx 0.0099998\n\\] when \\(r=0.1\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsider Laplace’s equation for functions \\(u(x,y)\\) on the rectangle\n\\[\n0 \\leq x \\leq 2\\pi, \\quad 0 \\leq y \\leq 1,\n\\]\nin \\(\\mathbb{R}^2\\). Suppose we impose the boundary conditions\n\\[\nu(x,0) = \\sin{x}-\\sin\\left(\\frac{3x}{2}\\right) \\quad \\text{and} \\quad u(x,1) = e\\sin{x} - e^{3/2}\\sin\\left(\\frac{3x}{2}\\right)\n\\]\nalong the bottom and top edges of the rectangle (draw the rectangle in the \\(xy\\)-plane), and\n\\[\nu(0,y) = 0, \\quad u(2\\pi,y) = 0\n\\]\nalong the left and right edges of the rectangle.\n\nUsing separation of variables, solve Laplace’s equation.\nPlot your solution using Desmos. Be sure to adjust the \\(x\\)- and \\(y\\)-axes so that you only view your solution over the rectangle \\(0\\leq x \\leq 2\\pi\\) and \\(0\\leq y \\leq 1\\). We discussed in class that harmonic functions are the optimally “boring” functions that are consistent with the boundary conditions. Does your plot support this intuition? Does your solution exhibit any unexpected features? Any extreme values that are not on the boundary?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nSuppose that \\(u\\) factors as \\[\nu(x,y) = X(x)Y(y).\n\\] Substituting this into Laplace’s equation gives \\[\nX''(x)Y(y) + X(x)Y''(y) = 0.\n\\] Rearranging and dividing both sides by \\(X(x)Y(y)\\) gives \\[\n\\frac{X''(x)}{X(x)} =- \\frac{Y''(y)}{Y(y)}.\n\\] Since the left-hand side is a function only of \\(x\\), and the right-hand side is a function only of \\(y\\), each side must be equal to a nonzero real constant, say \\(-\\lambda\\). Then \\[\n\\frac{X''(x)}{X(x)} = -\\lambda \\quad \\text{and} \\quad \\frac{Y''(y)}{Y(y)} = \\lambda,\n\\] which gives us the system of two ODEs \\[\nX''(x) + \\lambda X(x) = 0, \\quad Y''(y) - \\lambda Y(y) = 0.\n\\] The boundary conditions translate to \\[\nX(x)Y(0) = \\sin{x}-\\sin\\left(\\frac{3x}{2}\\right) \\quad \\text{and} \\quad X(x)Y(1)=e\\sin{x} - e^{3/2}\\sin\\left(\\frac{3x}{2}\\right),\n\\] along with \\[\n\\quad X(0)Y(y)=0 \\quad \\text{and} \\quad X(2\\pi)Y(y)=0.\n\\] Now, this part is a little different than the heat and wave equations, since we must consider two cases: either \\(\\lambda&lt;0\\) or \\(\\lambda&gt;0\\).\n\nIf \\(\\lambda&lt;0\\), then we may set \\(\\lambda = -a^2\\), for some nonzero real number \\(a\\). Then the general solution to the \\(X\\)-equation is \\[\nX(x) = C_1 e^{ax},\n\\] while the general solution to the \\(Y\\)-equation is \\[\nY(y) = C_2 \\sin{ay} + C_3 \\cos{ay}.\n\\] But then the boundary condition \\(X(x)Y(0)=0\\) implies \\[\nC_1C_3 e^{ax} = \\sin{x}-\\sin\\left(\\frac{3x}{2}\\right)\n\\] for all \\(0\\leq x \\leq 2\\pi\\). But this is impossible, and hence we cannot have \\(\\lambda&lt;0\\).\nSo, we assume \\(\\lambda&gt;0\\). In this case, we set \\(\\lambda = a^2\\), for some nonzero real number \\(a\\). Then the general solution to the \\(X\\)-equation is \\[\nX(x) = C_1 \\sin{ax} + C_2 \\cos{ax},\n\\] while the general solution to the \\(Y\\)-equation is \\[\nY(y) = C_3 e^{ay}.\n\\] Then the boundary condition \\(X(0)Y(y)=0\\) implies that \\(C_2 = 0\\), so we must have \\[\nX(x) = C_1 \\sin{ax}.\n\\] The boundary condition \\(X(2\\pi)Y(y)=0\\) implies that \\(C_1 \\sin{2\\pi a} = 0\\), so we must have \\(a = n/2\\) for some integer \\(n\\). Therefore, for each \\(n=1,2,\\ldots\\), we get a solution \\[\nu_n(x,y) = \\sin\\left(\\frac{nx}{2}\\right) \\exp\\left( \\frac{ny}{2}  \\right)\n\\] to Laplace’s equation. By comparing these solutions to the boundary conditions, we determine that our desired solution is the superposition \\[\nu(x,y) = u_2(x,y) - u_3(x,y) = \\sin{x} \\exp{y} - \\sin\\left(\\frac{3x}{2}\\right) \\exp\\left( \\frac{3y}{2} \\right).\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/03ex-the-laplacian-and-laplaces-equation.html#exercise-1-computing-laplacians-from-derivatives",
    "href": "teaching/pde-sp-26/exercises/03ex-the-laplacian-and-laplaces-equation.html#exercise-1-computing-laplacians-from-derivatives",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "In \\(n\\) variables, recall that the Laplacian operator is defined as \\[\n\\nabla^2 =\\sum_{i=1}^n \\frac{\\partial^2 }{\\partial x_i^2}.\n\\]\nRecall also that if \\(u(x_1, \\dots, x_n, t)\\) is a time-dependent function, the Laplacian only acts on the spatial variables.\n\n\n\n\n\n\nCompute the Laplacians of the following functions, using the definition above.\n\n\\(u(x,y) = x^2 + y^2\\)\n\\(u(x,y,z) = 2x^2\\sin{y} + e^z\\)\n\\(u(x) = x^2e^x\\)\n\\(u(x,y,t) = x^2yt + y^2t + t^2\\cos{x}\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(4\\)\n\\(-2 x^2 \\sin {y}+4 \\sin {y}+e^z\\)\n\\(e^x x^2+4 e^x x+2 e^x\\)\n\\(-t^2 \\cos {x}+ 2 y t+2 t\\)"
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/03ex-the-laplacian-and-laplaces-equation.html#exercise-2-computing-laplacians-as-limits-of-average-values",
    "href": "teaching/pde-sp-26/exercises/03ex-the-laplacian-and-laplaces-equation.html#exercise-2-computing-laplacians-as-limits-of-average-values",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "Let \\(u(x,y)\\) be a function with continuous second-order partial derivatives. In class, we showed that the Laplacian may be computed as the limit\n\\[\n(\\nabla^2 u)(x_0,y_0) = \\lim_{r\\to 0^+}\\left( \\frac{4}{r^2}\\cdot \\frac{1}{2\\pi r} \\int_C \\big( u(x,y) - u(x_0,y_0)\\big) \\, ds\\right),\n\\]\nwhere \\(C\\) is a circle centered at \\((x_0,y_0)\\) with radius \\(r\\). This formula sheds some light on the intuitive idea that the Laplacian measures how the value of a function at a point compares to the average value around that point.\n\n\n\n\n\n\nSuppose \\(u(x,y) = \\sin(x^2+y^2)\\).\n\nCompute the Laplacian at \\((x_0,y_0) = (0,0)\\) using the limit formula above.\nCheck that your answer is correct by computing the Laplacian from the definition.\nUse your value for the Laplacian to estimate the average value of \\(u\\) around the point \\((0,0)\\) on a circle of radius \\(r=0.1\\). Compare this to the true average value.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nParametrize the circle \\(C\\) as \\(x=r\\cos{t}\\) and \\(y=r\\sin{t}\\) for \\(0\\leq t \\leq 2\\pi\\). Then \\(dx = -r\\sin{t}\\, dt\\) and \\(dy = r\\cos{t}\\, dt\\), so that \\[\nds = \\sqrt{ dx^2 + dy^2} = \\sqrt{r^2\\sin^2{t}+ r^2\\cos^2{t}}\\, dt = r\\, dt\n\\] We then have \\[\n\\int_C \\big( u(x,y) - u(0,0)\\big) \\, ds = \\int_0^{2\\pi} r\\sin(r^2) \\, dt = 2\\pi r \\sin(r^2).\n\\] Inserting this into the limit formula gives \\[\n\\begin{align*}\n(\\nabla^2 u)(0,0) &= \\lim_{r\\to 0^+} \\frac{4}{r^2} \\cdot \\frac{1}{2\\pi r} \\cdot 2\\pi r \\sin(r^2) \\\\\n&= \\lim_{r\\to 0^+} \\frac{4\\sin(r^2)}{r^2}  \\\\\n&= \\lim_{r\\to 0^+} \\frac{ 8r\\cos(r^2)}{2r} \\\\\n&= 4.\n\\end{align*}\n\\]\nWe first compute: \\[\n\\begin{align*}\n\\frac{\\partial u}{\\partial x} &= 2x \\cos(x^2+y^2), \\\\\n\\frac{\\partial u}{\\partial y} &= 2y \\cos(x^2+y^2).\n\\end{align*}\n\\] Then: \\[\n\\begin{align*}\n\\frac{\\partial^2 u}{\\partial x^2} &= 2\\cos(x^2+y^2) - 4x^2\\sin(x^2+y^2), \\\\\n\\frac{\\partial^2 u}{\\partial y^2} &= 2\\cos(x^2+y^2) - 4y^2\\sin(x^2+y^2).\n\\end{align*}\n\\] Hence: \\[\n(\\nabla^2 u)(0,0) = \\frac{\\partial^2 u}{\\partial x^2}(0,0) + \\frac{\\partial^2 u}{\\partial y^2}(0,0) = 2 + 2 = 4.\n\\]\nAs long as \\(r\\) is small, the limit formula for the Laplacian may be rearranged to give \\[\n\\frac{r^2}{4} \\cdot (\\nabla^2 u)(0,0) \\approx \\frac{1}{2\\pi r} \\int_C u(x,y) \\, ds.\n\\] Note that \\(u(0,0)=0\\), so the integral on the right is the true average value of \\(u\\) around the circle of radius \\(r\\) centered at \\((0,0)\\). But from our solution to (a), we know \\[\n\\frac{1}{2\\pi r} \\int_C u(x,y) \\, ds = \\sin(r^2).\n\\] On the other hand, we have \\[\n\\frac{r^2}{4} \\cdot (\\nabla^2 u)(0,0) = r^2.\n\\] Note that \\[\nr^2 = 0.01 \\quad \\text{and} \\quad \\sin(r^2) \\approx 0.0099998\n\\] when \\(r=0.1\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/03ex-the-laplacian-and-laplaces-equation.html#exercise-3-solving-laplaces-equation",
    "href": "teaching/pde-sp-26/exercises/03ex-the-laplacian-and-laplaces-equation.html#exercise-3-solving-laplaces-equation",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "Consider Laplace’s equation for functions \\(u(x,y)\\) on the rectangle\n\\[\n0 \\leq x \\leq 2\\pi, \\quad 0 \\leq y \\leq 1,\n\\]\nin \\(\\mathbb{R}^2\\). Suppose we impose the boundary conditions\n\\[\nu(x,0) = \\sin{x}-\\sin\\left(\\frac{3x}{2}\\right) \\quad \\text{and} \\quad u(x,1) = e\\sin{x} - e^{3/2}\\sin\\left(\\frac{3x}{2}\\right)\n\\]\nalong the bottom and top edges of the rectangle (draw the rectangle in the \\(xy\\)-plane), and\n\\[\nu(0,y) = 0, \\quad u(2\\pi,y) = 0\n\\]\nalong the left and right edges of the rectangle.\n\nUsing separation of variables, solve Laplace’s equation.\nPlot your solution using Desmos. Be sure to adjust the \\(x\\)- and \\(y\\)-axes so that you only view your solution over the rectangle \\(0\\leq x \\leq 2\\pi\\) and \\(0\\leq y \\leq 1\\). We discussed in class that harmonic functions are the optimally “boring” functions that are consistent with the boundary conditions. Does your plot support this intuition? Does your solution exhibit any unexpected features? Any extreme values that are not on the boundary?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nSuppose that \\(u\\) factors as \\[\nu(x,y) = X(x)Y(y).\n\\] Substituting this into Laplace’s equation gives \\[\nX''(x)Y(y) + X(x)Y''(y) = 0.\n\\] Rearranging and dividing both sides by \\(X(x)Y(y)\\) gives \\[\n\\frac{X''(x)}{X(x)} =- \\frac{Y''(y)}{Y(y)}.\n\\] Since the left-hand side is a function only of \\(x\\), and the right-hand side is a function only of \\(y\\), each side must be equal to a nonzero real constant, say \\(-\\lambda\\). Then \\[\n\\frac{X''(x)}{X(x)} = -\\lambda \\quad \\text{and} \\quad \\frac{Y''(y)}{Y(y)} = \\lambda,\n\\] which gives us the system of two ODEs \\[\nX''(x) + \\lambda X(x) = 0, \\quad Y''(y) - \\lambda Y(y) = 0.\n\\] The boundary conditions translate to \\[\nX(x)Y(0) = \\sin{x}-\\sin\\left(\\frac{3x}{2}\\right) \\quad \\text{and} \\quad X(x)Y(1)=e\\sin{x} - e^{3/2}\\sin\\left(\\frac{3x}{2}\\right),\n\\] along with \\[\n\\quad X(0)Y(y)=0 \\quad \\text{and} \\quad X(2\\pi)Y(y)=0.\n\\] Now, this part is a little different than the heat and wave equations, since we must consider two cases: either \\(\\lambda&lt;0\\) or \\(\\lambda&gt;0\\).\n\nIf \\(\\lambda&lt;0\\), then we may set \\(\\lambda = -a^2\\), for some nonzero real number \\(a\\). Then the general solution to the \\(X\\)-equation is \\[\nX(x) = C_1 e^{ax},\n\\] while the general solution to the \\(Y\\)-equation is \\[\nY(y) = C_2 \\sin{ay} + C_3 \\cos{ay}.\n\\] But then the boundary condition \\(X(x)Y(0)=0\\) implies \\[\nC_1C_3 e^{ax} = \\sin{x}-\\sin\\left(\\frac{3x}{2}\\right)\n\\] for all \\(0\\leq x \\leq 2\\pi\\). But this is impossible, and hence we cannot have \\(\\lambda&lt;0\\).\nSo, we assume \\(\\lambda&gt;0\\). In this case, we set \\(\\lambda = a^2\\), for some nonzero real number \\(a\\). Then the general solution to the \\(X\\)-equation is \\[\nX(x) = C_1 \\sin{ax} + C_2 \\cos{ax},\n\\] while the general solution to the \\(Y\\)-equation is \\[\nY(y) = C_3 e^{ay}.\n\\] Then the boundary condition \\(X(0)Y(y)=0\\) implies that \\(C_2 = 0\\), so we must have \\[\nX(x) = C_1 \\sin{ax}.\n\\] The boundary condition \\(X(2\\pi)Y(y)=0\\) implies that \\(C_1 \\sin{2\\pi a} = 0\\), so we must have \\(a = n/2\\) for some integer \\(n\\). Therefore, for each \\(n=1,2,\\ldots\\), we get a solution \\[\nu_n(x,y) = \\sin\\left(\\frac{nx}{2}\\right) \\exp\\left( \\frac{ny}{2}  \\right)\n\\] to Laplace’s equation. By comparing these solutions to the boundary conditions, we determine that our desired solution is the superposition \\[\nu(x,y) = u_2(x,y) - u_3(x,y) = \\sin{x} \\exp{y} - \\sin\\left(\\frac{3x}{2}\\right) \\exp\\left( \\frac{3y}{2} \\right).\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/06ex-a-first-look-at-fourier-series-part-2.html",
    "href": "teaching/pde-sp-26/exercises/06ex-a-first-look-at-fourier-series-part-2.html",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "This exercise refers to the Fourier series listed in rows 1-20 of the file here, the same file as in Exercise 1 of the previous section.\n\n\n\n\n\n\n\nVerify, by hand, that the Fourier series for the function in row 7 of the table in the file above is the one shown.\nVerify, by hand, that the Fourier series for the function in row 16 of the table in the file above is the one shown.\n\nFor extra fun, open up an instance of Desmos, and see if you can manage to plot a few partial sums of the Fourier series.\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nThe solutions are given in the PDF itself! Make sure your calculations match the Fourier series shown in the table.\n\n\n\n\n\n\nWe saw the following “\\(ab\\)-version” of Bessel’s inequality in the slides: If \\(f:\\mathbb{R} \\to \\mathbb{C}\\) is a \\(2\\pi\\)-periodic function, integrable on \\([-\\pi, \\pi]\\), and if \\(a_n\\) and \\(b_n\\) are its Fourier coefficients, then \\[\n\\frac{|a_0|^2}{4} + \\frac{1}{2}\\sum_{n=1}^\\infty \\left( |a_n|^2 + |b_n|^2 \\right) \\leq \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi |f(\\theta)|^2 \\, d\\theta.\n\\]\n\n\n\n\n\n\nIn this exercise, you will derive the \\(ab\\)-version of Bessel’s inequality from the \\(c\\)-version of Bessel’s inequality that we discussed and proved in class.\n\nFirst, show that \\[\n|a_n|^2 + |b_n|^2 = 2\\left(|c_n|^2 + |c_{-n}|^2\\right), \\quad n\\geq 1.\n\\] You’ll need to use the conversion formulas between \\(a_n\\), \\(b_n\\) and \\(c_n\\), that we discussed in class.\nThen, derive the \\(ab\\)-version from the \\(c\\)-version.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nWe know that \\(a_n = c_n + c_{-n}\\) and \\(b_n = i(c_n - c_{-n})\\). So \\[\n\\begin{align*}\n|a_n|^2 + |b_n|^2 &= a_n \\overline{a_n} + b_n \\overline{b_n} \\\\\n&= (c_n + c_{-n})(\\overline{c_n} + \\overline{c_{-n}}) + i(c_n - c_{-n})(-i)(\\overline{c_n} - \\overline{c_{-n}}) \\\\\n&= 2c_n \\overline{c_n} + 2c_{-n} \\overline{c_{-n}} \\\\\n&= 2\\left(|c_n|^2 + |c_{-n}|^2\\right).\n\\end{align*}\n\\]\nWe have that \\[\n\\sum_{n=-\\infty}^\\infty |c_n|^2 = |c_0|^2 + \\sum_{n=1}^\\infty \\left( |c_n|^2 + |c_{-n}|^2 \\right) = \\frac{|a_0|^2}{4} + \\frac{1}{2}\\sum_{n=1}^\\infty \\left( |a_n|^2 + |b_n|^2 \\right),\n\\] where we used (a) and the fact that \\(a_0 = 2c_0\\). Now apply the \\(c\\)-version of Bessel’s inequality to get the \\(ab\\)-version: \\[\n\\frac{|a_0|^2}{4} + \\frac{1}{2}\\sum_{n=1}^\\infty \\left( |a_n|^2 + |b_n|^2 \\right) = \\sum_{n=-\\infty}^\\infty |c_n|^2 \\leq \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi |f(\\theta)|^2 \\, d\\theta.\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/06ex-a-first-look-at-fourier-series-part-2.html#exercise-1-more-practice-computing-fourier-series",
    "href": "teaching/pde-sp-26/exercises/06ex-a-first-look-at-fourier-series-part-2.html#exercise-1-more-practice-computing-fourier-series",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "This exercise refers to the Fourier series listed in rows 1-20 of the file here, the same file as in Exercise 1 of the previous section.\n\n\n\n\n\n\n\nVerify, by hand, that the Fourier series for the function in row 7 of the table in the file above is the one shown.\nVerify, by hand, that the Fourier series for the function in row 16 of the table in the file above is the one shown.\n\nFor extra fun, open up an instance of Desmos, and see if you can manage to plot a few partial sums of the Fourier series.\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nThe solutions are given in the PDF itself! Make sure your calculations match the Fourier series shown in the table."
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/06ex-a-first-look-at-fourier-series-part-2.html#exercise-2-bessels-inequality-ab-version",
    "href": "teaching/pde-sp-26/exercises/06ex-a-first-look-at-fourier-series-part-2.html#exercise-2-bessels-inequality-ab-version",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "We saw the following “\\(ab\\)-version” of Bessel’s inequality in the slides: If \\(f:\\mathbb{R} \\to \\mathbb{C}\\) is a \\(2\\pi\\)-periodic function, integrable on \\([-\\pi, \\pi]\\), and if \\(a_n\\) and \\(b_n\\) are its Fourier coefficients, then \\[\n\\frac{|a_0|^2}{4} + \\frac{1}{2}\\sum_{n=1}^\\infty \\left( |a_n|^2 + |b_n|^2 \\right) \\leq \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi |f(\\theta)|^2 \\, d\\theta.\n\\]\n\n\n\n\n\n\nIn this exercise, you will derive the \\(ab\\)-version of Bessel’s inequality from the \\(c\\)-version of Bessel’s inequality that we discussed and proved in class.\n\nFirst, show that \\[\n|a_n|^2 + |b_n|^2 = 2\\left(|c_n|^2 + |c_{-n}|^2\\right), \\quad n\\geq 1.\n\\] You’ll need to use the conversion formulas between \\(a_n\\), \\(b_n\\) and \\(c_n\\), that we discussed in class.\nThen, derive the \\(ab\\)-version from the \\(c\\)-version.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nWe know that \\(a_n = c_n + c_{-n}\\) and \\(b_n = i(c_n - c_{-n})\\). So \\[\n\\begin{align*}\n|a_n|^2 + |b_n|^2 &= a_n \\overline{a_n} + b_n \\overline{b_n} \\\\\n&= (c_n + c_{-n})(\\overline{c_n} + \\overline{c_{-n}}) + i(c_n - c_{-n})(-i)(\\overline{c_n} - \\overline{c_{-n}}) \\\\\n&= 2c_n \\overline{c_n} + 2c_{-n} \\overline{c_{-n}} \\\\\n&= 2\\left(|c_n|^2 + |c_{-n}|^2\\right).\n\\end{align*}\n\\]\nWe have that \\[\n\\sum_{n=-\\infty}^\\infty |c_n|^2 = |c_0|^2 + \\sum_{n=1}^\\infty \\left( |c_n|^2 + |c_{-n}|^2 \\right) = \\frac{|a_0|^2}{4} + \\frac{1}{2}\\sum_{n=1}^\\infty \\left( |a_n|^2 + |b_n|^2 \\right),\n\\] where we used (a) and the fact that \\(a_0 = 2c_0\\). Now apply the \\(c\\)-version of Bessel’s inequality to get the \\(ab\\)-version: \\[\n\\frac{|a_0|^2}{4} + \\frac{1}{2}\\sum_{n=1}^\\infty \\left( |a_n|^2 + |b_n|^2 \\right) = \\sum_{n=-\\infty}^\\infty |c_n|^2 \\leq \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi |f(\\theta)|^2 \\, d\\theta.\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/06-a-first-look-at-fourier-series-part-2.html#exercise-1-computing-another-fourier-series",
    "href": "teaching/pde-sp-26/slides/06-a-first-look-at-fourier-series-part-2.html#exercise-1-computing-another-fourier-series",
    "title": "06 A first look at Fourier series, part 2",
    "section": "Exercise 1: Computing another Fourier series",
    "text": "Exercise 1: Computing another Fourier series\n\n\n\nDefine the function \\(f(\\theta) = \\theta\\) for \\(\\theta \\in [-\\pi, \\pi]\\), and extend it to a function on \\(\\mathbb{R}\\) with period \\(2\\pi\\). Compute the Fourier coefficients and series of \\(f\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/06-a-first-look-at-fourier-series-part-2.html#bessels-inequality-statement",
    "href": "teaching/pde-sp-26/slides/06-a-first-look-at-fourier-series-part-2.html#bessels-inequality-statement",
    "title": "06 A first look at Fourier series, part 2",
    "section": "Bessel’s inequality: statement",
    "text": "Bessel’s inequality: statement\n\n\n\n\nTheorem (Bessel’s inequality — \\(c\\)-version).\n\n\nIf \\(f:\\mathbb{R} \\to \\mathbb{C}\\) is a \\(2\\pi\\)-periodic function, integrable on \\([-\\pi, \\pi]\\), and if \\(c_n\\) are its Fourier coefficients, then\n\\[\n\\sum_{n=-\\infty}^\\infty |c_n|^2 \\leq \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi |f(\\theta)|^2 \\, d\\theta.\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/06-a-first-look-at-fourier-series-part-2.html#bessels-inequality-context",
    "href": "teaching/pde-sp-26/slides/06-a-first-look-at-fourier-series-part-2.html#bessels-inequality-context",
    "title": "06 A first look at Fourier series, part 2",
    "section": "Bessel’s inequality: context",
    "text": "Bessel’s inequality: context\n\nFor the moment, consider a familiar column vector, say in \\(\\mathbf{x} \\in \\mathbb{C}^3\\) for simplicity: \\[\n\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}.\n\\]\nIf we consider the standard basis vectors \\[\n\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad\n\\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad\n\\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix},\n\\] then can consider the three inner products (dot products): \\[\n\\langle \\mathbf{x}, \\mathbf{e}_1 \\rangle = x_1, \\quad\n\\langle \\mathbf{x}, \\mathbf{e}_2 \\rangle = x_2, \\quad\n\\langle \\mathbf{x}, \\mathbf{e}_3 \\rangle = x_3.\n\\]\nThen we obviously have \\[\n\\sum_{n=1}^3 |\\langle \\mathbf{x}, \\mathbf{e}_n \\rangle|^2 = |x_1|^2 + |x_2|^2 + |x_3|^2 = \\|\\mathbf{x}\\|^2.\n\\]\nIn particular, we have \\[\n\\sum_{n=1}^3 |x_n|^2 = \\sum_{n=1}^3 |\\langle \\mathbf{x}, \\mathbf{e}_n \\rangle|^2 \\leq \\|\\mathbf{x}\\|^2.\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/06-a-first-look-at-fourier-series-part-2.html#bessels-inequality-more-context",
    "href": "teaching/pde-sp-26/slides/06-a-first-look-at-fourier-series-part-2.html#bessels-inequality-more-context",
    "title": "06 A first look at Fourier series, part 2",
    "section": "Bessel’s inequality: more context",
    "text": "Bessel’s inequality: more context\n\nNow, consider two functions \\(f,g: \\mathbb{R} \\to \\mathbb{C}\\) that are \\(2\\pi\\)-periodic and integrable on \\([-\\pi, \\pi]\\). Suppose we define an inner product on such functions by \\[\n\\langle f, g \\rangle = \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi f(\\theta) \\overline{g(\\theta)} \\, d\\theta.\n\\]\nThen, in complete analogy with the case of column vectors, it is natural to define \\[\n\\|f\\|^2 = \\langle f, f \\rangle = \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi |f(\\theta)|^2 \\, d\\theta.\n\\]\nNotice also that \\[\n\\langle f, e^{in\\theta} \\rangle = \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi f(\\theta) e^{-in\\theta} \\, d\\theta = c_n,\n\\] for each integer \\(n\\).\nIn other words, the Fourier coefficients of \\(f\\) are precisely the inner products of \\(f\\) with the complex exponentials \\(e^{in\\theta}\\).\nFrom the previous slide, we have \\[\n\\sum_{n=1}^3 |x_n|^2 =  \\sum_{n=1}^3 |\\langle \\mathbf{x}, \\mathbf{e}_n \\rangle|^2 \\leq \\|\\mathbf{x}\\|^2.\n\\]\nArguing by analogy, we might expect that \\[\n\\sum_{n=-\\infty}^\\infty |c_n|^2 = \\sum_{n=-\\infty}^\\infty |\\langle f, e^{in\\theta} \\rangle|^2 \\leq \\|f\\|^2 = \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi |f(\\theta)|^2 \\, d\\theta,\n\\] which is precisely Bessel’s inequality.\nIn fact, in the case of column vectors, we have an equality, which suggests by analogy that (maybe?) we have an equality in the case of functions as well. This is indeed the case!"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/06-a-first-look-at-fourier-series-part-2.html#bessels-inequality-proof",
    "href": "teaching/pde-sp-26/slides/06-a-first-look-at-fourier-series-part-2.html#bessels-inequality-proof",
    "title": "06 A first look at Fourier series, part 2",
    "section": "Bessel’s inequality: proof",
    "text": "Bessel’s inequality: proof\n\n\n\n\nTheorem (Bessel’s inequality — \\(c\\)-version).\n\n\nIf \\(f:\\mathbb{R} \\to \\mathbb{C}\\) is a \\(2\\pi\\)-periodic function, integrable on \\([-\\pi, \\pi]\\), and if \\(c_n\\) are its Fourier coefficients, then\n\\[\n\\sum_{n=-\\infty}^\\infty |c_n|^2 \\leq \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi |f(\\theta)|^2 \\, d\\theta.\n\\]\n\n\n\n\n\nBegin with the basic observation that \\(|z|^2 = z \\overline{z}\\) for any complex number \\(z\\).\nApplying this to \\(z = f(\\theta) - \\sum_{n=-N}^N c_n e^{in\\theta}\\), we have \\[\n\\left| f(\\theta) - \\sum_{n=-N}^N c_n e^{in\\theta} \\right|^2 = \\left( f(\\theta) - \\sum_{n=-N}^N c_n e^{in\\theta} \\right) \\overline{\\left( f(\\theta) - \\sum_{n=-N}^N c_n e^{in\\theta} \\right)} = \\left( f(\\theta) - \\sum_{n=-N}^N c_n e^{in\\theta} \\right) \\left( \\overline{f(\\theta)} - \\sum_{n=-N}^N \\overline{c_n} e^{-in\\theta}\\right).\n\\]\nExpanding the right-hand side, we have \\[\n\\left| f(\\theta) - \\sum_{n=-N}^N c_n e^{in\\theta} \\right|^2 = |f(\\theta)|^2 - \\sum_{n=-N}^N c_n \\overline{f(\\theta)} e^{in\\theta}  - \\sum_{n=-N}^N \\overline{c_n} f(\\theta) e^{-in\\theta}  + \\sum_{n=-N}^N \\sum_{m=-N}^N c_n \\overline{c_m} e^{i(n-m)\\theta}.\n\\]\nDivide both sides by \\(2\\pi\\), integrate from \\(-\\pi\\) to \\(\\pi\\), and recall that: \\[\n\\frac{1}{2\\pi} \\int_{-\\pi}^\\pi f(\\theta) e^{-in\\theta} \\, d\\theta = c_n, \\quad \\text{and} \\quad \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi e^{i(n-m)\\theta} \\, d\\theta = \\begin{cases}\n1 & : n=m, \\\\\n0 & : n \\neq m.\n\\end{cases}\n\\]\nWe get \\[\n\\begin{align*}\n\\frac{1}{2\\pi} \\int_{-\\pi}^\\pi \\left| f(\\theta) - \\sum_{n=-N}^N c_n e^{in\\theta} \\right|^2 \\, d\\theta &= \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi |f(\\theta)|^2 \\, d\\theta - \\sum_{n=-N}^N \\left( c_n \\overline{c_n} + \\overline{c_n} c_n \\right) + \\sum_{n=-N}^N c_n \\overline{c_n} \\\\\n&= \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi |f(\\theta)|^2 \\, d\\theta - \\sum_{n=-N}^N |c_n|^2.\n\\end{align*}\n\\]\nBut the left-hand side is non-negative, so we have \\[\n\\sum_{n=-N}^N |c_n|^2 \\leq \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi |f(\\theta)|^2 \\, d\\theta,\n\\] for each \\(N\\). Now let \\(N\\to \\infty\\). Q.E.D."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/06-a-first-look-at-fourier-series-part-2.html#bessels-inequality-ab-version",
    "href": "teaching/pde-sp-26/slides/06-a-first-look-at-fourier-series-part-2.html#bessels-inequality-ab-version",
    "title": "06 A first look at Fourier series, part 2",
    "section": "Bessel’s inequality: “\\(ab\\)-version”",
    "text": "Bessel’s inequality: “\\(ab\\)-version”\n\n\n\n\nTheorem (Bessel’s inequality — \\(ab\\)-version).\n\n\nIf \\(f:\\mathbb{R} \\to \\mathbb{C}\\) is a \\(2\\pi\\)-periodic function, integrable on \\([-\\pi, \\pi]\\), and if \\(a_n\\) and \\(b_n\\) are its Fourier coefficients, then \\[\n\\frac{|a_0|^2}{4} + \\frac{1}{2}\\sum_{n=1}^\\infty \\left( |a_n|^2 + |b_n|^2 \\right) \\leq \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi |f(\\theta)|^2 \\, d\\theta.\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/06-a-first-look-at-fourier-series-part-2.html#riemann-lebesgue-lemma",
    "href": "teaching/pde-sp-26/slides/06-a-first-look-at-fourier-series-part-2.html#riemann-lebesgue-lemma",
    "title": "06 A first look at Fourier series, part 2",
    "section": "Riemann-Lebesgue lemma",
    "text": "Riemann-Lebesgue lemma\n\n\n\n\nTheorem (Riemann-Lebesgue lemma).\n\n\nIf \\(f:\\mathbb{R} \\to \\mathbb{C}\\) is a \\(2\\pi\\)-periodic function, integrable on \\([-\\pi, \\pi]\\), and if \\(a_n\\), \\(b_n\\), and \\(c_n\\) are its Fourier coefficients, then \\[\n\\lim_{n\\to \\infty} a_n = \\lim_{n\\to \\infty} b_n = \\lim_{n\\to \\pm \\infty} c_n = 0.\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/05-a-first-look-at-fourier-series-part-1.html#the-fundamental-question",
    "href": "teaching/pde-sp-26/slides/05-a-first-look-at-fourier-series-part-1.html#the-fundamental-question",
    "title": "05 A first look at Fourier series, part 1",
    "section": "The fundamental question",
    "text": "The fundamental question\n\nOne of the fundamental questions raised when solving an initial-value problem via separation of variables is:\n\n\n\n\n\nThe Fourier question\n\n\nGiven a function \\(f:\\mathbb{R} \\to \\mathbb{R}\\), when can we write it as a series of the form\n\\[\nf(\\theta) = \\frac{a_0}{2} + \\sum_{n=1}^\\infty \\left( a_n \\cos{n\\theta} + b_n \\sin{n\\theta}\\right)?\n\\]\n\nWhen does the series on the right converge?\nIf it does converge, does it converge to \\(f\\)?\nWhat are the coefficients \\(a_n\\) and \\(b_n\\)?\n\n\n\n\n\n\nNote we use the variable \\(\\theta\\), rather than \\(x\\). No harm here.\nIf the answer to (2) is always, then the function \\(f\\) had better be periodic with period \\(2\\pi\\), so we will focus always on these types of functions."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/05-a-first-look-at-fourier-series-part-1.html#exericse-1-bringing-in-complex-numbers",
    "href": "teaching/pde-sp-26/slides/05-a-first-look-at-fourier-series-part-1.html#exericse-1-bringing-in-complex-numbers",
    "title": "05 A first look at Fourier series, part 1",
    "section": "Exericse 1: Bringing in complex numbers",
    "text": "Exericse 1: Bringing in complex numbers\n\n\n\nRecall that from Euler’s formula \\(e^{i\\theta} = \\cos{\\theta} + i\\sin{\\theta}\\), we can express the sine and cosine functions in terms of complex exponentials as follows:\n\\[\n\\cos{\\theta} = \\frac{e^{i\\theta} + e^{-i\\theta}}{2}, \\quad \\sin{\\theta} = \\frac{e^{i\\theta} - e^{-i\\theta}}{2i}.\n\\]\n\nExpress the series \\[\n\\frac{a_0}{2} + \\sum_{n=1}^\\infty \\left( a_n \\cos{n\\theta} + b_n \\sin{n\\theta}\\right)\n\\] in terms of complex exponentials with coefficients \\(c_n\\) for \\(n\\in \\mathbb{Z}\\).\nConversely, starting from the series of complex exponentials that you found in part (1), express it back in terms of sines and cosines with coefficients \\(a_n\\) and \\(b_n\\) for \\(n\\geq 0\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/05-a-first-look-at-fourier-series-part-1.html#deriving-a-formula-for-c_n",
    "href": "teaching/pde-sp-26/slides/05-a-first-look-at-fourier-series-part-1.html#deriving-a-formula-for-c_n",
    "title": "05 A first look at Fourier series, part 1",
    "section": "Deriving a formula for \\(c_n\\)",
    "text": "Deriving a formula for \\(c_n\\)\n\nNow, suppose we take for granted that \\[\nf(\\theta) = \\sum_{n=-\\infty}^\\infty c_n e^{in\\theta}, \\tag{1}\n\\] and that it is permissible to integrate term-by-term. (These are assumptions, not claims of fact!)\nHow might we find a formula for the coefficients \\(c_n\\)?\nIntegrate both sides of (1) against \\(e^{-ik\\theta}\\) for some fixed integer \\(k\\), from \\(-\\pi\\) to \\(\\pi\\): \\[\n\\int_{-\\pi}^\\pi f(\\theta) e^{-ik\\theta} \\, d\\theta = \\sum_{n=-\\infty}^\\infty c_n \\int_{-\\pi}^\\pi  e^{i(n-k)\\theta} \\, d\\theta.\n\\]\nNow, compute the integrals on the right-hand side: \\[\n\\int_{-\\pi}^\\pi  e^{i(n-k)\\theta} \\, d\\theta = \\begin{cases}\n2\\pi & : n=k, \\\\\n0 & : n \\neq k.\n\\end{cases}\n\\]\nTherefore, we have: \\[\n\\int_{-\\pi}^\\pi f(\\theta) e^{-ik\\theta} \\, d\\theta = 2\\pi c_k,\n\\] which is the same, after relabelling, as the formula \\[\nc_n = \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi f(\\theta) e^{-in\\theta} \\, d\\theta.\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/05-a-first-look-at-fourier-series-part-1.html#deriving-formulas-for-a_n-and-b_n",
    "href": "teaching/pde-sp-26/slides/05-a-first-look-at-fourier-series-part-1.html#deriving-formulas-for-a_n-and-b_n",
    "title": "05 A first look at Fourier series, part 1",
    "section": "Deriving formulas for \\(a_n\\) and \\(b_n\\)",
    "text": "Deriving formulas for \\(a_n\\) and \\(b_n\\)\n\nFrom Exercise 1, we know that \\[\na_0 = 2c_0, \\quad a_n = c_n + c_{-n}, \\quad b_n = i(c_n - c_{-n})\n\\] for \\(n\\geq 1\\).\nTherefore, we can express \\(a_n\\) and \\(b_n\\) in terms of integrals of \\(f\\) as follows: \\[\na_0 = \\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(\\theta) \\, d\\theta, \\quad a_n = \\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(\\theta) \\cos{n\\theta} \\, d\\theta, \\quad b_n = \\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(\\theta) \\sin{n\\theta} \\, d\\theta,\n\\] for \\(n\\geq 1\\).\n\n\n\n\n\nDefinition\n\n\nLet \\(f:\\mathbb{R} \\to \\mathbb{R}\\) be an integrable function with period \\(2\\pi\\).\n\nThe numbers \\(c_n\\) for \\(n\\in \\mathbb{Z}\\), \\(a_n\\) for \\(n\\geq 0\\), and \\(b_n\\) for \\(n\\geq 1\\), defined by \\[\nc_n = \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi f(\\theta) e^{-in\\theta} \\, d\\theta, \\quad a_n = \\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(\\theta) \\cos{n\\theta} \\, d\\theta, \\quad b_n = \\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(\\theta) \\sin{n\\theta} \\, d\\theta,\n\\] are called the Fourier coefficients of \\(f\\).\nThe series \\[\n\\sum_{n=-\\infty}^\\infty c_n e^{in\\theta} \\quad \\text{or} \\quad \\frac{a_0}{2} + \\sum_{n=1}^\\infty \\left( a_n \\cos{n\\theta} + b_n \\sin{n\\theta}\\right)\n\\] is called the Fourier series of \\(f\\).\n\n\n\n\n\n\n\n\n\nWARNING!!!\n\n\nNothing has been claimed about the convergence of the Fourier series!"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/05-a-first-look-at-fourier-series-part-1.html#exercise-2-fourier-coefficients-of-even-and-odd-functions",
    "href": "teaching/pde-sp-26/slides/05-a-first-look-at-fourier-series-part-1.html#exercise-2-fourier-coefficients-of-even-and-odd-functions",
    "title": "05 A first look at Fourier series, part 1",
    "section": "Exercise 2: Fourier coefficients of even and odd functions",
    "text": "Exercise 2: Fourier coefficients of even and odd functions\n\n\n\nLet \\(f:\\mathbb{R} \\to \\mathbb{R}\\) be an integrable function with period \\(2\\pi\\).\n\nIf \\(f\\) is even, show that \\(b_n=0\\) for all \\(n\\geq 1\\), and that \\[\na_n = \\frac{2}{\\pi} \\int_0^\\pi f(\\theta) \\cos{n\\theta} \\, d\\theta.\n\\]\nIf \\(f\\) is odd, show that \\(a_n=0\\) for all \\(n\\geq 0\\), and that \\[\nb_n = \\frac{2}{\\pi} \\int_0^\\pi f(\\theta) \\sin{n\\theta} \\, d\\theta.\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/05-a-first-look-at-fourier-series-part-1.html#exercise-3-computing-a-fourier-series",
    "href": "teaching/pde-sp-26/slides/05-a-first-look-at-fourier-series-part-1.html#exercise-3-computing-a-fourier-series",
    "title": "05 A first look at Fourier series, part 1",
    "section": "Exercise 3: Computing a Fourier series",
    "text": "Exercise 3: Computing a Fourier series\n\n\n\nDefine the function \\(f(\\theta) = |\\theta|\\) for \\(\\theta \\in [-\\pi, \\pi]\\), and extend it to a function on \\(\\mathbb{R}\\) with period \\(2\\pi\\). Compute the Fourier coefficients and series of \\(f\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#what-are-equations-what-do-we-do-with-them",
    "href": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#what-are-equations-what-do-we-do-with-them",
    "title": "01 An introduction to PDEs",
    "section": "What are equations? What do we do with them?",
    "text": "What are equations? What do we do with them?\n\nWe use equations to describe relationships between quantities.\n\nOften, some quantities are known, while others are unknown and need to be determined.\nWe need to “solve for” the unknown quantities in terms of the known ones.\n\n\n\n\n\n\nEquations with numbers and algebraic operations\n\n\n\nFor example, in the equation \\[\n2x + 3 = 9,\n\\] we know the numbers \\(2\\), \\(3\\), and \\(9\\), the algebraic operations connecting them, and we want to solve for the unknown number \\(x\\).\nThe answer is easy: \\(x = 3\\).\n\n\n\n\n\n\n\n\n\nEquations with functions and algebraic operations\n\n\n\nFor example, in the equation \\[\n2f(x) + 4 = 10x^2,\n\\] we know the numbers \\(2\\), \\(4\\), and the function \\(10x^2\\), the algebraic operations connecting them, and we want to solve for the unknown function \\(f(x)\\).\nThe answer is also easy: \\(f(x) = 5x^2 - 2\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#beyond-algebraic-equations",
    "href": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#beyond-algebraic-equations",
    "title": "01 An introduction to PDEs",
    "section": "Beyond “algebraic” equations",
    "text": "Beyond “algebraic” equations\n\n\n\n\nEquations with functions and differential operations\n\n\n\nFor example, in the equation \\[\n\\frac{dy}{dx} = 2y,\n\\] the function \\(y(x)\\) is unknown. It is embedded in an equation that involves an algebraic operation (multiplication by \\(2\\)) and a differential operation (taking the derivative with respect to \\(x\\)).\nThe answer is a bit less easy: \\(y(x) = Ce^{2x}\\), where \\(C\\) is an arbitrary constant determined by initial (or boundary) conditions.\n\n\n\n\n\n\nSuch equations are called differential equations because they involve differential operations.\nThis one happens to be an ordinary differential equation (ODE) because the unknown function \\(y\\) depends on a single variable \\(x\\).\n\nA synonym for “ordinary” (in this context) is “single-variable.”\n\nODEs appear everywhere in the real world.\n\n\n\n\n\nExample: Newton’s second law of motion\n\n\nThis equation says that \\[\nF = m \\frac{d^2x}{dt^2},\n\\]\nwhere \\(x(t)\\) is the position of an object as a function of time \\(t\\), \\(m\\) is its mass, and \\(F\\) is the force acting on it."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#the-central-definition",
    "href": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#the-central-definition",
    "title": "01 An introduction to PDEs",
    "section": "The central definition",
    "text": "The central definition\n\n\n\n\nDefinition\n\n\nA partial differential equation (PDE) is a differential equation that involves an unknown function of multiple variables and its partial derivatives.\n\n\n\n\n\nA synonym for “partial” (in this context) is “multi-variable.”"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#the-wave-equation-in-1-dimension",
    "href": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#the-wave-equation-in-1-dimension",
    "title": "01 An introduction to PDEs",
    "section": "The wave equation in 1 dimension",
    "text": "The wave equation in 1 dimension\n\n\n\n\nExample: the \\(1\\)-dimensional wave equation\n\n\nThis equation says that\n\\[\n\\frac{\\partial^2 u}{\\partial t^2} = a^2 \\frac{\\partial^2 u}{\\partial x^2},\n\\]\nwhere \\(u(x,t)\\) is a two-variable function and \\(a\\) is a known constant. It models wave-like phenomena, such as vibrations of a string, sound waves, electromagnetic waves, etc.\n\n\\(x\\) typically represents a spatial variable (e.g., position along a string).\n\\(t\\) typically represents time.\n\\(u(x,t)\\) represents the displacement of the wave at position \\(x\\) and time \\(t\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#the-wave-equation-in-2-dimensions",
    "href": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#the-wave-equation-in-2-dimensions",
    "title": "01 An introduction to PDEs",
    "section": "The wave equation in 2 dimensions",
    "text": "The wave equation in 2 dimensions\n\n\n\n\nExample: the \\(2\\)-dimensional wave equation\n\n\nThis equation says that\n\\[\n\\frac{\\partial^2 u}{\\partial t^2} = a^2 \\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right),\n\\]\nwhere \\(u(x,y,t)\\) is a three-variable function and \\(a\\) is a known constant. It models wave-like phenomena in two spatial dimensions, such as waves on the surface of a pond.\n\n\\(x\\) and \\(y\\) typically represent spatial variables (e.g., position on a surface).\n\\(t\\) typically represents time.\n\\(u(x,y,t)\\) represents the displacement of the wave at position \\((x,y)\\) and time \\(t\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#the-heat-equation-in-1-dimension",
    "href": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#the-heat-equation-in-1-dimension",
    "title": "01 An introduction to PDEs",
    "section": "The heat equation in 1 dimension",
    "text": "The heat equation in 1 dimension\n\n\n\n\nExample: the \\(1\\)-dimensional heat equation\n\n\nThis equation says that\n\\[\n\\frac{\\partial u}{\\partial t} = k \\frac{\\partial^2 u}{\\partial x^2},\n\\]\nwhere \\(u(x,t)\\) is a two-variable function and \\(k\\) is a known constant. It models “stationary” diffusion processes (i.e., no “drift”), such as the diffusion of heat in a rod over time.\n\n\\(x\\) typically represents a spatial variable (e.g., position along a rod).\n\\(t\\) typically represents time.\n\\(u(x,t)\\) represents the temperature at position \\(x\\) and time \\(t\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#the-fokker-planck-diffusion-equation",
    "href": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#the-fokker-planck-diffusion-equation",
    "title": "01 An introduction to PDEs",
    "section": "The Fokker-Planck (diffusion) equation",
    "text": "The Fokker-Planck (diffusion) equation\n\n\n\n\nExample: the Fokker-Planck (diffusion) equation\n\n\nThis equation says that\n\\[\n\\frac{\\partial u}{\\partial t} = -\\mu \\frac{\\partial u}{\\partial x} + \\frac{\\sigma^2}{2} \\frac{\\partial^2u}{\\partial x^2},\n\\]\nwhere \\(u(x,t)\\) is a two-variable function and \\(\\mu\\) and \\(\\sigma\\) are known constants. It models diffusion processes with “drift” or “trend,” such as the evolution of probability densities in random processes. A good example to have in mind is the random price of a stock or some financial asset over time:\n\n\\(x\\) then represents the stock price.\n\\(t\\) represents time.\n\\(\\mu\\) represents the trend of the stock, with \\(\\mu&gt;0\\) indicating an average upward trend and \\(\\mu&lt;0\\) indicating an average downward trend.\n\\(\\sigma\\) represents the volatility (i.e., randomness) of the stock price.\n\\(u(x,t)\\) represents the probability density of the stock price being \\(x\\) at time \\(t\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#deriving-the-wave-equation-part-1",
    "href": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#deriving-the-wave-equation-part-1",
    "title": "01 An introduction to PDEs",
    "section": "Deriving the wave equation (part 1)",
    "text": "Deriving the wave equation (part 1)\n\nWe imagine that we have a vibrating string of length \\(L\\) fixed at both ends.\n\nFor definiteness, suppose that the string lies along the \\(x\\)-axis, and is clamped at \\(x=0\\) and \\(x=L\\).\n\n\n\n\n\n\nThe wave problem as a boundary-value/initial-value problem\n\n\n\nNow, suppose you know:\n\nThe initial displacement of the string, i.e., you know the function \\(u(x,0)\\) for \\(0 \\leq x \\leq L\\).\nThe initial velocity of each point on the string, i.e., you know the derivative \\(\\displaystyle\\frac{\\partial u}{\\partial t}(x,0)\\) for \\(0 \\leq x \\leq L\\).\nThe string is fixed at both ends, so \\(u(0,t) = 0\\) and \\(u(L,t) = 0\\) for all \\(t \\geq 0\\).\n\nFrom these boundary and initial conditions, can we determine how the string moves over time? In other words, can we find \\(u(x,t)\\) for all \\(x\\) and \\(t\\)?\n\n\n\n\n\n\nThe answer is yes, provided that we are willing to make some simplifying assumptions about the string and its motion.\nEssentially, the goal is to derive an equation that \\(u(x,t)\\) must satisfy. This will be the wave equation.\nWe follow an argument begun by John Bernoulli in 1727, and later advanced by d’Alembert in 1747."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#deriving-the-wave-equation-part-2",
    "href": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#deriving-the-wave-equation-part-2",
    "title": "01 An introduction to PDEs",
    "section": "Deriving the wave equation (part 2)",
    "text": "Deriving the wave equation (part 2)\n\n\n\n\nDeriving the wave equation via beads\n\n\n\nHere’s the plan:\n\nSuppose that the string is chopped up into \\(n\\) equal length segments, with a small bead between each segment. The beads can move up and down, but not left and right. The beads all have equal mass. There are \\(n+1\\) beads in total, including the two fixed beads at the ends.\nThe \\(n\\) string segments are weightless, and perfectly flexible and elastic. In particular, the segments are always straight and do not curve. (You could think of the string segments as tiny springs.)\nWe analyze the motion of each of the \\(n-1\\) interior beads using Newton’s laws of motion. We obtain equations of motion for each bead.\nThen, to treat the “real, continuous” string, we let the number \\(n\\) of beads tend to \\(\\infty\\), while the mass of each bead also tends to \\(0\\).\nAs \\(n\\to \\infty\\), the equations derived in step 2 will combine to reveal the wave equation."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#deriving-the-wave-equation-part-3",
    "href": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#deriving-the-wave-equation-part-3",
    "title": "01 An introduction to PDEs",
    "section": "Deriving the wave equation (part 3)",
    "text": "Deriving the wave equation (part 3)\n\nThe \\(x\\)-positions of the beads are given by \\[\n  x_k = k\\frac{L}{n}, \\quad k = 1, 2, \\ldots, n,\n  \\] including the one at the right end \\(x=L\\) (which doesn’t move).\nThe vertical displacement of the \\(k\\)-th bead at time \\(t\\) is given by \\(u_k\\).\nEach of the interior beads feels a pull (i.e., tension force) from the bead to its left and a pull from the bead to its right.\n\nWe assume that all these tension forces have the same magnitude \\(T\\) (a known constant).\nWe assume that the mass of each bead is \\(m\\) (also a known constant).\n\nBy Newton’s second law, the vertical acceleration of the \\(k\\)-th bead is given by \\[\n  m \\frac{d^2 u_k}{dt^2} = F_{k},\n  \\] where \\(F_{k}\\) is the net vertical force acting on the \\(k\\)-th bead. Since the beads do not move left or right, the horizontal components of the tension forces cancel out.\nIf \\(\\theta_{k+1}\\) is the angle that the string to the right of the \\(k\\)-th bead makes with the positive \\(x\\)-axis, measured counterclockwise, and if \\(\\theta_{k-1}\\) is the angle that the string to the left of the \\(k\\)-th bead makes with the negative \\(x\\)-axis, measured clockwise, then a little trigonometry shows that \\[\n  F_{k} = T \\sin\\theta_{k+1} + T \\sin\\theta_{k-1}.\n  \\]\nBut as long as these angles are small, we have \\[\n  \\sin\\theta_{k+1} \\approx \\tan \\theta_{k+1} = \\frac{u_{k+1} - u_k}{L/n} \\quad \\text{and} \\quad \\sin\\theta_{k-1} \\approx \\tan \\theta_{k-1} = \\frac{u_{k-1} - u_k}{L/n}.\n  \\]"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#deriving-the-wave-equation-part-4",
    "href": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#deriving-the-wave-equation-part-4",
    "title": "01 An introduction to PDEs",
    "section": "Deriving the wave equation (part 4)",
    "text": "Deriving the wave equation (part 4)\n\nThus, for small angles, we have \\[\n  F_{k} = \\frac{Tn}{L} (u_{k+1} - 2u_k + u_{k-1})\n  \\] where we’ve written “\\(=\\)” for simplicity instead of “\\(\\approx\\)”.\nIf we write \\(M = nm\\) (which is almost the total mass of the string—why?), then Newton’s second law becomes \\[\n  \\frac{d^2 u_k}{dt^2} = \\frac{Tn^2}{ML} (u_{k+1} - 2u_k + u_{k-1}).\n  \\]\nSet \\(a^2 = \\frac{TL}{M}\\) and \\(\\Delta x = \\frac{L}{n}\\), and then rearrange to get \\[\n  \\frac{d^2 u_k}{dt^2} = a^2 \\left(\\frac{u_{k+1} - 2u_k + u_{k-1}}{(\\Delta x)^2}\\right).\n  \\]\nNow, switch to function notation by writing \\(u(x,t)\\) instead of \\(u_k\\), and \\(x\\) instead of \\(x_k\\). Then the equation becomes \\[\n  \\frac{\\partial^2 u(x, t)}{\\partial t^2} = a^2 \\left(\\frac{u(x + \\Delta x, t) - 2u(x, t) + u(x - \\Delta x, t)}{(\\Delta x)^2}\\right).\n  \\]"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#deriving-the-wave-equation-part-5",
    "href": "teaching/pde-sp-26/slides/01-an-introduction-to-pdes.html#deriving-the-wave-equation-part-5",
    "title": "01 An introduction to PDEs",
    "section": "Deriving the wave equation (part 5)",
    "text": "Deriving the wave equation (part 5)\n\nFrom calculus, we know that \\[\n  u(x+\\Delta x, t) = u(x,t) + \\frac{\\partial u}{\\partial x}(x,t)\\Delta x + \\frac{\\partial^2 u}{\\partial x^2}(x,t)\\frac{(\\Delta x)^2}{2} + (\\text{higher order terms in } \\Delta x),\n  \\] and \\[\n  u(x-\\Delta x, t) = u(x,t) - \\frac{\\partial u}{\\partial x}(x,t)\\Delta x + \\frac{\\partial^2 u}{\\partial x^2}(x,t)\\frac{(\\Delta x)^2}{2} + (\\text{higher order terms in } \\Delta x).\n  \\]\nThus \\[\n  u(x + \\Delta x, t) - 2u(x, t) + u(x - \\Delta x, t) = \\frac{\\partial^2 u}{\\partial x^2}(x,t)(\\Delta x)^2 + (\\text{higher order terms in } \\Delta x).\n  \\]\nAnd so \\[\n  \\frac{\\partial^2 u(x, t)}{\\partial t^2} = a^2 \\frac{\\partial^2 u}{\\partial x^2}(x,t) + (\\text{terms that vanish as } \\Delta x \\to 0).\n  \\]\nIf we remember that \\(M=mn\\) and \\(\\Delta x = L/n\\), then we want \\(n\\to \\infty\\) and \\(m\\to 0\\) in such a way that \\(M\\) remains constant. Then \\(\\Delta x \\to 0\\) as \\(n\\to \\infty\\), while \\(a^2 = \\frac{TL}{M}\\) remains constant.\nThis gives the wave equation: \\[\n  \\frac{\\partial^2 u}{\\partial t^2} = a^2 \\frac{\\partial^2 u}{\\partial x^2}.\n  \\] Here, \\(a^2 = T/ \\rho\\), where \\(\\rho\\) is the mass per unit length of the string and \\(T\\) is the tension force."
  },
  {
    "objectID": "teaching/calc-ii-fa-25/calc-ii-fa-25.html",
    "href": "teaching/calc-ii-fa-25/calc-ii-fa-25.html",
    "title": "mat220 calculus II, fall 2025",
    "section": "",
    "text": "instructor:\n\n\njohn myers\n\n\n\n\noffice + office hours:\n\n\nmarano 175, 12-12:30 MWF\n\n\n\n\nhomework:\n\n\nlink\n\n\n\n\nsyllabus:\n\n\nlink\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndate\ntopics\ninfo + due dates\n\n\n\n\n15\n12.08 mon\nfinal exam, 8-10am, Lanigan 102\nsection 11.10 hw due @ 8:00am  final exam extra review  review solutions\n\n\n14\n12.05 fri\n11.10 Taylor and Maclaurin series  \nsection 11.8 hw due @ 12:30pm\n\n\n\n12.04 thu\n11.8 power series  \nsection 11.6 hw due @ 9:30am\n\n\n\n12.03 wed\n11.8 power series  \nsection 11.5 hw due @ 12:30pm\n\n\n\n12.01 mon\n11.6 root and ratio tests  \n\n\n\n13\n11.21 fri\n11.5 alternating series  \n\n\n\n\n11.20 thu\nexam 2 on sections 7.4-11.4\nsection 11.4 hw due @ 9:30am  exam 2 extra review  review solutions\n\n\n\n11.19 wed\n11.4 comparison test  \nsection 11.3 hw due @ 12:30pm\n\n\n\n11.17 mon\n11.4 comparison test  \n\n\n\n12\n11.14 fri\n11.3 integral test  \nsection 11.2 hw due @ 12:30pm  quiz 12 on section 11.2\n\n\n\n11.13 thu\n11.2 series, part 2    11.3 integral test  \n\n\n\n\n11.12 wed\nno class\n\n\n\n\n11.10 mon\n11.2 series, part 2    11.3 integral test  \n\n\n\n11\n11.07 fri\n11.2 series, part 2  \nquiz 11 on section 11.1\n\n\n\n11.06 thu\n11.2 series, part 1  \nsection 11.1 hw due @ 12:30pm\n\n\n\n11.05 wed\n11.1 sequences, part 2    11.2 series, part 1  \n\n\n\n\n11.03 mon\n11.1 sequences, part 1    11.1 sequences, part 2  \n\n\n\n10\n10.31 fri\n11.1 sequences, part 1  \nsection 7.8 hw due @ 12:30pm  quiz 10 on sections 7.7 and 7.8\n\n\n\n10.30 thu\ngroup exploration 10\n\n\n\n\n10.29 wed\n7.8 improper integrals  \n\n\n\n\n10.27 mon\nno class\nsection 7.7 hw due @ 12:30pm\n\n\n9\n10.24 fri\n7.7 approximate integration, part 2    7.8 improper integrals  \nquiz 9 on section 7.4\n\n\n\n10.23 thu\n7.7 approximate integration, part 2  \n\n\n\n\n10.22 wed\n7.7 approximate integration, part 1  \nsection 7.4 hw due @ 12:30pm\n\n\n\n10.20 mon\n7.4 partial fractions, part 2  \n\n\n\n8\n10.17 fri\n7.4 partial fractions, part 1  \n\n\n\n\n10.16 thu\nexam 1 on sections 5.1-7.3\nexam 1 extra review  review solutions\n\n\n\n10.15 wed\nreview for exam\n\n\n\n\n10.13 mon\nno class\n\n\n\n7\n10.10 fri\nno class - fall break\n\n\n\n\n10.09 thu\nno class - fall break\n\n\n\n\n10.08 wed\n7.4 partial fractions, part 1  \nsection 7.3, part 2, hw due @ 12:30pm\n\n\n\n10.06 mon\nsec 800: class cancelled  sec 810: 7.3 trigonometric substitution, part 2  \nsection 7.3, part 1, hw due @ 12:30pm\n\n\n6\n10.03 fri\nsec 800: 7.3 trigonometric substitution, part 2    sec 810: class cancelled\nquiz 6 on section 7.1\n\n\n\n10.04 thu\n7.3 trigonometric substitution, part 1    7.3 trigonometric substitution, part 2  \n\n\n\n\n10.01 wed\n7.3 trigonometric substitution, part 1  \nsection 7.1 hw due @ 12:30pm\n\n\n\n09.29 mon\n7.1 integration by parts  \n\n\n\n5\n09.26 fri\n7.1 integration by parts  \nquiz 5 on sections 5.4 and 5.5\n\n\n\n09.25 thu\ngroup exploration 5\n\n\n\n\n09.24 wed\n7.1 integration by parts  \nsection 5.5 hw due @ 12:30pm\n\n\n\n09.22 mon\n5.5 average value of a function  \nsection 5.4 hw due @ 12:30pm\n\n\n4\n09.19 fri\n5.4 work  \nsection 5.3 hw due @ 12:30pmquiz 4 on section 5.3\n\n\n\n09.18 thu\ngroup exploration 4\n\n\n\n\n09.17 wed\n5.4 work  \n\n\n\n\n09.15 mon\n5.3 volumes by cylindrical shells  \nsection 5.2 hw due @ 12:30pm\n\n\n3\n09.12 fri\n5.2 volumes, part 2  \nquiz 3 on section 5.1\n\n\n\n09.11 thu\ngroup exploration 3\n\n\n\n\n09.10 wed\n5.2 volumes, part 1  5.2 volumes, part 2  \n\n\n\n\n09.08 mon\n5.2 volumes, part 1  \nsection 5.1 hw due @ 12:30pm\n\n\n2\n09.05 fri\n5.1 areas between curves  \n\n\n\n\n09.04 thu\ngroup exploration 2\ncalculus I review homework due @ 9:30am\n\n\n\n09.03 wed\ncalculus I review, part 2  \n\n\n\n1\n08.29 fri\nno class\n\n\n\n\n08.28 thu\nno class\n\n\n\n\n08.27 wed\ncalculus I review, part 1  \n\n\n\n\n08.25 mon\ncalculus I review, part 1"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/04ex-planes-and-linear-spaces-part-2.html",
    "href": "teaching/multi-calc-sp-26/exercises/04ex-planes-and-linear-spaces-part-2.html",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall that the point-slope form of the equation of a hyperplane in \\(\\mathbb{R}^4\\) is given by:\n\\[\nw = m(x - x_0) + n(y - y_0) + p(z - z_0) + w_0\n\\]\nwhere \\((x_0, y_0, z_0, w_0)\\) is a point on the plane, and \\(m\\), \\(n\\), \\(p\\), and \\(q\\) are the slopes in the positive \\(x\\)-, \\(y\\)-, \\(z\\)-, and \\(w\\)-directions, respectively.\nSimilarly, the point-slope form of the equation of a hyperplane in \\(\\mathbb{R}^5\\) is given by:\n\\[\nu = m(x - x_0) + n(y - y_0) + p(z - z_0) + q(w - w_0) + u_0\n\\]\nwhere \\((x_0, y_0, z_0, w_0, u_0)\\) is a point on the hyperplane, and \\(m\\), \\(n\\), \\(p\\), \\(q\\), and \\(r\\) are the slopes in the positive \\(x\\)-, \\(y\\)-, \\(z\\)-, \\(w\\)-, and \\(u\\)-directions, respectively.\n\n\n\n\n\n\n\nWrite the point-slope equation for a hyperplane in \\(\\mathbb{R}^4\\) that passes through the point \\((-1, 0, 4, 3)\\) and has slopes \\(2\\) in the positive \\(x\\)-direction, \\(-1\\) in the positive \\(y\\)-direction, and \\(3\\) in the positive \\(z\\)-direction.\nWrite the point-slope equation for a hyperplane in \\(\\mathbb{R}^5\\) that passes through the point \\((0, 1, -1, 2, 5)\\) and has slopes \\(1\\) in the positive \\(x\\)-direction, \\(2\\) in the positive \\(y\\)-direction, \\(-3\\) in the positive \\(z\\)-direction, and \\(4\\) in the positive \\(w\\)-direction.\nDoes the point \\((2, 3, 4, 6)\\) lie on the hyperplane from part (a)?\nDoes the point \\((1, 2, 0, 3, 8)\\) lie on the hyperplane from part (b)?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(w = 2(x +1 ) - y + 3(z - 4) + 3\\)\n\\(u = x + 2(y - 1) - 3(z + 1) + 4(w - 2) + 5\\)\nYes.\nNo.\n\n\n\n\n\n\n\nRecall that the standard form of the equation of a hyperplane in \\(\\mathbb{R}^4\\) is given by:\n\\[\na(x - x_0) + b(y - y_0) + c(z - z_0) + d(w - w_0) = 0\n\\]\nwhere \\((x_0, y_0, z_0, w_0)\\) is a point on the plane, and \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are the components of a normal vector to the hyperplane.\nThe affine form of the equation of a hyperplane in \\(\\mathbb{R}^4\\) is given by:\n\\[\nax + by + cz + dw = e\n\\]\nwhere \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are the components of a normal vector to the hyperplane, and \\(e\\) is a constant.\n\n\n\n\n\n\nConsider the hyperplane in \\(\\mathbb{R}^4\\) with point-slope equation: \\[\nw = 3(x-2) - 2(y+1) + 4(z-1) + 5\n\\]\n\nIdentify a point on the hyperplane.\nIdentify the slopes in the positive \\(x\\)-, \\(y\\)-, and \\(z\\)-directions.\nWrite the equation in standard form.\nIdentify a normal vector to the hyperplane.\nWrite the affine equation for the hyperplane.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\((2, -1, 1, 5)\\)\n\\(3\\), \\(-2\\), \\(4\\)\n\\(3(x-2) - 2(y+1) + 4(z-1) - (w - 5) = 0\\)\n\\(\\mathbf{n} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 4 \\\\ -1 \\end{bmatrix}\\)\n$ 3x - 2y + 4z - w = 7$\n\n\n\n\n\n\n\nThis next exercise is tricky. In order to solve it, you’ll need to leverage your intuition about planes in \\(\\mathbb{R}^3\\) and apply it to hyperplanes in higher dimensions. Don’t give up and look at the solution too quickly.\n\n\n\n\n\n\n\nConsider the two hyperplanes in \\(\\mathbb{R}^5\\) given by the equations: \\[\n2x + 3y - z + w - 4u = 10\n\\] and \\[\n-4x - 6y + 2z - 2w + 8u = 5\n\\]\n\nAre these hyperplanes parallel? Explain.\nWhat is the distance between these two hyperplanes?\n\nGive the equation of the hyperplane in \\(\\mathbb{R}^5\\) that passes through the point \\((-1, 2, 0, 8, 5)\\) and is parallel to the \\(xzwu\\)-coordinate hyperplane.\nYou are standing at the point \\((1, 0, -2, 3, 4)\\) on the hyperplane with standard equation \\[\n2(x-1) + 4y - 3(z+2) + 5(w-3) = 0.\n\\] Identify a normal vector to the plane with positive \\(x\\)-component. Then, suppose you step 2 units along this normal vector. What are your new coordinates?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\nYes, these hyperplanes are parallel. Their normal vectors are \\[\n  \\begin{bmatrix} 2 \\\\ 3 \\\\ -1 \\\\ 1 \\\\ -4 \\end{bmatrix} \\quad \\text{and} \\quad \\begin{bmatrix} -4 \\\\ -6 \\\\ 2 \\\\ -2 \\\\ 8 \\end{bmatrix},\n  \\] which are scalar multiples of each other (the second is \\(-2\\) times the first) and hence parallel. But if their normal vectors are parallel, then the hyperplanes themselves must be parallel.\nA point on the first hyperplane is \\((0, 0, -10, 0, 0)\\) with position vector \\[\n  \\mathbf{r} = \\begin{bmatrix} 0 \\\\ 0 \\\\ -10 \\\\ 0 \\\\ 0 \\end{bmatrix},\n  \\] while a normal vector to the same hyperplane is \\[\n  \\mathbf{n} = \\begin{bmatrix} 2 \\\\ 3 \\\\ -1 \\\\ 1 \\\\ -4 \\end{bmatrix}.\n  \\] If we can find a value of \\(t\\) for which the point with position vector \\(\\mathbf{r} + t\\mathbf{n}\\) lies on the second hyperplane, then the distance between the hyperplanes is given by \\(\\|t\\mathbf{n}\\|\\). However, we have \\[\n  \\mathbf{r} + t\\mathbf{n} = \\begin{bmatrix} 2t \\\\ 3t \\\\ -10 - t \\\\ t \\\\ -4t \\end{bmatrix},\n  \\] and substituting the components of this vector into the second hyperplane’s equation gives \\[\n  -4(2t) - 6(3t) + 2(-10 - t) - 2(t) + 8(-4t) = 5.\n  \\] Solving for \\(t\\) gives \\(t = -25/62\\), and thus the distance between the hyperplanes is \\[\n  \\|t\\mathbf{n}\\| = \\left\\| -\\frac{25}{62} \\begin{bmatrix} 2 \\\\ 3 \\\\ -1 \\\\ 1 \\\\ -4 \\end{bmatrix} \\right\\| \\approx 2.25.\n  \\]\n\nThe \\(xzwu\\)-coordinate hyperplane is the hyperplane in \\(\\mathbb{R}^5\\) given by the equation \\(y = 0\\). A hyperplane parallel to this hyperplane must also have the form \\(y = c\\) for some constant \\(c\\). Since the hyperplane must pass through the point \\((-1, 2, 0, 8, 5)\\), we must have \\(y=2\\).\nA normal vector to the plane with positive \\(x\\)-component is \\[\n\\mathbf{n} = \\begin{bmatrix} 2 \\\\ 4 \\\\ -3 \\\\ 5 \\\\ 0 \\end{bmatrix}.\n\\] In order to step 2 units along this normal vector, we first need to normalize it (i.e., make it a unit vector) and then scale it by 2. But the magnitude of \\(\\mathbf{n}\\) is \\[\n\\|\\mathbf{n}\\| = \\sqrt{2^2 + 4^2 + (-3)^2 + 5^2 + 0^2} = 3 \\sqrt{6}.\n\\] So, the point that we end up at has position vector \\[\n\\begin{bmatrix} 1 \\\\ 0 \\\\ -2 \\\\ 3 \\\\ 4 \\end{bmatrix} + \\frac{2}{3 \\sqrt{6}} \\begin{bmatrix} 2 \\\\ 4 \\\\ -3 \\\\ 5 \\\\ 0 \\end{bmatrix} \\approx \\begin{bmatrix} 0.54 \\\\ 1.09 \\\\ -0.82 \\\\ 1.36 \\\\ 0 \\end{bmatrix}.\n\\]\n\n\n\n\n\n\n\nRecall from class that the level set of a function \\(f: \\mathbb{R}^2 \\to \\mathbb{R}\\) is the set of points \\((x, y)\\) such that \\(f(x, y) = c\\) for some constant \\(c\\). Level sets of functions of two variables are curves in the \\(xy\\)-plane.\nSimilarly, the level set of a function \\(g: \\mathbb{R}^3 \\to \\mathbb{R}\\) is the set of points \\((x, y, z)\\) such that \\(g(x, y, z) = c\\) for some constant \\(c\\). Level sets of functions of three variables are surfaces in \\(\\mathbb{R}^3\\).\n\n\n\n\n\n\nConsider the linear functions \\(f: \\mathbb{R}^2 \\to \\mathbb{R}\\) and \\(g: \\mathbb{R}^3 \\to \\mathbb{R}\\) given by: \\[\nz = f(x, y) = 3x - 2y + 6\n\\]\nand\n\\[\nw = g(x, y, z) = x + 2y - z + 4\n\\]\n\nSketch the level sets of \\(f\\) for \\(c = 0, 6, 12\\) in the \\(xy\\)-plane.\nSketch the level sets of \\(g\\) for \\(c = 0, 4, 8\\) in \\(\\mathbb{R}^3\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe level sets of \\(f\\) are given by the equations:\n\nFor \\(c = 0\\): \\(3x - 2y + 6 = 0\\) or \\(y = \\frac{3}{2}x + 3\\)\nFor \\(c = 6\\): \\(3x - 2y + 6 = 6\\) or \\(y = \\frac{3}{2}x\\)\nFor \\(c = 12\\): \\(3x - 2y + 6 = 12\\) or \\(y = \\frac{3}{2}x - 3\\)\n\n\nThese are parallel lines in the \\(xy\\)-plane with slope \\(\\frac{3}{2}\\).\n\nThe level sets of \\(g\\) are given by the equations:\n\nFor \\(c = 0\\): \\(x + 2y - z + 4 = 0\\) or \\(z = x + 2y + 4\\)\nFor \\(c = 4\\): \\(x + 2y - z + 4 = 4\\) or \\(z = x + 2y\\)\nFor \\(c = 8\\): \\(x + 2y - z + 4 = 8\\) or \\(z = x + 2y - 4\\)\n\n\nThese are parallel planes in \\(\\mathbb{R}^3\\) with normal vector \\[\n\\mathbf{n} = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}.\n\\]\n\n\n\n\n\n\nThough we have emphasize the geometric nature of hyperplanes in this section, they also have many applications in real-world problems. In this exercise, you’ll apply your understanding of hyperplanes to solve a problem in business.\n\n\n\n\n\n\nA company’s profit, \\(P\\) (in thousands of dollars), depends on four factors:\n\n\\(x\\) = advertising budget (in thousands of dollars)\n\\(y\\) = number of sales representatives\n\\(z\\) = product quality rating (on a scale of 0-10)\n\\(w\\) = competitor’s price (in dollars)\n\nThe profit function is given by: \\[\nP = f(x, y, z, w) = 2x + 5y + 10z + 0.5w - 100\n\\]\n\nWhat is the base profit when all variables are zero?\nHow much does profit increase for each additional thousand dollars spent on advertising?\nHow much does profit increase for each additional sales representative?\nIf the company spends \\(\\$50{,}000\\) on advertising, employs \\(15\\) sales representatives, has a product quality rating of \\(8\\), and the competitor’s price is \\(\\$120\\), what is the profit?\nThe level set \\(P = 0\\) represents the break-even condition (zero profit). Write the equation of this level set. What geometric object does it represent in \\(\\mathbb{R}^4\\)?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(-\\$100{,}000\\)\n\\(\\$2{,}000\\) per additional thousand dollars spent on advertising.\n\\(\\$5{,}000\\) per additional sales representative.\n\\(\\$215{,}000\\)\nThe equation is \\[\n2x + 5y + 10z + 0.5w = 100,\n\\] which is the affine form of a hyperplane in \\(\\mathbb{R}^4\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/04ex-planes-and-linear-spaces-part-2.html#exercise-1-point-slope-equations-in-higher-dimensions",
    "href": "teaching/multi-calc-sp-26/exercises/04ex-planes-and-linear-spaces-part-2.html#exercise-1-point-slope-equations-in-higher-dimensions",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall that the point-slope form of the equation of a hyperplane in \\(\\mathbb{R}^4\\) is given by:\n\\[\nw = m(x - x_0) + n(y - y_0) + p(z - z_0) + w_0\n\\]\nwhere \\((x_0, y_0, z_0, w_0)\\) is a point on the plane, and \\(m\\), \\(n\\), \\(p\\), and \\(q\\) are the slopes in the positive \\(x\\)-, \\(y\\)-, \\(z\\)-, and \\(w\\)-directions, respectively.\nSimilarly, the point-slope form of the equation of a hyperplane in \\(\\mathbb{R}^5\\) is given by:\n\\[\nu = m(x - x_0) + n(y - y_0) + p(z - z_0) + q(w - w_0) + u_0\n\\]\nwhere \\((x_0, y_0, z_0, w_0, u_0)\\) is a point on the hyperplane, and \\(m\\), \\(n\\), \\(p\\), \\(q\\), and \\(r\\) are the slopes in the positive \\(x\\)-, \\(y\\)-, \\(z\\)-, \\(w\\)-, and \\(u\\)-directions, respectively.\n\n\n\n\n\n\n\nWrite the point-slope equation for a hyperplane in \\(\\mathbb{R}^4\\) that passes through the point \\((-1, 0, 4, 3)\\) and has slopes \\(2\\) in the positive \\(x\\)-direction, \\(-1\\) in the positive \\(y\\)-direction, and \\(3\\) in the positive \\(z\\)-direction.\nWrite the point-slope equation for a hyperplane in \\(\\mathbb{R}^5\\) that passes through the point \\((0, 1, -1, 2, 5)\\) and has slopes \\(1\\) in the positive \\(x\\)-direction, \\(2\\) in the positive \\(y\\)-direction, \\(-3\\) in the positive \\(z\\)-direction, and \\(4\\) in the positive \\(w\\)-direction.\nDoes the point \\((2, 3, 4, 6)\\) lie on the hyperplane from part (a)?\nDoes the point \\((1, 2, 0, 3, 8)\\) lie on the hyperplane from part (b)?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(w = 2(x +1 ) - y + 3(z - 4) + 3\\)\n\\(u = x + 2(y - 1) - 3(z + 1) + 4(w - 2) + 5\\)\nYes.\nNo."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/04ex-planes-and-linear-spaces-part-2.html#exercise-2-converting-between-equation-forms-in-mathbbr4",
    "href": "teaching/multi-calc-sp-26/exercises/04ex-planes-and-linear-spaces-part-2.html#exercise-2-converting-between-equation-forms-in-mathbbr4",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall that the standard form of the equation of a hyperplane in \\(\\mathbb{R}^4\\) is given by:\n\\[\na(x - x_0) + b(y - y_0) + c(z - z_0) + d(w - w_0) = 0\n\\]\nwhere \\((x_0, y_0, z_0, w_0)\\) is a point on the plane, and \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are the components of a normal vector to the hyperplane.\nThe affine form of the equation of a hyperplane in \\(\\mathbb{R}^4\\) is given by:\n\\[\nax + by + cz + dw = e\n\\]\nwhere \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are the components of a normal vector to the hyperplane, and \\(e\\) is a constant.\n\n\n\n\n\n\nConsider the hyperplane in \\(\\mathbb{R}^4\\) with point-slope equation: \\[\nw = 3(x-2) - 2(y+1) + 4(z-1) + 5\n\\]\n\nIdentify a point on the hyperplane.\nIdentify the slopes in the positive \\(x\\)-, \\(y\\)-, and \\(z\\)-directions.\nWrite the equation in standard form.\nIdentify a normal vector to the hyperplane.\nWrite the affine equation for the hyperplane.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\((2, -1, 1, 5)\\)\n\\(3\\), \\(-2\\), \\(4\\)\n\\(3(x-2) - 2(y+1) + 4(z-1) - (w - 5) = 0\\)\n\\(\\mathbf{n} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 4 \\\\ -1 \\end{bmatrix}\\)\n$ 3x - 2y + 4z - w = 7$"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/04ex-planes-and-linear-spaces-part-2.html#exercise-3-leveraging-intuition-from-planes-in-mathbbr3",
    "href": "teaching/multi-calc-sp-26/exercises/04ex-planes-and-linear-spaces-part-2.html#exercise-3-leveraging-intuition-from-planes-in-mathbbr3",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "This next exercise is tricky. In order to solve it, you’ll need to leverage your intuition about planes in \\(\\mathbb{R}^3\\) and apply it to hyperplanes in higher dimensions. Don’t give up and look at the solution too quickly.\n\n\n\n\n\n\n\nConsider the two hyperplanes in \\(\\mathbb{R}^5\\) given by the equations: \\[\n2x + 3y - z + w - 4u = 10\n\\] and \\[\n-4x - 6y + 2z - 2w + 8u = 5\n\\]\n\nAre these hyperplanes parallel? Explain.\nWhat is the distance between these two hyperplanes?\n\nGive the equation of the hyperplane in \\(\\mathbb{R}^5\\) that passes through the point \\((-1, 2, 0, 8, 5)\\) and is parallel to the \\(xzwu\\)-coordinate hyperplane.\nYou are standing at the point \\((1, 0, -2, 3, 4)\\) on the hyperplane with standard equation \\[\n2(x-1) + 4y - 3(z+2) + 5(w-3) = 0.\n\\] Identify a normal vector to the plane with positive \\(x\\)-component. Then, suppose you step 2 units along this normal vector. What are your new coordinates?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\nYes, these hyperplanes are parallel. Their normal vectors are \\[\n  \\begin{bmatrix} 2 \\\\ 3 \\\\ -1 \\\\ 1 \\\\ -4 \\end{bmatrix} \\quad \\text{and} \\quad \\begin{bmatrix} -4 \\\\ -6 \\\\ 2 \\\\ -2 \\\\ 8 \\end{bmatrix},\n  \\] which are scalar multiples of each other (the second is \\(-2\\) times the first) and hence parallel. But if their normal vectors are parallel, then the hyperplanes themselves must be parallel.\nA point on the first hyperplane is \\((0, 0, -10, 0, 0)\\) with position vector \\[\n  \\mathbf{r} = \\begin{bmatrix} 0 \\\\ 0 \\\\ -10 \\\\ 0 \\\\ 0 \\end{bmatrix},\n  \\] while a normal vector to the same hyperplane is \\[\n  \\mathbf{n} = \\begin{bmatrix} 2 \\\\ 3 \\\\ -1 \\\\ 1 \\\\ -4 \\end{bmatrix}.\n  \\] If we can find a value of \\(t\\) for which the point with position vector \\(\\mathbf{r} + t\\mathbf{n}\\) lies on the second hyperplane, then the distance between the hyperplanes is given by \\(\\|t\\mathbf{n}\\|\\). However, we have \\[\n  \\mathbf{r} + t\\mathbf{n} = \\begin{bmatrix} 2t \\\\ 3t \\\\ -10 - t \\\\ t \\\\ -4t \\end{bmatrix},\n  \\] and substituting the components of this vector into the second hyperplane’s equation gives \\[\n  -4(2t) - 6(3t) + 2(-10 - t) - 2(t) + 8(-4t) = 5.\n  \\] Solving for \\(t\\) gives \\(t = -25/62\\), and thus the distance between the hyperplanes is \\[\n  \\|t\\mathbf{n}\\| = \\left\\| -\\frac{25}{62} \\begin{bmatrix} 2 \\\\ 3 \\\\ -1 \\\\ 1 \\\\ -4 \\end{bmatrix} \\right\\| \\approx 2.25.\n  \\]\n\nThe \\(xzwu\\)-coordinate hyperplane is the hyperplane in \\(\\mathbb{R}^5\\) given by the equation \\(y = 0\\). A hyperplane parallel to this hyperplane must also have the form \\(y = c\\) for some constant \\(c\\). Since the hyperplane must pass through the point \\((-1, 2, 0, 8, 5)\\), we must have \\(y=2\\).\nA normal vector to the plane with positive \\(x\\)-component is \\[\n\\mathbf{n} = \\begin{bmatrix} 2 \\\\ 4 \\\\ -3 \\\\ 5 \\\\ 0 \\end{bmatrix}.\n\\] In order to step 2 units along this normal vector, we first need to normalize it (i.e., make it a unit vector) and then scale it by 2. But the magnitude of \\(\\mathbf{n}\\) is \\[\n\\|\\mathbf{n}\\| = \\sqrt{2^2 + 4^2 + (-3)^2 + 5^2 + 0^2} = 3 \\sqrt{6}.\n\\] So, the point that we end up at has position vector \\[\n\\begin{bmatrix} 1 \\\\ 0 \\\\ -2 \\\\ 3 \\\\ 4 \\end{bmatrix} + \\frac{2}{3 \\sqrt{6}} \\begin{bmatrix} 2 \\\\ 4 \\\\ -3 \\\\ 5 \\\\ 0 \\end{bmatrix} \\approx \\begin{bmatrix} 0.54 \\\\ 1.09 \\\\ -0.82 \\\\ 1.36 \\\\ 0 \\end{bmatrix}.\n\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/04ex-planes-and-linear-spaces-part-2.html#exercise-4-level-sets-of-linear-functions",
    "href": "teaching/multi-calc-sp-26/exercises/04ex-planes-and-linear-spaces-part-2.html#exercise-4-level-sets-of-linear-functions",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall from class that the level set of a function \\(f: \\mathbb{R}^2 \\to \\mathbb{R}\\) is the set of points \\((x, y)\\) such that \\(f(x, y) = c\\) for some constant \\(c\\). Level sets of functions of two variables are curves in the \\(xy\\)-plane.\nSimilarly, the level set of a function \\(g: \\mathbb{R}^3 \\to \\mathbb{R}\\) is the set of points \\((x, y, z)\\) such that \\(g(x, y, z) = c\\) for some constant \\(c\\). Level sets of functions of three variables are surfaces in \\(\\mathbb{R}^3\\).\n\n\n\n\n\n\nConsider the linear functions \\(f: \\mathbb{R}^2 \\to \\mathbb{R}\\) and \\(g: \\mathbb{R}^3 \\to \\mathbb{R}\\) given by: \\[\nz = f(x, y) = 3x - 2y + 6\n\\]\nand\n\\[\nw = g(x, y, z) = x + 2y - z + 4\n\\]\n\nSketch the level sets of \\(f\\) for \\(c = 0, 6, 12\\) in the \\(xy\\)-plane.\nSketch the level sets of \\(g\\) for \\(c = 0, 4, 8\\) in \\(\\mathbb{R}^3\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe level sets of \\(f\\) are given by the equations:\n\nFor \\(c = 0\\): \\(3x - 2y + 6 = 0\\) or \\(y = \\frac{3}{2}x + 3\\)\nFor \\(c = 6\\): \\(3x - 2y + 6 = 6\\) or \\(y = \\frac{3}{2}x\\)\nFor \\(c = 12\\): \\(3x - 2y + 6 = 12\\) or \\(y = \\frac{3}{2}x - 3\\)\n\n\nThese are parallel lines in the \\(xy\\)-plane with slope \\(\\frac{3}{2}\\).\n\nThe level sets of \\(g\\) are given by the equations:\n\nFor \\(c = 0\\): \\(x + 2y - z + 4 = 0\\) or \\(z = x + 2y + 4\\)\nFor \\(c = 4\\): \\(x + 2y - z + 4 = 4\\) or \\(z = x + 2y\\)\nFor \\(c = 8\\): \\(x + 2y - z + 4 = 8\\) or \\(z = x + 2y - 4\\)\n\n\nThese are parallel planes in \\(\\mathbb{R}^3\\) with normal vector \\[\n\\mathbf{n} = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}.\n\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/04ex-planes-and-linear-spaces-part-2.html#exercise-5-applied-problem-with-hyperplanes",
    "href": "teaching/multi-calc-sp-26/exercises/04ex-planes-and-linear-spaces-part-2.html#exercise-5-applied-problem-with-hyperplanes",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Though we have emphasize the geometric nature of hyperplanes in this section, they also have many applications in real-world problems. In this exercise, you’ll apply your understanding of hyperplanes to solve a problem in business.\n\n\n\n\n\n\nA company’s profit, \\(P\\) (in thousands of dollars), depends on four factors:\n\n\\(x\\) = advertising budget (in thousands of dollars)\n\\(y\\) = number of sales representatives\n\\(z\\) = product quality rating (on a scale of 0-10)\n\\(w\\) = competitor’s price (in dollars)\n\nThe profit function is given by: \\[\nP = f(x, y, z, w) = 2x + 5y + 10z + 0.5w - 100\n\\]\n\nWhat is the base profit when all variables are zero?\nHow much does profit increase for each additional thousand dollars spent on advertising?\nHow much does profit increase for each additional sales representative?\nIf the company spends \\(\\$50{,}000\\) on advertising, employs \\(15\\) sales representatives, has a product quality rating of \\(8\\), and the competitor’s price is \\(\\$120\\), what is the profit?\nThe level set \\(P = 0\\) represents the break-even condition (zero profit). Write the equation of this level set. What geometric object does it represent in \\(\\mathbb{R}^4\\)?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(-\\$100{,}000\\)\n\\(\\$2{,}000\\) per additional thousand dollars spent on advertising.\n\\(\\$5{,}000\\) per additional sales representative.\n\\(\\$215{,}000\\)\nThe equation is \\[\n2x + 5y + 10z + 0.5w = 100,\n\\] which is the affine form of a hyperplane in \\(\\mathbb{R}^4\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/05ex-functions-of-multiple-variables-part-1.html",
    "href": "teaching/multi-calc-sp-26/exercises/05ex-functions-of-multiple-variables-part-1.html",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "A car rental company charges \\(\\$40\\) a day and \\(15\\) cents a mile for its cars.\n\nWrite a formula for the cost, \\(C\\), of renting a car as a function, \\(f\\), of the number of days, \\(d\\), and the number of miles driven, \\(m\\).\nIf \\(C= f(d,m)\\), find \\(f(5,300)\\) and interpret it.\nDraw the graph of \\(f\\).\n\nGive a formula for the function \\(m= f(b,t)\\) where \\(m\\) is the amount of money in a bank account \\(t\\) years after an initial investment of \\(b\\) dollars, if interest is accrued at a rate of \\(1.2\\%\\) per year compounded annually. (Hint: Annual compounding means that \\(m\\) increases by a factor of \\(1.012\\) each year.)\nThe concentration \\(C\\) (mg/L) of a drug in the blood is modeled by \\[\nC= f(x,t) = te^{−t(5−x)},\n\\] where \\(x\\) is the dose (mg) and \\(t\\) is time (hours) since injection. Here \\(0 \\leq x \\leq 4\\) and \\(t \\geq 0\\).\n\nFind \\(f(3,2)\\). Give units and interpret your answer.\nGraph the function \\(C = f(4,t)\\) in the \\(tC\\)-plane, and explain its significance.\nGraph the function \\(C = f(x,1)\\) in the \\(xC\\)-plane, and explain its significance.\n\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\n\\(C = f(d,m) = 40d + 0.15m\\)\n\\(f(5,300) = 245\\), which means that the cost of renting a car for \\(5\\) days and driving \\(300\\) miles is \\(\\$245\\).\nThe graph is a plane in \\(dmC\\)-space, going through the origin, with a slope of \\(40\\) in the \\(d\\)-direction and a slope of \\(0.15\\) in the \\(m\\)-direction.\n\n\\(m = f(b,t) = b(1.012)^t\\)\n\n\\(f(3,2) \\approx 0.0366\\) mg/L, which means that the concentration of the drug in the blood is approximately \\(0.0366\\) mg/L two hours after an injection of \\(3\\) mg.\nFor the graph, use Desmos. The significance of the graph is that it shows how the concentration of the drug changes over time for a fixed dose of \\(4\\) mg.\nFor the graph, use Desmos. The significance of the graph is that it shows how the concentration of the drug changes with different doses at a fixed time of \\(1\\) hour.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEach of the following real-world scenarios can be modeled by a function of the form \\(f: \\mathbb{R}^m \\to \\mathbb{R}^n\\) for some \\(m\\) and \\(n\\). For each part, identify the appropriate values of \\(m\\) and \\(n\\).\n\nWind velocity as a function of position in \\(3\\)-dimensional space. (Remember, velocity is a vector, so it has three components.)\nRGB color values as a function of position on a computer screen. (RGB color values consist of a triple of numbers, each representing the intensity of red, green, and blue light, respectively.)\nTrajectory of a baseball (position in space as a function of time).\nProfit and revenue as functions of price and advertising budget.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(f: \\mathbb{R}^3 \\to \\mathbb{R}^3\\). Position in space has three components, and velocity has three components.\n\\(f: \\mathbb{R}^2 \\to \\mathbb{R}^3\\). Position on a computer screen has two components (horizontal and vertical), and RGB color values have three components.\n\\(f: \\mathbb{R} \\to \\mathbb{R}^3\\). Time has one component, and position in space has three components.\n\\(f: \\mathbb{R}^2 \\to \\mathbb{R}^2\\). Price and advertising budget have two components, and profit and revenue have two components."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/05ex-functions-of-multiple-variables-part-1.html#exercise-1-multivariable-functions-in-context",
    "href": "teaching/multi-calc-sp-26/exercises/05ex-functions-of-multiple-variables-part-1.html#exercise-1-multivariable-functions-in-context",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "A car rental company charges \\(\\$40\\) a day and \\(15\\) cents a mile for its cars.\n\nWrite a formula for the cost, \\(C\\), of renting a car as a function, \\(f\\), of the number of days, \\(d\\), and the number of miles driven, \\(m\\).\nIf \\(C= f(d,m)\\), find \\(f(5,300)\\) and interpret it.\nDraw the graph of \\(f\\).\n\nGive a formula for the function \\(m= f(b,t)\\) where \\(m\\) is the amount of money in a bank account \\(t\\) years after an initial investment of \\(b\\) dollars, if interest is accrued at a rate of \\(1.2\\%\\) per year compounded annually. (Hint: Annual compounding means that \\(m\\) increases by a factor of \\(1.012\\) each year.)\nThe concentration \\(C\\) (mg/L) of a drug in the blood is modeled by \\[\nC= f(x,t) = te^{−t(5−x)},\n\\] where \\(x\\) is the dose (mg) and \\(t\\) is time (hours) since injection. Here \\(0 \\leq x \\leq 4\\) and \\(t \\geq 0\\).\n\nFind \\(f(3,2)\\). Give units and interpret your answer.\nGraph the function \\(C = f(4,t)\\) in the \\(tC\\)-plane, and explain its significance.\nGraph the function \\(C = f(x,1)\\) in the \\(xC\\)-plane, and explain its significance.\n\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\n\\(C = f(d,m) = 40d + 0.15m\\)\n\\(f(5,300) = 245\\), which means that the cost of renting a car for \\(5\\) days and driving \\(300\\) miles is \\(\\$245\\).\nThe graph is a plane in \\(dmC\\)-space, going through the origin, with a slope of \\(40\\) in the \\(d\\)-direction and a slope of \\(0.15\\) in the \\(m\\)-direction.\n\n\\(m = f(b,t) = b(1.012)^t\\)\n\n\\(f(3,2) \\approx 0.0366\\) mg/L, which means that the concentration of the drug in the blood is approximately \\(0.0366\\) mg/L two hours after an injection of \\(3\\) mg.\nFor the graph, use Desmos. The significance of the graph is that it shows how the concentration of the drug changes over time for a fixed dose of \\(4\\) mg.\nFor the graph, use Desmos. The significance of the graph is that it shows how the concentration of the drug changes with different doses at a fixed time of \\(1\\) hour."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/05ex-functions-of-multiple-variables-part-1.html#exercise-2-identifying-function-types",
    "href": "teaching/multi-calc-sp-26/exercises/05ex-functions-of-multiple-variables-part-1.html#exercise-2-identifying-function-types",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Each of the following real-world scenarios can be modeled by a function of the form \\(f: \\mathbb{R}^m \\to \\mathbb{R}^n\\) for some \\(m\\) and \\(n\\). For each part, identify the appropriate values of \\(m\\) and \\(n\\).\n\nWind velocity as a function of position in \\(3\\)-dimensional space. (Remember, velocity is a vector, so it has three components.)\nRGB color values as a function of position on a computer screen. (RGB color values consist of a triple of numbers, each representing the intensity of red, green, and blue light, respectively.)\nTrajectory of a baseball (position in space as a function of time).\nProfit and revenue as functions of price and advertising budget.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(f: \\mathbb{R}^3 \\to \\mathbb{R}^3\\). Position in space has three components, and velocity has three components.\n\\(f: \\mathbb{R}^2 \\to \\mathbb{R}^3\\). Position on a computer screen has two components (horizontal and vertical), and RGB color values have three components.\n\\(f: \\mathbb{R} \\to \\mathbb{R}^3\\). Time has one component, and position in space has three components.\n\\(f: \\mathbb{R}^2 \\to \\mathbb{R}^2\\). Price and advertising budget have two components, and profit and revenue have two components."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/07ex-functions-of-multiple-variables-part-3.html",
    "href": "teaching/multi-calc-sp-26/exercises/07ex-functions-of-multiple-variables-part-3.html",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Consider the linear function \\(f: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) given by\n\\[\nf(x,y) = (2x - y + 4, x + 3y - 1).\n\\]\nAs I mentioned in class, this is called a linear function because it only contains terms of degrees \\(0\\) and \\(1\\).\nIf we blur the distinction between a point \\((x,y)\\) and its position vector\n\\[\n\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix},\n\\]\nthen we can rewrite the formula for \\(f\\) as\n\\[\nf\\left( \\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix} \\right) =\n\\begin{bmatrix}\n2 & -1 \\\\\n1 & 3\n\\end{bmatrix} \\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix} + \\begin{bmatrix} 4 \\\\ -1 \\end{bmatrix}.\n\\]\nCheck for yourself that if you multiply out the right-hand side, you will get\n\\[\nf\\left( \\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix} \\right) = \\begin{bmatrix}2x - y + 4 \\\\\nx + 3y - 1\n\\end{bmatrix},\n\\]\nwhich is the same as the original formula for \\(f\\), but written in a different way.\nThe same procedure works in higher dimensions, as you will discover in the following exercises:\n\n\n\n\n\n\nIn each of the following, convert the given formula for a linear function into a matrix/vector representation as shown above.\n\n\\(f: \\mathbb{R}^2 \\to \\mathbb{R}^2\\), \\(f(x,y) = (x + 2y - 3, 4x - y + 5)\\)\n\\(g: \\mathbb{R}^2 \\to \\mathbb{R}^2\\), \\(g(x,y) = (3x - 2y , -x + 5y - 4)\\)\n\\(h: \\mathbb{R}^3 \\to \\mathbb{R}^3\\), \\(h(x, y, z) = (x + 2y - z + 1, 3x - y + 4z - 2, -2x + 5y + z + 3)\\)\n\\(k: \\mathbb{R}^2 \\to \\mathbb{R}^3\\), \\(k(x, y) = (x - 4y + 2, -2x + 3y - 1, 3x - y)\\)\n\nNow go backwards. For each of the following, convert the given matrix/vector representation into a formula for a linear function.\n\n\\(r: \\mathbb{R}^2 \\to \\mathbb{R}^2\\), \\(r\\left( \\begin{bmatrix}x \\\\\ny\n\\end{bmatrix} \\right) =\n\\begin{bmatrix}1 & 4 \\\\\n-2 & 3\n\\end{bmatrix} \\begin{bmatrix}x \\\\\ny\n\\end{bmatrix} + \\begin{bmatrix} -3 \\\\ 2 \\end{bmatrix}\\)\n\\(s: \\mathbb{R}^2 \\to \\mathbb{R}^2\\), \\(s\\left( \\begin{bmatrix}x \\\\\ny\n\\end{bmatrix} \\right) =\n\\begin{bmatrix}0 & 5 \\\\\n-1 & 2\n\\end{bmatrix} \\begin{bmatrix}x \\\\\ny\n\\end{bmatrix} + \\begin{bmatrix} 1 \\\\ -4 \\end{bmatrix}\\)\n\\(t: \\mathbb{R}^3 \\to \\mathbb{R}^3\\), \\(t\\left( \\begin{bmatrix}x \\\\\ny \\\\\nz\n\\end{bmatrix} \\right) =\n\\begin{bmatrix}2 & -1 & 3 \\\\\n-4 & 5 & 0 \\\\\n1 & 2 & -2\n\\end{bmatrix} \\begin{bmatrix}x \\\\\ny \\\\\nz\n\\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix}\\)\n\\(u: \\mathbb{R}^2 \\to \\mathbb{R}^3\\), \\(u\\left( \\begin{bmatrix}x \\\\ y\\end{bmatrix} \\right) =\n\\begin{bmatrix}1 & -2 \\\\\n3 & 0 \\\\\n-1 & 4\n\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix} + \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix}\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(f\\left( \\begin{bmatrix}x \\\\ y\\end{bmatrix} \\right) =\n\\begin{bmatrix}1 & 2 \\\\\n4 & -1\n\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix} + \\begin{bmatrix} -3 \\\\ 5 \\end{bmatrix}\\)\n\\(g\\left( \\begin{bmatrix}x \\\\ y\\end{bmatrix} \\right) =\n\\begin{bmatrix}3 & -2 \\\\ -1 & 5\n\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix} + \\begin{bmatrix} 0 \\\\ -4 \\end{bmatrix}\\)\n\\(h\\left( \\begin{bmatrix}x \\\\ y \\\\ z\\end{bmatrix} \\right) =\n\\begin{bmatrix}1 & 2 & -1 \\\\ 3 & -1 & 4 \\\\ -2 & 5 & 1\n\\end{bmatrix} \\begin{bmatrix}x \\\\ y \\\\ z\\end{bmatrix} + \\begin{bmatrix} 1 \\\\ -2 \\\\ 3 \\end{bmatrix}\\)\n\\(k\\left( \\begin{bmatrix}x \\\\ y\\end{bmatrix} \\right) =\n\\begin{bmatrix}1 & -4 \\\\ -2 & 3 \\\\ 3 & -1\n\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix} + \\begin{bmatrix} 2 \\\\ -1 \\\\ 0 \\end{bmatrix}\\)\n\\(r(x,y) = (x + 4y - 3, -2x + 3y + 2)\\)\n\\(s(x,y) = (5y + 1, -x + 2y - 4)\\)\n\\(t(x,y,z) = (2x - y + 3z, -4x + 5y +1, x + 2y - 2z - 1)\\)\n\\(u(x,y) = (x - 2y + 2, 3x - 1, -x + 4y + 3)\\)\n\n\n\n\n\n\n\nThe following exercise is meant to give you practice with the ideas in Exercise 1 of the slides.\n\n\n\n\n\n\nDescribe the action of the following functions \\(\\mathbb{R} \\to \\mathbb{R}\\) as physical transformations of the real line into itself. Draw pictures!\n\n\\(f(x) = -x^2\\)\n\\(g(x) = -2x + 2\\)\n\\(h(x) = e^x\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nThese are nearly impossible to plot on a computer. You’ll have to catch me after class and show me your sketches to check your work.\n\n\n\n\n\n\nThe following exercise is meant to give you practice with the ideas in Exercise 2 of the slides.\n\n\n\n\n\n\nDescribe the action of the following functions \\(\\mathbb{R} \\to \\mathbb{R}^2\\) as physical transformations of the real line into the plane. Draw pictures!\n\n\\(f(t) = (-t^2, t)\\)\n\\(g(t) = (-2t + 2, 3t)\\)\n\\(h(t) = (e^t, t^2)\\)\n\\(k(t) = (\\cos{t}, 2\\sin{t})\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nDesmos\nDesmos\nDesmos\nDesmos\n\n\n\n\n\n\n\nThe following exercise is meant to give you practice with the ideas in Exercises 4-6 of the slides.\n\n\n\n\n\n\nDescribe the action of the following functions \\(\\mathbb{R}^2 \\to \\mathbb{R}^2\\) as physical transformations of the plane into itself. Draw pictures!\n\n\\(f(x, y) = (-x + 1, x+y)\\)\n\\(g(x, y) = (x, e^x + y)\\)\n\\(h(x,y) = (xy, x)\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/07ex-functions-of-multiple-variables-part-3.html#exercise-1-matrixvector-representations-of-linear-functions",
    "href": "teaching/multi-calc-sp-26/exercises/07ex-functions-of-multiple-variables-part-3.html#exercise-1-matrixvector-representations-of-linear-functions",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Consider the linear function \\(f: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) given by\n\\[\nf(x,y) = (2x - y + 4, x + 3y - 1).\n\\]\nAs I mentioned in class, this is called a linear function because it only contains terms of degrees \\(0\\) and \\(1\\).\nIf we blur the distinction between a point \\((x,y)\\) and its position vector\n\\[\n\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix},\n\\]\nthen we can rewrite the formula for \\(f\\) as\n\\[\nf\\left( \\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix} \\right) =\n\\begin{bmatrix}\n2 & -1 \\\\\n1 & 3\n\\end{bmatrix} \\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix} + \\begin{bmatrix} 4 \\\\ -1 \\end{bmatrix}.\n\\]\nCheck for yourself that if you multiply out the right-hand side, you will get\n\\[\nf\\left( \\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix} \\right) = \\begin{bmatrix}2x - y + 4 \\\\\nx + 3y - 1\n\\end{bmatrix},\n\\]\nwhich is the same as the original formula for \\(f\\), but written in a different way.\nThe same procedure works in higher dimensions, as you will discover in the following exercises:\n\n\n\n\n\n\nIn each of the following, convert the given formula for a linear function into a matrix/vector representation as shown above.\n\n\\(f: \\mathbb{R}^2 \\to \\mathbb{R}^2\\), \\(f(x,y) = (x + 2y - 3, 4x - y + 5)\\)\n\\(g: \\mathbb{R}^2 \\to \\mathbb{R}^2\\), \\(g(x,y) = (3x - 2y , -x + 5y - 4)\\)\n\\(h: \\mathbb{R}^3 \\to \\mathbb{R}^3\\), \\(h(x, y, z) = (x + 2y - z + 1, 3x - y + 4z - 2, -2x + 5y + z + 3)\\)\n\\(k: \\mathbb{R}^2 \\to \\mathbb{R}^3\\), \\(k(x, y) = (x - 4y + 2, -2x + 3y - 1, 3x - y)\\)\n\nNow go backwards. For each of the following, convert the given matrix/vector representation into a formula for a linear function.\n\n\\(r: \\mathbb{R}^2 \\to \\mathbb{R}^2\\), \\(r\\left( \\begin{bmatrix}x \\\\\ny\n\\end{bmatrix} \\right) =\n\\begin{bmatrix}1 & 4 \\\\\n-2 & 3\n\\end{bmatrix} \\begin{bmatrix}x \\\\\ny\n\\end{bmatrix} + \\begin{bmatrix} -3 \\\\ 2 \\end{bmatrix}\\)\n\\(s: \\mathbb{R}^2 \\to \\mathbb{R}^2\\), \\(s\\left( \\begin{bmatrix}x \\\\\ny\n\\end{bmatrix} \\right) =\n\\begin{bmatrix}0 & 5 \\\\\n-1 & 2\n\\end{bmatrix} \\begin{bmatrix}x \\\\\ny\n\\end{bmatrix} + \\begin{bmatrix} 1 \\\\ -4 \\end{bmatrix}\\)\n\\(t: \\mathbb{R}^3 \\to \\mathbb{R}^3\\), \\(t\\left( \\begin{bmatrix}x \\\\\ny \\\\\nz\n\\end{bmatrix} \\right) =\n\\begin{bmatrix}2 & -1 & 3 \\\\\n-4 & 5 & 0 \\\\\n1 & 2 & -2\n\\end{bmatrix} \\begin{bmatrix}x \\\\\ny \\\\\nz\n\\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix}\\)\n\\(u: \\mathbb{R}^2 \\to \\mathbb{R}^3\\), \\(u\\left( \\begin{bmatrix}x \\\\ y\\end{bmatrix} \\right) =\n\\begin{bmatrix}1 & -2 \\\\\n3 & 0 \\\\\n-1 & 4\n\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix} + \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix}\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(f\\left( \\begin{bmatrix}x \\\\ y\\end{bmatrix} \\right) =\n\\begin{bmatrix}1 & 2 \\\\\n4 & -1\n\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix} + \\begin{bmatrix} -3 \\\\ 5 \\end{bmatrix}\\)\n\\(g\\left( \\begin{bmatrix}x \\\\ y\\end{bmatrix} \\right) =\n\\begin{bmatrix}3 & -2 \\\\ -1 & 5\n\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix} + \\begin{bmatrix} 0 \\\\ -4 \\end{bmatrix}\\)\n\\(h\\left( \\begin{bmatrix}x \\\\ y \\\\ z\\end{bmatrix} \\right) =\n\\begin{bmatrix}1 & 2 & -1 \\\\ 3 & -1 & 4 \\\\ -2 & 5 & 1\n\\end{bmatrix} \\begin{bmatrix}x \\\\ y \\\\ z\\end{bmatrix} + \\begin{bmatrix} 1 \\\\ -2 \\\\ 3 \\end{bmatrix}\\)\n\\(k\\left( \\begin{bmatrix}x \\\\ y\\end{bmatrix} \\right) =\n\\begin{bmatrix}1 & -4 \\\\ -2 & 3 \\\\ 3 & -1\n\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix} + \\begin{bmatrix} 2 \\\\ -1 \\\\ 0 \\end{bmatrix}\\)\n\\(r(x,y) = (x + 4y - 3, -2x + 3y + 2)\\)\n\\(s(x,y) = (5y + 1, -x + 2y - 4)\\)\n\\(t(x,y,z) = (2x - y + 3z, -4x + 5y +1, x + 2y - 2z - 1)\\)\n\\(u(x,y) = (x - 2y + 2, 3x - 1, -x + 4y + 3)\\)"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/07ex-functions-of-multiple-variables-part-3.html#exercise-2-functions-mathbbr-to-mathbbr-as-transformations",
    "href": "teaching/multi-calc-sp-26/exercises/07ex-functions-of-multiple-variables-part-3.html#exercise-2-functions-mathbbr-to-mathbbr-as-transformations",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "The following exercise is meant to give you practice with the ideas in Exercise 1 of the slides.\n\n\n\n\n\n\nDescribe the action of the following functions \\(\\mathbb{R} \\to \\mathbb{R}\\) as physical transformations of the real line into itself. Draw pictures!\n\n\\(f(x) = -x^2\\)\n\\(g(x) = -2x + 2\\)\n\\(h(x) = e^x\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nThese are nearly impossible to plot on a computer. You’ll have to catch me after class and show me your sketches to check your work."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/07ex-functions-of-multiple-variables-part-3.html#exercise-3-functions-mathbbr-to-mathbbr2-as-transformations",
    "href": "teaching/multi-calc-sp-26/exercises/07ex-functions-of-multiple-variables-part-3.html#exercise-3-functions-mathbbr-to-mathbbr2-as-transformations",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "The following exercise is meant to give you practice with the ideas in Exercise 2 of the slides.\n\n\n\n\n\n\nDescribe the action of the following functions \\(\\mathbb{R} \\to \\mathbb{R}^2\\) as physical transformations of the real line into the plane. Draw pictures!\n\n\\(f(t) = (-t^2, t)\\)\n\\(g(t) = (-2t + 2, 3t)\\)\n\\(h(t) = (e^t, t^2)\\)\n\\(k(t) = (\\cos{t}, 2\\sin{t})\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nDesmos\nDesmos\nDesmos\nDesmos"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/07ex-functions-of-multiple-variables-part-3.html#exercise-4-functions-mathbbr2-to-mathbbr2-as-transformations",
    "href": "teaching/multi-calc-sp-26/exercises/07ex-functions-of-multiple-variables-part-3.html#exercise-4-functions-mathbbr2-to-mathbbr2-as-transformations",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "The following exercise is meant to give you practice with the ideas in Exercises 4-6 of the slides.\n\n\n\n\n\n\nDescribe the action of the following functions \\(\\mathbb{R}^2 \\to \\mathbb{R}^2\\) as physical transformations of the plane into itself. Draw pictures!\n\n\\(f(x, y) = (-x + 1, x+y)\\)\n\\(g(x, y) = (x, e^x + y)\\)\n\\(h(x,y) = (xy, x)\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/02ex-vectors-and-matrices-part-2.html",
    "href": "teaching/multi-calc-sp-26/exercises/02ex-vectors-and-matrices-part-2.html",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "In class, we learned that if \\(A\\) and \\(B\\) are matrices, then the product \\(AB\\) is defined if the number of columns of \\(A\\) equals the number of rows of \\(B\\). In other words, if \\(A\\) is \\(m\\times n\\) and \\(B\\) is \\(n\\times p\\), then \\(AB\\) is defined and is an \\(m\\times p\\) matrix.\nIf the \\((i,j)\\)-th entry of \\(A\\) is \\(a_{ij}\\), and the \\((j,k)\\)-th entry of \\(B\\) is \\(b_{jk}\\), then the \\((i,k)\\)-th entry of \\(AB\\) is given by\n\\[\n\\sum_{j=1}^n a_{ij} b_{jk}.\n\\]\n\n\n\n\n\n\nMultiply the following matrices, if possible. If the multiplication is not defined, explain why not.\n\n\\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\\)\n\\(C = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\) and \\(D = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}\\)\n\\(E = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 2 & 3 \\end{bmatrix}\\) and \\(F = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\)\n\\(G = \\begin{bmatrix} 2 & -1 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(H = \\begin{bmatrix} 1 & 0 & 2 \\\\ -1 & 3 & 1 \\end{bmatrix}\\)\n\\(J = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}\\) and \\(K = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nSince \\(A\\) is \\(2 \\times 2\\) and \\(B\\) is \\(2 \\times 2\\), the product \\(AB\\) is defined and will be \\(2 \\times 2\\): \\[\nAB = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n\\]\nSince \\(C\\) is \\(2 \\times 3\\) and \\(D\\) is \\(3 \\times 2\\), the product \\(CD\\) is defined and will be \\(2 \\times 2\\): \\[\nCD = \\begin{bmatrix} 22 & 28 \\\\ 49 & 64 \\end{bmatrix}\n\\]\nSince \\(E\\) is \\(3 \\times 2\\) and \\(F\\) is \\(2 \\times 3\\), the product \\(EF\\) is defined and will be \\(3 \\times 3\\): \\[\nEF = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 14 & 19 & 24 \\end{bmatrix}\n\\]\nSince \\(G\\) is \\(2 \\times 2\\) and \\(H\\) is \\(2 \\times 3\\), the product \\(GH\\) is defined and will be \\(2 \\times 3\\): \\[\nGH = \\begin{bmatrix} 3 & -3 & 3 \\\\ -1 & 12 & 10 \\end{bmatrix}\n\\]\nSince \\(J\\) is \\(3 \\times 2\\) and \\(K\\) is \\(2 \\times 3\\), the product \\(JK\\) is defined and will be \\(3 \\times 3\\): \\[\nJK =  \\begin{bmatrix} 9 & 12 & 15 \\\\ 19 & 26 & 33 \\\\ 29 & 40 & 51 \\end{bmatrix}\n\\]\n\n\n\n\n\n\n\nIn this exercise, we will work with vectors in \\(\\mathbb{R}^4\\) and \\(\\mathbb{R}^5\\) to get comfortable with higher-dimensional vector algebra. All the rules we learned for vectors in \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\) extend naturally to higher dimensions.\nFor vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(\\mathbb{R}^n\\), recall that:\n\nVector addition and subtraction are performed componentwise.\nScalar multiplication is performed by multiplying each component by the scalar.\nThe dot product (inner product) is \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = u_1v_1 + u_2v_2 + \\cdots + u_nv_n\\).\nThe norm (length) of \\(\\mathbf{u}\\) is \\(\\|\\mathbf{u}\\| = \\sqrt{\\langle \\mathbf{u}, \\mathbf{u} \\rangle}\\).\n\n\n\n\n\n\n\nLet\n\\[\n\\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{v} = \\begin{bmatrix} 0 \\\\ -1 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad \\text{and} \\quad \\mathbf{w} = \\begin{bmatrix} 2 \\\\ 0 \\\\ 1 \\\\ -2 \\end{bmatrix}\n\\]\nbe vectors in \\(\\mathbb{R}^4\\).\n\nCompute \\(\\mathbf{u} + \\mathbf{v}\\).\nCompute \\(2\\mathbf{u} - 3\\mathbf{v}\\).\nCompute \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle\\).\nCompute \\(\\|\\mathbf{u}\\|\\) and \\(\\|\\mathbf{v}\\|\\).\nCompute \\(\\mathbf{u} + \\mathbf{v} + \\mathbf{w}\\).\nAre \\(\\mathbf{u}\\) and \\(\\mathbf{w}\\) orthogonal? Explain.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 4 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 2 \\\\ 7 \\\\ -8 \\\\ 3 \\end{bmatrix}\\)\n\\(-1\\)\n\\(\\sqrt{15}\\), \\(\\sqrt{6}\\)\n\\(\\begin{bmatrix} 3 \\\\ 1 \\\\ 2 \\\\ 2 \\end{bmatrix}\\)\nTo check if \\(\\mathbf{u}\\) and \\(\\mathbf{w}\\) are orthogonal, we compute their dot product: \\[\n\\langle \\mathbf{u}, \\mathbf{w} \\rangle = (1)(2) + (2)(0) + (-1)(1) + (3)(-2) = 2 + 0 - 1 - 6 = -5.\n\\] Since the dot product is not zero, \\(\\mathbf{u}\\) and \\(\\mathbf{w}\\) are not orthogonal.\n\n\n\n\n\n\n\nRecall that the standard basis vectors in \\(\\mathbb{R}^n\\) are the vectors \\(\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n\\), where \\(\\mathbf{e}_i\\) has a 1 in the \\(i\\)-th position and 0s elsewhere. For example, in \\(\\mathbb{R}^4\\),\n\\[\n\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_4 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n\\]\nAny vector\n\\[\n\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ v_4 \\end{bmatrix}\n\\]\nin \\(\\mathbb{R}^4\\) can be written as a linear combination of the standard basis vectors:\n\\[\n\\mathbf{v} = v_1\\mathbf{e}_1 + v_2\\mathbf{e}_2 + v_3\\mathbf{e}_3 + v_4\\mathbf{e}_4.\n\\]\nThis was called resolving a vector into the standard basis.\n\n\n\n\n\n\n\nResolve the vector \\[\n\\mathbf{v} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 5 \\\\ 1 \\end{bmatrix}\n\\] as a linear combination of the standard basis vectors in \\(\\mathbb{R}^4\\).\nLet \\(\\mathbf{w} = 2\\mathbf{e}_1 - 4\\mathbf{e}_2 + \\mathbf{e}_3 - 3\\mathbf{e}_4\\). Write \\(\\mathbf{w}\\) in column vector form.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\mathbf{v} = 3\\mathbf{e}_1 - 2\\mathbf{e}_2 + 5\\mathbf{e}_3 + \\mathbf{e}_4\\)\n\\(\\mathbf{w} = \\begin{bmatrix} 2 \\\\ -4 \\\\ 1 \\\\ -3 \\end{bmatrix}\\)\n\n\n\n\n\n\n\nIn class, we learned that every \\(m \\times n\\) matrix \\(A\\) defines a function \\(T:\\mathbb{R}^n \\to \\mathbb{R}^m\\) by the rule \\(T(\\mathbf{v}) = A\\mathbf{v}\\). This means that \\(T\\) takes an \\(n\\)-dimensional vector as input and produces an \\(m\\)-dimensional vector as output.\n\n\n\n\n\n\nLet \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}\\).\n\nWhat is the domain of the function \\(T\\) defined by \\(T(\\mathbf{v}) = A\\mathbf{v}\\)? What is the codomain?\nCompute \\(T\\left(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\right)\\).\nCompute \\(T\\left(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\right)\\).\nCompute \\(T\\left(\\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}\\right)\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe matrix \\(A\\) is \\(3 \\times 2\\), so it multiplies vectors in \\(\\mathbb{R}^2\\) and produces vectors in \\(\\mathbb{R}^3\\). Therefore, the domain of \\(T\\) is \\(\\mathbb{R}^2\\) and the codomain is \\(\\mathbb{R}^3\\). We write \\(T:\\mathbb{R}^2 \\to \\mathbb{R}^3\\).\n\\(T\\left(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\right) = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} (1)(1) + (2)(0) \\\\ (3)(1) + (4)(0) \\\\ (5)(1) + (6)(0) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\\\ 5 \\end{bmatrix}\\)\n\\(T\\left(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\right) = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} (1)(0) + (2)(1) \\\\ (3)(0) + (4)(1) \\\\ (5)(0) + (6)(1) \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\end{bmatrix}\\)\n\\(T\\left(\\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}\\right) = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} (1)(2) + (2)(-1) \\\\ (3)(2) + (4)(-1) \\\\ (5)(2) + (6)(-1) \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 2 \\\\ 4 \\end{bmatrix}\\)\n\n\n\n\n\n\n\nThe determinant of a \\(2 \\times 2\\) matrix \\(A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) is defined as\n\\[\n\\det(A) = ad - bc.\n\\]\nThe determinant of a \\(3 \\times 3\\) matrix \\(A = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix}\\) is defined as\n\\[\n\\det(A) = a(ei - fh) - b(di - fg) + c(dh - eg).\n\\]\n\n\n\n\n\n\n\nCompute the determinant of \\(A = \\begin{bmatrix} 3 & 5 \\\\ 2 & 4 \\end{bmatrix}\\).\nCompute the determinant of \\(B = \\begin{bmatrix} -1 & 2 \\\\ 3 & 6 \\end{bmatrix}\\).\nCompute the determinant of \\(C = \\begin{bmatrix} 1 & 0 & 2 \\\\ -1 & 3 & 1 \\\\ 2 & 4 & -2 \\end{bmatrix}\\).\nCompute the determinant of \\(D = \\begin{bmatrix} 2 & 1 & 3 \\\\ 0 & 4 & 1 \\\\ 0 & 0 & 5 \\end{bmatrix}\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\det(A) = (3)(4) - (5)(2) = 12 - 10 = 2\\)\n\\(\\det(B) = (-1)(6) - (2)(3) = -6 - 6 = -12\\)\nUsing the formula: \\[\n\\begin{align*}\n\\det(C) &= 1[(3)(-2) - (1)(4)] - 0 + 2[(-1)(4) - (3)(2)]\\\\\n&= 1[-6 - 4] + 2[-4 - 6]\\\\\n&= -10 + 2(-10) = -10 - 20 \\\\\n&= -30\n\\end{align*}\n\\]\nUsing the formula: \\[\n\\begin{align*}\n\\det(D) = &= 2[(4)(5) - (1)(0)] - 1[(0)(5) - (1)(0)] + 3[(0)(0) - (4)(0)] \\\\\n&= 2(20) - 1(0) + 3(0) \\\\\n&= 40 \\\\\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nThe absolute value of the determinant of a \\(2 \\times 2\\) matrix gives the area of the parallelogram formed by its column vectors. Similarly, the absolute value of the determinant of a \\(3 \\times 3\\) matrix gives the volume of the parallelepiped (a 3D generalization of a parallelogram) formed by its column vectors.\n\n\n\n\n\n\n\nFind the area of the parallelogram with vertices at \\((0, 0)\\), \\((3, 1)\\), and \\((4, 5)\\).\nFind the area of the parallelogram formed by the vectors \\[\n\\mathbf{v}_1 = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix}.\n\\]\nFind the volume of the parallelepiped with one vertex at the origin and vertices including \\((2, 0, 0)\\), \\((1, 3, 0)\\), and \\((1, 1, 2)\\).\nFind the volume of the parallelepiped formed by the vectors \\[\n\\mathbf{u}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{u}_2 = \\begin{bmatrix} 0 \\\\ 2 \\\\ 1 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{u}_3 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}.\n\\]\nDetermine whether the vectors \\[\n\\mathbf{a} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{b} = \\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\end{bmatrix}\\quad \\text{and} \\quad \\mathbf{c} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}\n\\] lie in the same plane.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe parallelogram has sides from \\((0,0)\\) to \\((3,1)\\) and from \\((0,0)\\) to \\((4,5)\\). The area is the absolute value of the determinant: \\[\n\\left|\\det\\begin{bmatrix} 3 & 4 \\\\ 1 & 5 \\end{bmatrix}\\right| = |(3)(5) - (4)(1)| = |15 - 4| = 11\n\\]\nThe area is: \\[\n\\left|\\det\\begin{bmatrix} 2 & 1 \\\\ 3 & 4 \\end{bmatrix}\\right| = |(2)(4) - (1)(3)| = |8 - 3| = 5\n\\]\nThe volume is the absolute value of the determinant of the matrix with these vectors as columns: \\[\n\\left|\\det\\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & 3 & 1 \\\\ 0 & 0 & 2 \\end{bmatrix}\\right| = 12.\n\\]\nThe volume is: \\[\n\\left|\\det\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 2 & 1 \\\\ 1 & 1 & 0 \\end{bmatrix}\\right| = 3\n\\]\nTo check if the vectors are coplanar, we check if the determinant of the matrix with these vectors as columns is zero: \\[\n\\det\\begin{bmatrix} 1 & 2 & 1 \\\\ 2 & 4 & 0 \\\\ 3 & 6 & 1 \\end{bmatrix} = 0\n\\] Since the determinant is zero, the vectors lie in the same plane."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/02ex-vectors-and-matrices-part-2.html#exercise-1-more-practice-with-matrix-multiplication",
    "href": "teaching/multi-calc-sp-26/exercises/02ex-vectors-and-matrices-part-2.html#exercise-1-more-practice-with-matrix-multiplication",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "In class, we learned that if \\(A\\) and \\(B\\) are matrices, then the product \\(AB\\) is defined if the number of columns of \\(A\\) equals the number of rows of \\(B\\). In other words, if \\(A\\) is \\(m\\times n\\) and \\(B\\) is \\(n\\times p\\), then \\(AB\\) is defined and is an \\(m\\times p\\) matrix.\nIf the \\((i,j)\\)-th entry of \\(A\\) is \\(a_{ij}\\), and the \\((j,k)\\)-th entry of \\(B\\) is \\(b_{jk}\\), then the \\((i,k)\\)-th entry of \\(AB\\) is given by\n\\[\n\\sum_{j=1}^n a_{ij} b_{jk}.\n\\]\n\n\n\n\n\n\nMultiply the following matrices, if possible. If the multiplication is not defined, explain why not.\n\n\\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\\)\n\\(C = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\) and \\(D = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}\\)\n\\(E = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 2 & 3 \\end{bmatrix}\\) and \\(F = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\)\n\\(G = \\begin{bmatrix} 2 & -1 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(H = \\begin{bmatrix} 1 & 0 & 2 \\\\ -1 & 3 & 1 \\end{bmatrix}\\)\n\\(J = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}\\) and \\(K = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nSince \\(A\\) is \\(2 \\times 2\\) and \\(B\\) is \\(2 \\times 2\\), the product \\(AB\\) is defined and will be \\(2 \\times 2\\): \\[\nAB = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n\\]\nSince \\(C\\) is \\(2 \\times 3\\) and \\(D\\) is \\(3 \\times 2\\), the product \\(CD\\) is defined and will be \\(2 \\times 2\\): \\[\nCD = \\begin{bmatrix} 22 & 28 \\\\ 49 & 64 \\end{bmatrix}\n\\]\nSince \\(E\\) is \\(3 \\times 2\\) and \\(F\\) is \\(2 \\times 3\\), the product \\(EF\\) is defined and will be \\(3 \\times 3\\): \\[\nEF = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 14 & 19 & 24 \\end{bmatrix}\n\\]\nSince \\(G\\) is \\(2 \\times 2\\) and \\(H\\) is \\(2 \\times 3\\), the product \\(GH\\) is defined and will be \\(2 \\times 3\\): \\[\nGH = \\begin{bmatrix} 3 & -3 & 3 \\\\ -1 & 12 & 10 \\end{bmatrix}\n\\]\nSince \\(J\\) is \\(3 \\times 2\\) and \\(K\\) is \\(2 \\times 3\\), the product \\(JK\\) is defined and will be \\(3 \\times 3\\): \\[\nJK =  \\begin{bmatrix} 9 & 12 & 15 \\\\ 19 & 26 & 33 \\\\ 29 & 40 & 51 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/02ex-vectors-and-matrices-part-2.html#exercise-2-vectors-in-higher-dimensions",
    "href": "teaching/multi-calc-sp-26/exercises/02ex-vectors-and-matrices-part-2.html#exercise-2-vectors-in-higher-dimensions",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "In this exercise, we will work with vectors in \\(\\mathbb{R}^4\\) and \\(\\mathbb{R}^5\\) to get comfortable with higher-dimensional vector algebra. All the rules we learned for vectors in \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\) extend naturally to higher dimensions.\nFor vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(\\mathbb{R}^n\\), recall that:\n\nVector addition and subtraction are performed componentwise.\nScalar multiplication is performed by multiplying each component by the scalar.\nThe dot product (inner product) is \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = u_1v_1 + u_2v_2 + \\cdots + u_nv_n\\).\nThe norm (length) of \\(\\mathbf{u}\\) is \\(\\|\\mathbf{u}\\| = \\sqrt{\\langle \\mathbf{u}, \\mathbf{u} \\rangle}\\).\n\n\n\n\n\n\n\nLet\n\\[\n\\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{v} = \\begin{bmatrix} 0 \\\\ -1 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad \\text{and} \\quad \\mathbf{w} = \\begin{bmatrix} 2 \\\\ 0 \\\\ 1 \\\\ -2 \\end{bmatrix}\n\\]\nbe vectors in \\(\\mathbb{R}^4\\).\n\nCompute \\(\\mathbf{u} + \\mathbf{v}\\).\nCompute \\(2\\mathbf{u} - 3\\mathbf{v}\\).\nCompute \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle\\).\nCompute \\(\\|\\mathbf{u}\\|\\) and \\(\\|\\mathbf{v}\\|\\).\nCompute \\(\\mathbf{u} + \\mathbf{v} + \\mathbf{w}\\).\nAre \\(\\mathbf{u}\\) and \\(\\mathbf{w}\\) orthogonal? Explain.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 4 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 2 \\\\ 7 \\\\ -8 \\\\ 3 \\end{bmatrix}\\)\n\\(-1\\)\n\\(\\sqrt{15}\\), \\(\\sqrt{6}\\)\n\\(\\begin{bmatrix} 3 \\\\ 1 \\\\ 2 \\\\ 2 \\end{bmatrix}\\)\nTo check if \\(\\mathbf{u}\\) and \\(\\mathbf{w}\\) are orthogonal, we compute their dot product: \\[\n\\langle \\mathbf{u}, \\mathbf{w} \\rangle = (1)(2) + (2)(0) + (-1)(1) + (3)(-2) = 2 + 0 - 1 - 6 = -5.\n\\] Since the dot product is not zero, \\(\\mathbf{u}\\) and \\(\\mathbf{w}\\) are not orthogonal."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/02ex-vectors-and-matrices-part-2.html#exercise-3-standard-basis-vectors-in-higher-dimensions",
    "href": "teaching/multi-calc-sp-26/exercises/02ex-vectors-and-matrices-part-2.html#exercise-3-standard-basis-vectors-in-higher-dimensions",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall that the standard basis vectors in \\(\\mathbb{R}^n\\) are the vectors \\(\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n\\), where \\(\\mathbf{e}_i\\) has a 1 in the \\(i\\)-th position and 0s elsewhere. For example, in \\(\\mathbb{R}^4\\),\n\\[\n\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_4 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n\\]\nAny vector\n\\[\n\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ v_4 \\end{bmatrix}\n\\]\nin \\(\\mathbb{R}^4\\) can be written as a linear combination of the standard basis vectors:\n\\[\n\\mathbf{v} = v_1\\mathbf{e}_1 + v_2\\mathbf{e}_2 + v_3\\mathbf{e}_3 + v_4\\mathbf{e}_4.\n\\]\nThis was called resolving a vector into the standard basis.\n\n\n\n\n\n\n\nResolve the vector \\[\n\\mathbf{v} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 5 \\\\ 1 \\end{bmatrix}\n\\] as a linear combination of the standard basis vectors in \\(\\mathbb{R}^4\\).\nLet \\(\\mathbf{w} = 2\\mathbf{e}_1 - 4\\mathbf{e}_2 + \\mathbf{e}_3 - 3\\mathbf{e}_4\\). Write \\(\\mathbf{w}\\) in column vector form.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\mathbf{v} = 3\\mathbf{e}_1 - 2\\mathbf{e}_2 + 5\\mathbf{e}_3 + \\mathbf{e}_4\\)\n\\(\\mathbf{w} = \\begin{bmatrix} 2 \\\\ -4 \\\\ 1 \\\\ -3 \\end{bmatrix}\\)"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/02ex-vectors-and-matrices-part-2.html#exercise-4-matrix-vector-multiplication-as-a-function",
    "href": "teaching/multi-calc-sp-26/exercises/02ex-vectors-and-matrices-part-2.html#exercise-4-matrix-vector-multiplication-as-a-function",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "In class, we learned that every \\(m \\times n\\) matrix \\(A\\) defines a function \\(T:\\mathbb{R}^n \\to \\mathbb{R}^m\\) by the rule \\(T(\\mathbf{v}) = A\\mathbf{v}\\). This means that \\(T\\) takes an \\(n\\)-dimensional vector as input and produces an \\(m\\)-dimensional vector as output.\n\n\n\n\n\n\nLet \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}\\).\n\nWhat is the domain of the function \\(T\\) defined by \\(T(\\mathbf{v}) = A\\mathbf{v}\\)? What is the codomain?\nCompute \\(T\\left(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\right)\\).\nCompute \\(T\\left(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\right)\\).\nCompute \\(T\\left(\\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}\\right)\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe matrix \\(A\\) is \\(3 \\times 2\\), so it multiplies vectors in \\(\\mathbb{R}^2\\) and produces vectors in \\(\\mathbb{R}^3\\). Therefore, the domain of \\(T\\) is \\(\\mathbb{R}^2\\) and the codomain is \\(\\mathbb{R}^3\\). We write \\(T:\\mathbb{R}^2 \\to \\mathbb{R}^3\\).\n\\(T\\left(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\right) = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} (1)(1) + (2)(0) \\\\ (3)(1) + (4)(0) \\\\ (5)(1) + (6)(0) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\\\ 5 \\end{bmatrix}\\)\n\\(T\\left(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\right) = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} (1)(0) + (2)(1) \\\\ (3)(0) + (4)(1) \\\\ (5)(0) + (6)(1) \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\end{bmatrix}\\)\n\\(T\\left(\\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}\\right) = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} (1)(2) + (2)(-1) \\\\ (3)(2) + (4)(-1) \\\\ (5)(2) + (6)(-1) \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 2 \\\\ 4 \\end{bmatrix}\\)"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/02ex-vectors-and-matrices-part-2.html#exercise-5-computing-determinants",
    "href": "teaching/multi-calc-sp-26/exercises/02ex-vectors-and-matrices-part-2.html#exercise-5-computing-determinants",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "The determinant of a \\(2 \\times 2\\) matrix \\(A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) is defined as\n\\[\n\\det(A) = ad - bc.\n\\]\nThe determinant of a \\(3 \\times 3\\) matrix \\(A = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix}\\) is defined as\n\\[\n\\det(A) = a(ei - fh) - b(di - fg) + c(dh - eg).\n\\]\n\n\n\n\n\n\n\nCompute the determinant of \\(A = \\begin{bmatrix} 3 & 5 \\\\ 2 & 4 \\end{bmatrix}\\).\nCompute the determinant of \\(B = \\begin{bmatrix} -1 & 2 \\\\ 3 & 6 \\end{bmatrix}\\).\nCompute the determinant of \\(C = \\begin{bmatrix} 1 & 0 & 2 \\\\ -1 & 3 & 1 \\\\ 2 & 4 & -2 \\end{bmatrix}\\).\nCompute the determinant of \\(D = \\begin{bmatrix} 2 & 1 & 3 \\\\ 0 & 4 & 1 \\\\ 0 & 0 & 5 \\end{bmatrix}\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\det(A) = (3)(4) - (5)(2) = 12 - 10 = 2\\)\n\\(\\det(B) = (-1)(6) - (2)(3) = -6 - 6 = -12\\)\nUsing the formula: \\[\n\\begin{align*}\n\\det(C) &= 1[(3)(-2) - (1)(4)] - 0 + 2[(-1)(4) - (3)(2)]\\\\\n&= 1[-6 - 4] + 2[-4 - 6]\\\\\n&= -10 + 2(-10) = -10 - 20 \\\\\n&= -30\n\\end{align*}\n\\]\nUsing the formula: \\[\n\\begin{align*}\n\\det(D) = &= 2[(4)(5) - (1)(0)] - 1[(0)(5) - (1)(0)] + 3[(0)(0) - (4)(0)] \\\\\n&= 2(20) - 1(0) + 3(0) \\\\\n&= 40 \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/02ex-vectors-and-matrices-part-2.html#exercise-6-determinants-and-geometric-interpretations",
    "href": "teaching/multi-calc-sp-26/exercises/02ex-vectors-and-matrices-part-2.html#exercise-6-determinants-and-geometric-interpretations",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "The absolute value of the determinant of a \\(2 \\times 2\\) matrix gives the area of the parallelogram formed by its column vectors. Similarly, the absolute value of the determinant of a \\(3 \\times 3\\) matrix gives the volume of the parallelepiped (a 3D generalization of a parallelogram) formed by its column vectors.\n\n\n\n\n\n\n\nFind the area of the parallelogram with vertices at \\((0, 0)\\), \\((3, 1)\\), and \\((4, 5)\\).\nFind the area of the parallelogram formed by the vectors \\[\n\\mathbf{v}_1 = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix}.\n\\]\nFind the volume of the parallelepiped with one vertex at the origin and vertices including \\((2, 0, 0)\\), \\((1, 3, 0)\\), and \\((1, 1, 2)\\).\nFind the volume of the parallelepiped formed by the vectors \\[\n\\mathbf{u}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{u}_2 = \\begin{bmatrix} 0 \\\\ 2 \\\\ 1 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{u}_3 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}.\n\\]\nDetermine whether the vectors \\[\n\\mathbf{a} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{b} = \\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\end{bmatrix}\\quad \\text{and} \\quad \\mathbf{c} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}\n\\] lie in the same plane.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe parallelogram has sides from \\((0,0)\\) to \\((3,1)\\) and from \\((0,0)\\) to \\((4,5)\\). The area is the absolute value of the determinant: \\[\n\\left|\\det\\begin{bmatrix} 3 & 4 \\\\ 1 & 5 \\end{bmatrix}\\right| = |(3)(5) - (4)(1)| = |15 - 4| = 11\n\\]\nThe area is: \\[\n\\left|\\det\\begin{bmatrix} 2 & 1 \\\\ 3 & 4 \\end{bmatrix}\\right| = |(2)(4) - (1)(3)| = |8 - 3| = 5\n\\]\nThe volume is the absolute value of the determinant of the matrix with these vectors as columns: \\[\n\\left|\\det\\begin{bmatrix} 2 & 1 & 1 \\\\ 0 & 3 & 1 \\\\ 0 & 0 & 2 \\end{bmatrix}\\right| = 12.\n\\]\nThe volume is: \\[\n\\left|\\det\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 2 & 1 \\\\ 1 & 1 & 0 \\end{bmatrix}\\right| = 3\n\\]\nTo check if the vectors are coplanar, we check if the determinant of the matrix with these vectors as columns is zero: \\[\n\\det\\begin{bmatrix} 1 & 2 & 1 \\\\ 2 & 4 & 0 \\\\ 3 & 6 & 1 \\end{bmatrix} = 0\n\\] Since the determinant is zero, the vectors lie in the same plane."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/05-functions-of-multiple-variables-part-1.html#exercise-1-a-real-estate-function",
    "href": "teaching/multi-calc-sp-26/slides/05-functions-of-multiple-variables-part-1.html#exercise-1-a-real-estate-function",
    "title": "05 Functions of multiple variables, part 1",
    "section": "Exercise 1: A real estate function",
    "text": "Exercise 1: A real estate function\n\n\n\nThe sale price, \\(s\\), and annual property tax, \\(t\\), (both measured in thousands of dollars) are jointly a function \\((s,t) = f(q,b,a)\\) of three variables:\n\n\\(q\\), the square footage of the house in hundreds of feet,\n\\(b\\), the number of bedrooms,\n\\(a\\), the age of the house in years.\n\nThe formula for the function \\(f\\) has been determined to be\n\\[\nf(q,b,a) = (100 + 2q+10b-a, 1+ 0.01q + 0.05).\n\\]\n\nWhat is the sale price and annual property tax for a house with \\(2000\\) square feet, \\(3\\) bedrooms, and \\(5\\) years of age?\nAssuming (unrealisticly) that \\(q\\), \\(b\\), and \\(a\\) can take on any values, express the domain and codomain of the function \\(f\\) in the form \\(f: \\mathbb{R}^m \\to \\mathbb{R}^n\\).\nIf you could draw the graph of \\(f\\), in how many ambient dimensions would it be drawn?"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/05-functions-of-multiple-variables-part-1.html#exercise-2-a-manufacturing-function",
    "href": "teaching/multi-calc-sp-26/slides/05-functions-of-multiple-variables-part-1.html#exercise-2-a-manufacturing-function",
    "title": "05 Functions of multiple variables, part 1",
    "section": "Exercise 2: A manufacturing function",
    "text": "Exercise 2: A manufacturing function\n\n\n\nThe number of units produced, \\(u\\), the total cost of production, \\(c\\) (in dollars), and the totel energy consumed, \\(k\\) (in kWH), are jointly a function \\((u,c,k) = g(r,h,m,e)\\) of four variables:\n\n\\(r\\), the production rate in units per hour,\n\\(h\\), the number of hours worked,\n\\(m\\), the amount of raw materials used in kilograms,\n\\(e\\), the equipment maintenance time in hours.\n\nThe formula for the function \\(g\\) has been determined to be\n\\[\ng(r,h,m,e) = (r(h-e), 20h+5m+50e, 10h+5e).\n\\]\n\nWhat is the number of units produced, total cost of production, and total energy consumed for a production rate of \\(100\\) units per hour, \\(8\\) hours worked, \\(50\\) kilograms of raw materials used, and \\(2\\) hours of equipment maintenance time?\nAssuming (unrealisticly) that \\(r\\), \\(h\\), \\(m\\), and \\(e\\) can take on any values, express the domain and codomain of the function \\(g\\) in the form \\(g: \\mathbb{R}^m \\to \\mathbb{R}^n\\).\nIf you could draw the graph of \\(g\\), in how many ambient dimensions would it be drawn?"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/05-functions-of-multiple-variables-part-1.html#exercise-3-a-pure-mathematical-function",
    "href": "teaching/multi-calc-sp-26/slides/05-functions-of-multiple-variables-part-1.html#exercise-3-a-pure-mathematical-function",
    "title": "05 Functions of multiple variables, part 1",
    "section": "Exercise 3: A pure mathematical function",
    "text": "Exercise 3: A pure mathematical function\n\n\n\nConsider the function \\(h:\\mathbb{R}^4 \\to \\mathbb{R}^3\\) defined by\n\\[\nh(x,y,z,w) = \\big(x^2y - \\sin{z}, e^w - 15y, (z+w)^2\\big).\n\\]\n\nWhat is \\(h(-1,0,2,3)\\)?\nIf you could draw the graph of \\(h\\), in how many ambient dimensions would it be drawn?"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/05-functions-of-multiple-variables-part-1.html#functions-of-multiple-variables",
    "href": "teaching/multi-calc-sp-26/slides/05-functions-of-multiple-variables-part-1.html#functions-of-multiple-variables",
    "title": "05 Functions of multiple variables, part 1",
    "section": "Functions of multiple variables",
    "text": "Functions of multiple variables\n\n\n\n\nFunctions of multiple variables\n\n\nA multivariable function is a function of the form \\(f: \\mathbb{R}^m \\to \\mathbb{R}^n\\) for some positive integers \\(m\\) and \\(n\\).\n\n\n\n\n\nThe functions you are familiar with are the simple ones of the form \\(f: \\mathbb{R} \\to \\mathbb{R}\\). One input, one output.\nNow we may have \\(m\\) inputs and \\(n\\) outputs.\nFunctions of the form \\(f: \\mathbb{R}^m \\to \\mathbb{R}\\) are often called real-valued functions.\nThe graph of a function \\(f: \\mathbb{R}^m \\to \\mathbb{R}^n\\) lives inside of \\(\\mathbb{R}^{m+n}\\). We cannot see this graph unless \\(m+n \\leq 3\\):\n\nWe can see the graphs of functions \\(f:\\mathbb{R} \\to \\mathbb{R}\\). (Duh.)\nWe can see the graphs of functions \\(f:\\mathbb{R}^2 \\to \\mathbb{R}\\).\nWe can see the graphs of functions \\(f:\\mathbb{R} \\to \\mathbb{R}^2\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/05-functions-of-multiple-variables-part-1.html#how-to-graph-functions",
    "href": "teaching/multi-calc-sp-26/slides/05-functions-of-multiple-variables-part-1.html#how-to-graph-functions",
    "title": "05 Functions of multiple variables, part 1",
    "section": "How to graph functions",
    "text": "How to graph functions\n\n\n\n\nHow to plot the graph of a function \\(f:\\mathbb{R} \\to \\mathbb{R}\\)\n\n\nSuppose the function is \\(y=f(x)\\). To plot the graph of \\(f\\), we:\n\nChoose a value of \\(x\\).\nCompute \\(y=f(x)\\).\nPlot the point \\((x,y)\\) in the \\(xy\\)-plane (i.e., \\(\\mathbb{R}^2\\)).\nRepeat for many values of \\(x\\). Connect the points with a curve.\n\n\n\n\n\n\n\n\n\nHow to plot the graph of a function \\(f:\\mathbb{R}^2 \\to \\mathbb{R}\\)\n\n\nSuppose the function is \\(z=f(x,y)\\). To plot the graph of \\(f\\), we:\n\nChoose a value of \\(x\\) and a value of \\(y\\).\nCompute \\(z=f(x,y)\\).\nPlot the point \\((x,y,z)\\) in \\(xyz\\)-space (i.e., \\(\\mathbb{R}^3\\)).\nRepeat for many values of \\(x\\) and \\(y\\). Connect the points with a surface.\n\n\n\n\n\n\n\n\n\nHow to plot the graph of a function \\(f:\\mathbb{R} \\to \\mathbb{R}^2\\)\n\n\nSuppose the function is \\((y,z)=f(x)\\). To plot the graph of \\(f\\), we:\n\nChoose a value of \\(x\\).\nCompute \\((y,z)=f(x)\\).\nPlot the point \\((x,y,z)\\) in \\(xyz\\)-space (i.e., \\(\\mathbb{R}^3\\)).\nRepeat for many values of \\(x\\). Connect the points with a curve.\n\n\n\n\n\n\nThis makes it sound easy. It is very often not.\nOften you resort to other methods to visualize graphs, such as:\n\nPlotting level curves for functions \\(f:\\mathbb{R}^2 \\to \\mathbb{R}\\).\nPlotting cross sections for functions \\(f:\\mathbb{R}^2 \\to \\mathbb{R}\\).\nUsing Desmos or GeoGebra."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/05-functions-of-multiple-variables-part-1.html#exercise-3-plotting-functions",
    "href": "teaching/multi-calc-sp-26/slides/05-functions-of-multiple-variables-part-1.html#exercise-3-plotting-functions",
    "title": "05 Functions of multiple variables, part 1",
    "section": "Exercise 3: Plotting functions",
    "text": "Exercise 3: Plotting functions\n\n\n\na. Graphing functions of the form \\(\\mathbb{R}^2 \\to \\mathbb{R}\\)\n\n\nGraph the following functions of the form \\(\\mathbb{R}^2 \\to \\mathbb{R}\\):\n\n\\(f(x,y) = 2x + 3y + 5\\)\n\\(g(x,y) = x^2 + y^2\\)\n\\(h(x,y) = \\sin{x} + \\cos{y}\\)\n\\(k(x,y) = \\sin(x^2+y^2)\\)\n\n\n\n\n\n\n\nb. Graphing functions of the form \\(\\mathbb{R} \\to \\mathbb{R}^2\\)\n\n\nGraph the following functions of the form \\(\\mathbb{R} \\to \\mathbb{R}^2\\):\n\n\\(s(x) = (x-1, -x+1)\\)\n\\(t(x) = (\\sin{x}, \\cos{x})\\)\n\\(p(x) = (e^x, e^{-x})\\)\n\\(u(x) = (\\sin{x}, \\sin{2x})\\)"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/07-functions-of-multiple-variables-part-3.html#functions-as-transformations",
    "href": "teaching/multi-calc-sp-26/slides/07-functions-of-multiple-variables-part-3.html#functions-as-transformations",
    "title": "07 Functions of multiple variables, part 3",
    "section": "Functions as transformations",
    "text": "Functions as transformations\n\nWe may try to visualize a function \\(f:\\mathbb{R}^m \\to \\mathbb{R}^n\\) via its graph, as we’ve seen.\nHowever, we can also think of \\(f\\) as a physical transformation of \\(\\mathbb{R}^m\\) into \\(\\mathbb{R}^n\\).\nThis will allow us to visualize functions all the way up to \\(\\mathbb{R}^3 \\to \\mathbb{R}^3\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/07-functions-of-multiple-variables-part-3.html#exercise-1-functions-mathbbr-to-mathbbr-as-transformations",
    "href": "teaching/multi-calc-sp-26/slides/07-functions-of-multiple-variables-part-3.html#exercise-1-functions-mathbbr-to-mathbbr-as-transformations",
    "title": "07 Functions of multiple variables, part 3",
    "section": "Exercise 1: Functions \\(\\mathbb{R} \\to \\mathbb{R}\\) as transformations",
    "text": "Exercise 1: Functions \\(\\mathbb{R} \\to \\mathbb{R}\\) as transformations\n\n\n\nDescribe the action of the following functions \\(\\mathbb{R} \\to \\mathbb{R}\\) as physical transformations of the real line into itself. Draw pictures!\n\n\\(f(x) = x^2\\)\n\\(g(x) = 2x+1\\)\n\\(h(x) = -x-1\\)\n\\(k(x) = x^3\\)\n\\(j(x) = \\sin{x}\\)"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/07-functions-of-multiple-variables-part-3.html#exercise-2-functions-mathbbr-to-mathbbr2-as-transformations",
    "href": "teaching/multi-calc-sp-26/slides/07-functions-of-multiple-variables-part-3.html#exercise-2-functions-mathbbr-to-mathbbr2-as-transformations",
    "title": "07 Functions of multiple variables, part 3",
    "section": "Exercise 2: Functions \\(\\mathbb{R} \\to \\mathbb{R}^2\\) as transformations",
    "text": "Exercise 2: Functions \\(\\mathbb{R} \\to \\mathbb{R}^2\\) as transformations\n\n\n\nDescribe the action of the following functions \\(\\mathbb{R} \\to \\mathbb{R}^2\\) as physical transformations of the real line into the plane. Draw pictures!\n\n\\(f(t) = (t, t^2)\\)\n\\(g(t) = (2t+1, -t)\\)\n\\(h(t) = (\\cos{t}, \\sin{t})\\)\n\\(k(t) = (\\sin{t}, \\cos{t})\\)\n\\(j(t) = (t^2, t^3)\\)"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/07-functions-of-multiple-variables-part-3.html#exercise-3-comparison-to-linear-functions-part-1",
    "href": "teaching/multi-calc-sp-26/slides/07-functions-of-multiple-variables-part-3.html#exercise-3-comparison-to-linear-functions-part-1",
    "title": "07 Functions of multiple variables, part 3",
    "section": "Exercise 3: Comparison to linear functions, part 1",
    "text": "Exercise 3: Comparison to linear functions, part 1\n\n\n\nCompare the function \\(j(t) = (t^2, t^3)\\) from the previous exercise to the linear function \\(L:\\mathbb{R} \\to \\mathbb{R}^2\\) defined by \\(L(t) = (2t-1,3t-2)\\) at the point \\((1,1)\\). Draw pictures!"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/07-functions-of-multiple-variables-part-3.html#exercise-4-a-linear-function-fmathbbr2-to-mathbbr2-as-a-transformation",
    "href": "teaching/multi-calc-sp-26/slides/07-functions-of-multiple-variables-part-3.html#exercise-4-a-linear-function-fmathbbr2-to-mathbbr2-as-a-transformation",
    "title": "07 Functions of multiple variables, part 3",
    "section": "Exercise 4: A linear function \\(f:\\mathbb{R}^2 \\to \\mathbb{R}^2\\) as a transformation",
    "text": "Exercise 4: A linear function \\(f:\\mathbb{R}^2 \\to \\mathbb{R}^2\\) as a transformation\n\n\n\nDefine a function \\(f:\\mathbb{R}^2 \\to \\mathbb{R}^2\\) by \\[\n(u,v) = f(x,y) = (2x+1, x-y-1).\n\\] Describe the action of \\(f\\) as a physical transformation of the plane into itself. Draw pictures!"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/07-functions-of-multiple-variables-part-3.html#exercise-5-a-nonlinear-function-gmathbbr2-to-mathbbr2-as-a-transformation",
    "href": "teaching/multi-calc-sp-26/slides/07-functions-of-multiple-variables-part-3.html#exercise-5-a-nonlinear-function-gmathbbr2-to-mathbbr2-as-a-transformation",
    "title": "07 Functions of multiple variables, part 3",
    "section": "Exercise 5: A nonlinear function \\(g:\\mathbb{R}^2 \\to \\mathbb{R}^2\\) as a transformation",
    "text": "Exercise 5: A nonlinear function \\(g:\\mathbb{R}^2 \\to \\mathbb{R}^2\\) as a transformation\n\n\n\nDefine a function \\(g:\\mathbb{R}^2 \\to \\mathbb{R}^2\\) by \\[\n(u,v) = g(x,y) = (x, x^3 + y ).\n\\] Describe the action of \\(g\\) as a physical transformation of the plane into itself. Draw pictures!"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/07-functions-of-multiple-variables-part-3.html#exercise-6-a-nonlinear-function-kmathbbr2-to-mathbbr2-as-a-transformation",
    "href": "teaching/multi-calc-sp-26/slides/07-functions-of-multiple-variables-part-3.html#exercise-6-a-nonlinear-function-kmathbbr2-to-mathbbr2-as-a-transformation",
    "title": "07 Functions of multiple variables, part 3",
    "section": "Exercise 6: A nonlinear function \\(k:\\mathbb{R}^2 \\to \\mathbb{R}^2\\) as a transformation",
    "text": "Exercise 6: A nonlinear function \\(k:\\mathbb{R}^2 \\to \\mathbb{R}^2\\) as a transformation\n\n\n\nDefine a function \\(k:\\mathbb{R}^2 \\to \\mathbb{R}^2\\) by \\[\n(u,v) = k(x,y) = (y^2 +x, y).\n\\] Describe the action of \\(k\\) as a physical transformation of the plane into itself. Draw pictures!"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/07-functions-of-multiple-variables-part-3.html#comparison-to-linear-functions-part-2",
    "href": "teaching/multi-calc-sp-26/slides/07-functions-of-multiple-variables-part-3.html#comparison-to-linear-functions-part-2",
    "title": "07 Functions of multiple variables, part 3",
    "section": "Comparison to linear functions, part 2",
    "text": "Comparison to linear functions, part 2\n\nReturn to the function \\(k(x,y) = (y^2 + x, y)\\), and focus on a point in the \\(xy\\)-plane, say \\((1,1)\\), and look at its image in the \\(uv\\)-plane, which is \\((2,1)\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow consider the linear function \\(L:\\mathbb{R}^2 \\to \\mathbb{R}^2\\) defined by \\(L(x,y) = (x+2y-1,y)\\), and focus on the same points:\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo you notice how that, if you only compare the graphs in the \\(uv\\)-plane near the point \\((2,1)\\), the function \\(k\\) looks a lot like the linear function \\(L\\)? That’s because \\(L\\) is the “best” linear approximation of \\(k\\) near \\((1,1)\\), cooked up using the derivative of \\(k\\) at \\((1,1)\\).\nSame story here as in single-variable calculus. Nothing new conceptually."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#sets-of-points-and-vectors",
    "href": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#sets-of-points-and-vectors",
    "title": "01 Vectors and matrices, part 1",
    "section": "Sets of points and vectors",
    "text": "Sets of points and vectors\n\n\n\n\n\\(\\mathbb{R}^2\\) as a set of points\n\n\nWe write \\(\\mathbb{R}^2\\) to denote the set of all points in the plane.\n\nA point is represented by an ordered pair of real numbers \\((x, y)\\).\nThe numbers \\(x\\) and \\(y\\) are called the coordinates of the point.\n\n\n\n\n\n\nThis is the usual way of thinking about points in the plane. You’ve known this your entire mathematical life.\nHowever, sometimes it is convenient to think of \\(\\mathbb{R}^2\\) in a different way:\n\n\n\n\n\n\\(\\mathbb{R}^2\\) as a set of vectors\n\n\nWe write \\(\\mathbb{R}^2\\) to denote the set of all vectors in the plane.\n\nSuch a vector is represented as a \\(2\\)-dimensional column vectors written as \\[\n  \\begin{bmatrix}\n  x \\\\ y\n  \\end{bmatrix}\n  \\] where \\(x\\) and \\(y\\) are real numbers.\nThe numbers \\(x\\) and \\(y\\) are called the components of the vector.\n\n\n\n\n\n\nVisually, we can think of a vector \\(\\begin{bmatrix} x \\\\ y \\end{bmatrix}\\) as an arrow with its tail at the origin \\((0,0)\\) and its head at the point \\((x,y)\\).\nWhich way we think of \\(\\mathbb{R}^2\\) (as points or as vectors) depends on the context. We will go back and forth between these two interpretations repeatedly."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#vector-algebra-in-the-plane",
    "href": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#vector-algebra-in-the-plane",
    "title": "01 Vectors and matrices, part 1",
    "section": "Vector algebra in the plane",
    "text": "Vector algebra in the plane\n\n\n\n\nVector addition/subtraction\n\n\nGiven two vectors \\[\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix},\\] their sum/difference \\(\\mathbf{u} \\pm \\mathbf{v}\\) is defined as \\[\\mathbf{u} \\pm \\mathbf{v} = \\begin{bmatrix} u_1 \\pm v_1 \\\\ u_2 \\pm v_2 \\end{bmatrix}.\\]\n\n\n\n\n\nGeometrically, vector addition can be visualized using the “tip-to-tail” method. (See the upcoming exercise.)\n\n\n\n\n\nScalar multiplication\n\n\nGiven a vector \\[\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix}\\] and a scalar (real number) \\(c\\), the scalar multiple \\(c\\mathbf{u}\\) is defined as \\[c\\mathbf{u} = \\begin{bmatrix} cu_1 \\\\ cu_2 \\end{bmatrix}.\\]\n\n\n\n\n\nGeometrically, scalar multiplication stretches or shrinks the vector by a factor of \\(|c|\\). If \\(c\\) is negative, it also reverses the direction of the vector. (Again, see the upcoming exercise.)"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#properties-of-vector-algebra",
    "href": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#properties-of-vector-algebra",
    "title": "01 Vectors and matrices, part 1",
    "section": "Properties of vector algebra",
    "text": "Properties of vector algebra\n\n\n\n\nProperties of vector addition and scalar multiplication\n\n\nLet \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{w}\\) be vectors in \\(\\mathbb{R}^2\\), and let \\(a\\) and \\(b\\) be scalars. Then the following properties hold:\n\nCommutative property of addition: \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\).\nAssociative property of addition: \\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\).\nDistributive property of scalar multiplication over vector addition: \\(a(\\mathbf{u} + \\mathbf{v}) = a\\mathbf{u} + a\\mathbf{v}\\).\nDistributive property of scalar multiplication over scalar addition: \\((a + b)\\mathbf{u} = a\\mathbf{u} + b\\mathbf{u}\\).\nAssociative property of scalar multiplication: \\(a(b\\mathbf{u}) = (ab)\\mathbf{u}\\).\nIdentity property of scalar multiplication: \\(1\\mathbf{u} = \\mathbf{u}\\).\n\n\n\n\n\n\n\n\n\nZero vector\n\n\n\nThe zero vector in \\(\\mathbb{R}^2\\) is denoted by \\(\\mathbf{0}\\) and is defined as \\[\\mathbf{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.\\]\nIt has the property that for any vector \\(\\mathbf{u}\\) in \\(\\mathbb{R}^2\\), \\[\\mathbf{u} + \\mathbf{0} = \\mathbf{u}.\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#exercise-1-vector-algebra",
    "href": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#exercise-1-vector-algebra",
    "title": "01 Vectors and matrices, part 1",
    "section": "Exercise 1: Vector algebra",
    "text": "Exercise 1: Vector algebra\n\n\n\nPractice with vector addition, subtraction, and scalar multiplication\n\n\nGiven the vectors \\[\n\\mathbf{u} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{v} = \\begin{bmatrix} -1 \\\\ 4 \\end{bmatrix},\n\\] compute the following:\n\n\\(\\mathbf{u} + \\mathbf{v}\\)\n\\(\\mathbf{u} - \\mathbf{v}\\)\n\\(3\\mathbf{u}\\)\n\\(-2\\mathbf{v}\\)\n\nThen, draw the vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(\\mathbb{R}^2\\), and the results of each operation."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#norms-and-magnitudes",
    "href": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#norms-and-magnitudes",
    "title": "01 Vectors and matrices, part 1",
    "section": "Norms and magnitudes",
    "text": "Norms and magnitudes\n\n\n\n\nMagnitude of a vector\n\n\n\nThe norm (or magnitude) of a vector \\[\n  \\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix}\n  \\] is denoted by \\(\\lVert \\mathbf{u}\\rVert\\) and is defined as \\[\n  \\lVert \\mathbf{u}\\rVert = \\sqrt{u_1^2 + u_2^2}.\n  \\]\nA vector is called a unit vector if its norm is \\(1\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#exercise-2-norms",
    "href": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#exercise-2-norms",
    "title": "01 Vectors and matrices, part 1",
    "section": "Exercise 2: Norms",
    "text": "Exercise 2: Norms\n\n\n\na) Geometric interpretation of norms\n\n\n\nExplain why the norm of a vector is its length, interpreting the vector as an arrow in the plane.\nGiven a vector \\(\\mathbf{u}\\) and a scalar \\(c\\), explain why \\(\\lVert c\\mathbf{u} \\rVert = |c| \\lVert \\mathbf{u} \\rVert\\). Does this make sense geometrically?\n\n\n\n\n\n\n\nb) Practice with computing norms\n\n\nGiven the vectors \\[\n\\mathbf{a} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{b} = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix},\n\\]\ncompute the following:\n\n\\(\\lVert \\mathbf{a} \\rVert\\)\n\\(\\lVert \\mathbf{b} \\rVert\\)\nFind a unit vector in the direction of \\(\\mathbf{a}\\).\nFind a unit vector in the direction of \\(\\mathbf{b}\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#dotinner-products",
    "href": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#dotinner-products",
    "title": "01 Vectors and matrices, part 1",
    "section": "Dot/inner products",
    "text": "Dot/inner products\n\n\n\n\nAlgebraic definition of the dot product\n\n\n\nGiven two vectors \\[\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix},\\] their dot product \\(\\mathbf{u} \\cdot \\mathbf{v}\\) is defined as \\[\\mathbf{u} \\cdot \\mathbf{v} = u_1v_1 + u_2v_2.\\]\nThe dot product is also called an inner product and is written alternatively as \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle\\).\n\n\n\n\n\n\nWe will switch between both notations \\(\\mathbf{u} \\cdot \\mathbf{v}\\) and \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle\\) very often!\nThe above definition is called the algebraic definition of the dot product since it only involes algebraic operations on the components of the vectors.\nThere is also a geometric definition of the dot product, which relates it to the angle between the two vectors and their norms.\n\n\n\n\n\nGeometric definition of the dot product\n\n\nGiven two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), the dot product can also be defined as \\[\\mathbf{u} \\cdot \\mathbf{v} = \\lVert \\mathbf{u} \\rVert \\lVert \\mathbf{v} \\rVert \\cos \\theta,\\] where \\(\\theta\\) (\\(0 \\leq \\theta \\leq \\pi\\)) is the angle between the two vectors."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#properties-of-dotinner-products",
    "href": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#properties-of-dotinner-products",
    "title": "01 Vectors and matrices, part 1",
    "section": "Properties of dot/inner products",
    "text": "Properties of dot/inner products\n\n\n\n\nAlgebraic and geometric properties of the dot product\n\n\n\nThe dot product is commutative: \\(\\mathbf{u} \\cdot \\mathbf{v} = \\mathbf{v} \\cdot \\mathbf{u}\\).\nThe dot product is distributive over vector addition: \\(\\mathbf{u} \\cdot (\\mathbf{v} + \\mathbf{w}) = \\mathbf{u} \\cdot \\mathbf{v} + \\mathbf{u} \\cdot \\mathbf{w}\\).\nThe dot product is bilinear: for any scalar \\(c\\), \\((c\\mathbf{u}) \\cdot \\mathbf{v} = c(\\mathbf{u} \\cdot \\mathbf{v})\\).\nIf \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are nonzero, then they are orthogonal (i.e., perpendicular) if and only if \\(\\mathbf{u} \\cdot \\mathbf{v} = 0\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#exercise-3-dot-products",
    "href": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#exercise-3-dot-products",
    "title": "01 Vectors and matrices, part 1",
    "section": "Exercise 3: Dot products",
    "text": "Exercise 3: Dot products\n\n\n\na) Practice with computing dot/inner products\n\n\nGiven the vectors \\[\n\\mathbf{p} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{q} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix},\n\\] compute the following:\n\n\\(\\mathbf{p} \\cdot \\mathbf{q}\\)\n\\(\\langle\\mathbf{q}, \\mathbf{p}\\rangle\\)\n\\(\\langle \\mathbf{p}, \\mathbf{p} \\rangle\\)\n\\(\\mathbf{q} \\cdot \\mathbf{q}\\)\n\n\n\n\n\n\n\nb) Geometric interpretation of the dot/inner product\n\n\nExplain why the algebraic and geometric definitions of the inner product \\(\\langle \\mathbf{u} , \\mathbf{v}\\rangle\\) are equivalent. For simplicity, you may assume that \\(\\mathbf{u}\\) points along the x-axis, i.e., \\[\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ 0 \\end{bmatrix}.\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#standard-basis-vectors-in-the-plane",
    "href": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#standard-basis-vectors-in-the-plane",
    "title": "01 Vectors and matrices, part 1",
    "section": "Standard basis vectors in the plane",
    "text": "Standard basis vectors in the plane\n\n\n\n\nStandard basis vectors in \\(\\mathbb{R}^2\\)\n\n\nThe standard basis vectors in \\(\\mathbb{R}^2\\) are denoted by \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\), and are defined as \\[\n\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}.\n\\]\n\n\n\n\n\n\n\n\nTheorem: Resolving vectors into standard basis vectors\n\n\nAny vector\n\\[\n\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix}\n\\]\nin \\(\\mathbb{R}^2\\) can be expressed as a linear combination of the standard basis vectors (i.e., a sum of scalar multiples of the basis vectors) as:\n\\[\\mathbf{u} = u_1 \\mathbf{e}_1 + u_2 \\mathbf{e}_2.\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#exercise-4-basis-vectors",
    "href": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#exercise-4-basis-vectors",
    "title": "01 Vectors and matrices, part 1",
    "section": "Exercise 4: Basis vectors",
    "text": "Exercise 4: Basis vectors\n\n\n\nPractice with basis vectors\n\n\nGiven any vector \\(\\mathbf{u}\\), explain why \\[\n\\mathbf{u} = \\langle \\mathbf{u}, \\mathbf{e}_1 \\rangle \\mathbf{e}_1 + \\langle \\mathbf{u}, \\mathbf{e}_2 \\rangle \\mathbf{e}_2.\n\\]\nTake care to notice that there are three vector operations happening in this equation: inner products, scalar multiplication, and vector addition."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#sets-of-points-and-vectors-in-space",
    "href": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#sets-of-points-and-vectors-in-space",
    "title": "01 Vectors and matrices, part 1",
    "section": "Sets of points and vectors in space",
    "text": "Sets of points and vectors in space\n\n\n\n\n\\(\\mathbb{R}^3\\) as a set of points\n\n\nWe write \\(\\mathbb{R}^3\\) to denote the set of all points in \\(3\\)-dimensional space.\n\nA point is represented by an ordered triple of real numbers \\((x, y, z)\\).\nThe numbers \\(x\\), \\(y\\), and \\(z\\) are called the coordinates of the point.\n\n\n\n\n\n\nAs with \\(\\mathbb{R}^2\\), we can also think of \\(\\mathbb{R}^3\\) as a set of vectors:\n\n\n\n\n\n\\(\\mathbb{R}^3\\) as a set of vectors\n\n\nWe write \\(\\mathbb{R}^3\\) to denote the set of all vectors in \\(3\\)-dimensional space.\n\nSuch a vector is represented as a \\(3\\)-dimensional column vectors written as \\[\n  \\begin{bmatrix}\n  x \\\\ y \\\\ z\n  \\end{bmatrix}\n  \\] where \\(x\\), \\(y\\), and \\(z\\) are real numbers.\nThe numbers \\(x\\), \\(y\\), and \\(z\\) are called the components of the vector."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#alegebra-norms-and-dot-products-in-space",
    "href": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#alegebra-norms-and-dot-products-in-space",
    "title": "01 Vectors and matrices, part 1",
    "section": "Alegebra, norms, and dot products in space",
    "text": "Alegebra, norms, and dot products in space\n\nAll the definitions and properties of vector algebra, norms, and dot products that we discussed for \\(\\mathbb{R}^2\\) carry over to \\(\\mathbb{R}^3\\) with only minor modifications.\nFor exmaple, given two vectors \\[\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix},\\] their sum/difference is defined as \\[\\mathbf{u} \\pm \\mathbf{v} = \\begin{bmatrix} u_1 \\pm v_1 \\\\ u_2 \\pm v_2 \\\\ u_3 \\pm v_3 \\end{bmatrix}.\\]\nTheir norm is defined as \\[\\lVert \\mathbf{u} \\rVert = \\sqrt{u_1^2 + u_2^2 + u_3^2}.\\]\nTheir inner product is defined as \\[\\langle \\mathbf{u}, \\mathbf{v} \\rangle = u_1v_1 + u_2v_2 + u_3v_3.\\]\nAll the properties we discussed for vectors in \\(\\mathbb{R}^2\\) also hold in \\(\\mathbb{R}^3\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#standard-basis-vectors-in-space",
    "href": "teaching/multi-calc-sp-26/slides/01-vectors-and-matrices-part-1.html#standard-basis-vectors-in-space",
    "title": "01 Vectors and matrices, part 1",
    "section": "Standard basis vectors in space",
    "text": "Standard basis vectors in space\n\n\n\n\nStandard basis vectors in \\(\\mathbb{R}^3\\)\n\n\nThe standard basis vectors in \\(\\mathbb{R}^3\\) are denoted by \\(\\mathbf{e}_1\\), \\(\\mathbf{e}_2\\), and \\(\\mathbf{e}_3\\), and are defined as \\[\n\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n\\]\n\n\n\n\n\n\n\n\nTheorem: Resolving vectors into standard basis vectors\n\n\nAny vector\n\\[\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\end{bmatrix}\\]\nin \\(\\mathbb{R}^3\\) can be expressed as a linear combination of the standard basis vectors (i.e., a sum of scalar multiples of the basis vectors) as:\n\\[\\mathbf{u} = u_1 \\mathbf{e}_1 + u_2 \\mathbf{e}_2 + u_3 \\mathbf{e}_3.\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/03-planes-and-linear-spaces-part-1.html#point-slope-equations-of-planes",
    "href": "teaching/multi-calc-sp-26/slides/03-planes-and-linear-spaces-part-1.html#point-slope-equations-of-planes",
    "title": "03 Planes and linear spaces, part 1",
    "section": "Point-slope equations of planes",
    "text": "Point-slope equations of planes\n\n\n\n\nPoint-slope equation for a line in \\(\\mathbb{R}^2\\)\n\n\n\nThe point-slope equation for a line through a point \\((x_0,y_0)\\) with slope \\(m\\) is \\[\ny = m(x-x_0) + y_0.\n\\]\n\n\n\n\n\n\nWe interpret the slope \\(m\\) as the slope in the positive \\(x\\)-direction.\n\n\n\n\n\nPoint-slope equation for a plane in \\(\\mathbb{R}^3\\)\n\n\n\nThe point-slope equation for a plane through a point \\((x_0,y_0,z_0)\\) with slope \\(m\\) in the positive \\(x\\)-direction and slope \\(n\\) in the positive \\(y\\)-direction is \\[\nz = m(x-x_0) + n(y-y_0) + z_0.\n\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/03-planes-and-linear-spaces-part-1.html#exercise-1-point-slope-equations",
    "href": "teaching/multi-calc-sp-26/slides/03-planes-and-linear-spaces-part-1.html#exercise-1-point-slope-equations",
    "title": "03 Planes and linear spaces, part 1",
    "section": "Exercise 1: Point-slope equations",
    "text": "Exercise 1: Point-slope equations\n\n\n\n\nCompute the point-slope equation for a plane through the point \\((1,2,3)\\) with slope \\(2\\) in the positive \\(x\\)-direction and slope \\(-1\\) in the positive \\(y\\)-direction.\nThe point-slope equation of a plane is \\[\nz = 4(x-2) - 3(y-2) + 5.\n\\] Through what point does this plane pass? What are the slopes in the \\(+x\\) and \\(+y\\) directions?\nThe point-slope equation of a plane is \\[\nz = 3x+ 4.\n\\] Through what point does this plane pass? What are the slopes in the \\(+x\\) and \\(+y\\) directions?\nThe point-slope equation of a plane is \\[\nz = 0.\n\\] Through what point does this plane pass? What are the slopes in the \\(+x\\) and \\(+y\\) directions?"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/03-planes-and-linear-spaces-part-1.html#exercise-2-linear-models",
    "href": "teaching/multi-calc-sp-26/slides/03-planes-and-linear-spaces-part-1.html#exercise-2-linear-models",
    "title": "03 Planes and linear spaces, part 1",
    "section": "Exercise 2: Linear models",
    "text": "Exercise 2: Linear models\n\n\n\nThe profit, \\(p\\), that a company makes depends on the number of two types of gadgets that it produces, Gadget 1 and Gadget 2. Let \\(g_1\\) and \\(g_2\\) be the number of gadgets that are manufactured of each type, and suppose they sell for \\(s_1\\) and \\(s_2\\) dollars (per gadget), respectively. Suppose the company believes that the profit depends linearly on the number of gadgets produced.\nLet \\(p = f(g_1,g_2)\\) be the profit function, and assume that if no gadgets are products, then \\(p=0\\).\n\nWhat is the domain of \\(f\\)? What is its codomain?\nFind a formula for the profit function \\(p = f(g_1, g_2)\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/03-planes-and-linear-spaces-part-1.html#vector-equations-of-planes",
    "href": "teaching/multi-calc-sp-26/slides/03-planes-and-linear-spaces-part-1.html#vector-equations-of-planes",
    "title": "03 Planes and linear spaces, part 1",
    "section": "Vector equations of planes",
    "text": "Vector equations of planes\n\nSuppose a point \\((x,y,z)\\) lies on a plane with equation \\[\nz = m(x-x_0) + n(y-y_0) + z_0.\n\\]\nThe two points \\((x,y,z)\\) and \\((x_0,y_0,z_0)\\) have position vectors \\[\n\\mathbf{r} = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{r}_0 = \\begin{bmatrix} x_0 \\\\ y_0 \\\\ z_0 \\end{bmatrix}.\n\\]\nIf the vector \\(\\mathbf{r}-\\mathbf{r_0}\\) has its tail moved to the point \\((x_0,y_0,z_0)\\), then it lies in the plane. Draw the picture!\nDefine \\[\n\\mathbf{n} = \\begin{bmatrix} -m \\\\ -n \\\\ 1 \\end{bmatrix},\n\\] which is an example of a normal vector to the plane. (See below.)\nThen, observe: \\[\n\\langle \\mathbf{n}, \\mathbf{r}-\\mathbf{r}_0 \\rangle = \\left\\langle \\begin{bmatrix} -m \\\\ -n \\\\ 1 \\end{bmatrix}, \\begin{bmatrix} x-x_0 \\\\ y-y_0 \\\\ z-z_0 \\end{bmatrix} \\right\\rangle = -m(x-x_0) -n(y-y_0) + (z-z_0) = 0.\n\\]\n\n\n\n\n\nVector equation of a plane in \\(\\mathbb{R}^3\\)\n\n\nLet \\(\\mathbf{n}\\) and \\(\\mathbf{r}_0\\) be two given vectors in \\(\\mathbb{R}^3\\). A vector equation of the plane with normal vector \\(\\mathbf{n}\\), passing through the point with position vector \\(\\mathbf{r}_0\\), is \\[\n\\langle \\mathbf{n}, \\mathbf{r}-\\mathbf{r}_0 \\rangle = 0,\n\\] where \\(\\mathbf{r}\\) is a variable position vector in \\(\\mathbb{R}^3\\).\n\n\n\n\n\nAny vector that is orthogonal to all vectors in a plane is called a normal vector to the plane."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/03-planes-and-linear-spaces-part-1.html#standard-equations-of-planes",
    "href": "teaching/multi-calc-sp-26/slides/03-planes-and-linear-spaces-part-1.html#standard-equations-of-planes",
    "title": "03 Planes and linear spaces, part 1",
    "section": "Standard equations of planes",
    "text": "Standard equations of planes\n\nIf a plane has a point-slope equation, then a normal vector can always be found of the form \\[\n\\mathbf{n} = \\begin{bmatrix} -m \\\\ -n \\\\ 1 \\end{bmatrix},\n\\] as we saw on the prevoius slide.\nBut not all planes have normal vectors of this special form!\nSo, we must resort to the vector equation. If we write \\[\n\\mathbf{n} = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix},\n\\] and expand the inner product in the vector equation, we get the following:\n\n\n\n\n\nStandard equation of a plane in \\(\\mathbb{R}^3\\)\n\n\nA standard equation of a plane in \\(\\mathbb{R}^3\\) passing through the point \\((x_0,y_0,z_0)\\) is \\[\na(x-x_0) + b(y-y_0) + c(z-z_0) = 0,\n\\] where \\(a\\), \\(b\\), and \\(c\\) are components of a normal vector \\(\\mathbf{n}\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/03-planes-and-linear-spaces-part-1.html#exercise-3-normal-vectors-and-standard-equations",
    "href": "teaching/multi-calc-sp-26/slides/03-planes-and-linear-spaces-part-1.html#exercise-3-normal-vectors-and-standard-equations",
    "title": "03 Planes and linear spaces, part 1",
    "section": "Exercise 3: Normal vectors and standard equations",
    "text": "Exercise 3: Normal vectors and standard equations\n\n\n\na. Practice with normal vectors\n\n\n\nFind a normal vector to the plane \\(z = 2(x-1) - (y-2) + 3\\).\nFind a normal vector to the plane \\(z = 4(x-2) + 5\\).\nFind a normal vector to the plane \\(z = -y\\).\nFind a normal vector to the plane \\(x=2\\).\nFind a point-slope equation of the plane with normal vector \\[\n\\mathbf{n} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}\n\\] passing through the point \\((1,2,3)\\).\n\n\n\n\n\n\n\nb. Normal vectors and the coordinate planes\n\n\n\nFind a normal vector to the \\(xy\\)-plane in \\(\\mathbb{R}^3\\).\nFind a normal vector to the \\(xz\\)-plane in \\(\\mathbb{R}^3\\).\nFind a normal vector to the \\(yz\\)-plane in \\(\\mathbb{R}^3\\).\n\n\n\n\n\n\n\nb. Normal vectors and standard equations\n\n\n\nFind a normal vector to the plane with standard equation \\(2(x-2) - 3y + z = 0\\). Identify a point on this plane.\nA plane passes through the point \\((1,2,-1)\\) and has normal vector \\(\\mathbf{e}_2\\). Find its standard equation."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/03-planes-and-linear-spaces-part-1.html#affine-equations-of-planes",
    "href": "teaching/multi-calc-sp-26/slides/03-planes-and-linear-spaces-part-1.html#affine-equations-of-planes",
    "title": "03 Planes and linear spaces, part 1",
    "section": "Affine equations of planes",
    "text": "Affine equations of planes\n\n\n\nAffine equation of a plane in \\(\\mathbb{R}^3\\)\n\n\nAn affine equation for a plane in \\(\\mathbb{R}^3\\) is \\[\nax + by + cz = d,\n\\]\nwhere:\n\n\\(a\\), \\(b\\), \\(c\\), and \\(d\\) are constants,\nnot all of \\(a\\), \\(b\\), and \\(c\\) are \\(0\\), and\nthe numbers \\(a\\), \\(b\\), and \\(c\\) are the components of a normal vector to the plane."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/03-planes-and-linear-spaces-part-1.html#exercise-4-affine-equations",
    "href": "teaching/multi-calc-sp-26/slides/03-planes-and-linear-spaces-part-1.html#exercise-4-affine-equations",
    "title": "03 Planes and linear spaces, part 1",
    "section": "Exercise 4: Affine equations",
    "text": "Exercise 4: Affine equations\n\n\n\na. From affine equations to standard equations\n\n\nConvert the following affine equations to standard equations. Give a normal vector and a point on each plane.\n\n\\(2x - 3y + z = 5\\)\n\\(x + y  = 1\\)\n\\(2y - z = 2\\)\n\\(x = 4\\)\n\n\n\n\n\n\n\nb. From standard equations to affine equations\n\n\nConvert the following standard equations to affine equations:\n\n\\(4(x-1) + 2(y-2) - 5(z-3) = 0\\)\n\\((x-1) + 3(y-2) = 0\\)\n\\(2(y-1) - (z-2) = 0\\)\n\\(2(x-8) = 0\\)\n\n\n\n\n\n\n\nc. From affine equations to point-slope equations\n\n\nConvert the following affine equations to point-slope equations, if possible.\n\n\\(2x - 3y + z = 5\\)\n\\(6x + y = 1\\)\n\\(2y - 3z = 2\\)\n\\(x = 4\\)"
  },
  {
    "objectID": "teaching/analysis-fa-25/analysis-fa-25.html",
    "href": "teaching/analysis-fa-25/analysis-fa-25.html",
    "title": "mat347 analysis, fall 2025",
    "section": "",
    "text": "instructor:\n\n\njohn myers\n\n\n\n\noffice:\n\n\nmarano 175\n\n\n\n\noffice hours:\n\n\n12-12:30 MWF\n\n\n\n\nsyllabus:\n\n\nlink\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndate\ntopics\ninfo + due dates\n\n\n\n\n15\n12.10 wed\nfinal exam, 2-4pm, Shineman 183\n\n\n\n14\n12.05 fri\n7.5 The Fundamental Theorem of Calculus \nweek 14 hw due\n\n\n\n12.03 wed\n7.2 The definition of the Riemann integral, part 2 \n\n\n\n\n12.01 mon\n7.2 The definition of the Riemann integral, part 1  7.2 The definition of the Riemann integral, part 2 \n\n\n\n13\n11.21 fri\n5.2 derivatives, part 1     5.2 derivatives, part 2 \nweek 13 hw due\n\n\n\n11.19 wed\n4.4 continuous functions on compact sets, part 1     5.2 derivatives, part 1   \n\n\n\n\n11.17 mon\n4.3 continuous functions     4.4 continuous functions on compact sets, part 1   \n\n\n\n12\n11.14 fri\n4.2 functional limits, part 2     4.3 continuous functions   \nweek 12 hw due\n\n\n\n11.12 wed\nno class\n\n\n\n\n11.10 mon\n4.2 functional limits, part 1     4.2 functional limits, part 2     4.3 continuous functions   \n\n\n\n11\n11.07 fri\n4.2 functional limits, part 1   \nweek 11 hw due\n\n\n\n11.05 wed\n3.3 compact sets, part 2     4.2 functional limits, part 1   \n\n\n\n\n11.03 mon\n3.3 compact sets, part 1     3.3 compact sets, part 2   \n\n\n\n10\n10.31 fri\n3.3 compact sets, part 1   \nweek 10 hw due\n\n\n\n10.29 wed\n3.2 open and closed sets   \n\n\n\n\n10.27 mon\nno class\n\n\n\n9\n10.24 fri\n3.2 open and closed sets   \nweek 9 hw due\n\n\n\n10.22 wed\n3.2 open and closed sets   \n\n\n\n\n10.20 mon\n2.6 the Cauchy criterion   \n\n\n\n8\n10.17 fri\nexam 1 on sections 1.2-2.5\n\n\n\n\n10.15 wed\n2.4-2.5 Monotone Conv. and B-W theorems, part 2   2.6 the Cauchy criterion   \n\n\n\n\n10.13 mon\n2.4-2.5 Monotone Conv. and B-W theorems, part 1     2.4-2.5 Monotone Conv. and B-W theorems, part 2 \n\n\n\n7\n10.10 fri\nno class - fall break\n\n\n\n\n10.08 wed\n2.4-2.5 Monotone Conv. and B-W theorems, part 1   \n\n\n\n\n10.06 mon\n2.3 the algebraic and order limit theorems, part 2   \n\n\n\n6\n10.03 fri\nno class\nweek 6 hw due\n\n\n\n10.01 wed\n2.3 the algebraic and order limit theorems, part 1     2.3 the algebraic and order limit theorems, part 2   \n\n\n\n\n09.29 mon\n2.3 the algebraic and order limit theorems, part 1   \n\n\n\n5\n09.26 fri\n2.2 the limit of a sequence, part 2   \nweek 5 hw due\n\n\n\n09.24 wed\n2.2 the limit of a sequence, part 1   \n\n\n\n\n09.22 mon\n1.5 cardinality, part 2   2.2 the limit of a sequence, part 1   \n\n\n\n4\n09.19 fri\n1.5 cardinality, part 2   \nweek 4 hw due\n\n\n\n09.17 wed\n1.5 cardinality, part 1   \n\n\n\n\n09.15 mon\n1.5 cardinality, part 1   \n\n\n\n3\n09.12 fri\n1.4 consequences of completeness   \nweek 2 & 3 homework due\n\n\n\n09.10 wed\n1.4 consequences of completeness   \n\n\n\n\n09.08 mon\n1.3 axiom of completeness   \n\n\n\n2\n09.05 fri\n1.2 some preliminaries1.3 axiom of completeness   \n\n\n\n\n09.03 wed\n1.2 some preliminaries   \n\n\n\n1\n08.29 fri\nno class\n\n\n\n\n08.27 wed\n1.1 introduction   1.2 some preliminaries   \n\n\n\n\n08.25 mon\n1.1 introduction"
  },
  {
    "objectID": "posts/info-1/index.html",
    "href": "posts/info-1/index.html",
    "title": "Entropy & information",
    "section": "",
    "text": "Large language models (LLMs) like ChatGPT have brought probabilistic models into mainstream conversation. Unlike deterministic models that always produce the same output for a given input, these models generate a probability distribution over many possible outputs. Information theory provides the mathematical framework for understanding these distributions, and how sensitive they are to the inputs of the model.\nTo understand this more concretely, consider what happens when you prompt an LLM. You might ask “What’s the capital of France?” and reliably get “Paris” as the answer. The model is highly confident: the output probability distribution is concentrated over the one-word output “Paris”. But if you ask “Write me a poem about autumn,” you’ll get a different response each time. Here, the probability distribution is spread across countless possible poems, each with some small probability of being generated. In the first case, the model is certain, reflected in the strongly peaked output distribution, while in the latter case it is uncertain, reflected in the highly dispersed output distribution. The input to the model (i.e., the prompt) thus has a strong effect on the certainty in the output distribution.\nThe gap between certainty and uncertainty lies at the heart of information theory. When Claude Shannon founded the field (see Shannon 1948), he was grappling with a fundamental question in telecommunications: how much data can you reliably transmit through a noisy communication channel? Telephone wires introduce random distortions like static and interference, scrambling the signal in unpredictable ways. Shannon realized he could model this mathematically using probability theory, and what began as a framework for analyzing communication over literal telephone wires evolved into an abstract mathematical theory with remarkably broad applications. Indeed, Shannon has been credited with laying the foundations for our modern information-based society.\nInterestingly, Shannon’s notion of entropy has deep connections to the concept of entropy in statistical mechanics and thermodynamics. In 1957, E. T. Jaynes famously formalized this connection in his influential paper (Jaynes 1957), where he wrote:\n\n“The mere fact that the same mathematical expression \\(-\\sum p_i \\log{p_i}\\) [for entropy] occurs both in statistical mechanics and in information theory does not in itself establish any connection between these fields. This can be done only by finding new viewpoints from which thermodynamic entropy and information-theory entropy appear as the same concept. In this paper we suggest a reinterpretation of statistical mechanics which accomplishes this, so that information theory can be applied to the problem of justification of statistical mechanics.”\n\nEven my undergraduate thermodynamics textbook devoted an entire chapter to Shannon’s information theory, emphasizing how these mathematical ideas provide a unifying perspective across seemingly different domains.\nWe will begin by surveying the most basic quantities of information theory: surprisal, entropy, Kullback–Leibler (KL) divergence, and mutual information. Surprisal is a quantity attached to a random outcome that is inversely linked to its probability: a likely outcome is not surprising, while an unlikely one is highly surprising. When we average surprisal over all possible random outcomes, we obtain the entropy, which quantifies the overall uncertainty in a probability distribution. The KL divergence measures how one probability distribution differs from another, capturing the “distance” between them. Mutual information can be viewed as a special kind of KL divergence applied to two random variables, \\(X\\) and \\(Y\\): it measures how much observing \\(X\\) reduces the uncertainty (entropy) in \\(Y\\) on average. This last quantity connects directly to our discussion of LLMs: it captures precisely how much the input prompt narrows down the output distribution.\nThis post is the first in a series on information. In future posts, we will explore other ways the concept of information appears—for example, through \\(\\sigma\\)-algebras—and apply these ideas to a range of problems, from gambling strategies and games of chance (the historical origin of mathematical probability theory), to options pricing in mathematical finance, and to probabilistic models in machine learning. I have discussed information theory previously in a chapter of my book; while some material overlaps with that chapter, this series also introduces many new perspectives and examples.\nIf you’d like to follow along with the code examples in this post, please see the dropdown code block below for the usual imports and setup.\n\n\nCode\n# Import probability distributions, integration, and plotting libraries\nfrom scipy.stats import norm, multivariate_normal, beta, poisson, binom, entropy\nfrom scipy.integrate import quad\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib.gridspec as gridspec\n\n\n# Set custom matplotlib style (user must use their own style file)\nplt.style.use(\"../../aux-files/custom-theme.mplstyle\")\n\n# Define color palette for plots\nyellow = \"#FFC300\"\nblue = \"#3399FF\"\npurple = \"#AA77CC\"\n\n\nclass RV:\n    \"\"\"\n    A class representing a random variable (discrete or continuous), with optional support for conditional densities.\n\n    Attributes\n    ----------\n    support : array-like or None\n        The support of the random variable (e.g., possible values for discrete, grid for continuous).\n    density : callable\n        The marginal density or mass function.\n    cond_density : callable or None\n        The conditional density function, if provided.\n    cond_support : array-like or None\n        The support for the conditional variable, if applicable.\n    density_array : np.ndarray or None\n        The marginal density evaluated on the support, if support is array-like.\n    _cond_density_array : dict or None\n        Precomputed conditional densities, if available.\n    \"\"\"\n\n    def __init__(\n        self,\n        support=None,\n        density=None,\n        cond_density=None,\n        cond_support=None,\n    ):\n        \"\"\"\n        Initialize an RV object.\n\n        Parameters\n        ----------\n        support : array-like or None\n            The support of the random variable (e.g., possible values for discrete, grid for continuous).\n        density : callable\n            The marginal density or mass function. Should accept a value (or array of values) and return the density/mass.\n        cond_density : callable, optional\n            The conditional density function f(x|y). Should accept (x, y) and return the density of x given y.\n        cond_support : array-like or None, optional\n            The support for the conditional variable, if applicable.\n        \"\"\"\n        self.support = support\n        self.density = density\n        self.cond_density = cond_density\n        self.cond_support = cond_support\n\n        # Precompute the marginal density array if possible\n        if support is not None and density is not None:\n            self.density_array = np.array([density(x) for x in support])\n        else:\n            self.density_array = None\n\n        # Precompute the conditional density array as a dictionary, if possible\n        if (\n            support is not None\n            and density is not None\n            and cond_density is not None\n            and cond_support is not None\n        ):\n            self._cond_density_array = {\n                y: cond_density(support, y) for y in cond_support\n            }\n        else:\n            self._cond_density_array = None\n\n    def pdf(self, x):\n        \"\"\"\n        Evaluate the marginal density or mass function at x.\n        \"\"\"\n        return self.density(x)\n\n    def pmf(self, x):\n        \"\"\"\n        Alias for pdf, for discrete random variables.\n        \"\"\"\n        return self.pdf(x)\n\n    def set_cond_density(self, cond_density):\n        \"\"\"\n        Set the conditional density function f(x|y).\n        \"\"\"\n        self.cond_density = cond_density\n\n    def cond_pdf(self, x, y):\n        \"\"\"\n        Evaluate the conditional density f(x|y).\n        Raises a ValueError if the conditional density function is not set.\n        \"\"\"\n        if self.cond_density is None:\n            raise ValueError(\"Conditional density function not set.\")\n        return self.cond_density(x, y)\n\n    def cond_density_array(self, y):\n        \"\"\"\n        Get the conditional density array f(x|y) for a fixed y.\n        Raises a ValueError if the conditional density array is not precomputed.\n        \"\"\"\n        if self._cond_density_array is None:\n            raise ValueError(\"Conditional density array not precomputed.\")\n        return self._cond_density_array[y]"
  },
  {
    "objectID": "posts/info-1/index.html#introduction",
    "href": "posts/info-1/index.html#introduction",
    "title": "Entropy & information",
    "section": "",
    "text": "Large language models (LLMs) like ChatGPT have brought probabilistic models into mainstream conversation. Unlike deterministic models that always produce the same output for a given input, these models generate a probability distribution over many possible outputs. Information theory provides the mathematical framework for understanding these distributions, and how sensitive they are to the inputs of the model.\nTo understand this more concretely, consider what happens when you prompt an LLM. You might ask “What’s the capital of France?” and reliably get “Paris” as the answer. The model is highly confident: the output probability distribution is concentrated over the one-word output “Paris”. But if you ask “Write me a poem about autumn,” you’ll get a different response each time. Here, the probability distribution is spread across countless possible poems, each with some small probability of being generated. In the first case, the model is certain, reflected in the strongly peaked output distribution, while in the latter case it is uncertain, reflected in the highly dispersed output distribution. The input to the model (i.e., the prompt) thus has a strong effect on the certainty in the output distribution.\nThe gap between certainty and uncertainty lies at the heart of information theory. When Claude Shannon founded the field (see Shannon 1948), he was grappling with a fundamental question in telecommunications: how much data can you reliably transmit through a noisy communication channel? Telephone wires introduce random distortions like static and interference, scrambling the signal in unpredictable ways. Shannon realized he could model this mathematically using probability theory, and what began as a framework for analyzing communication over literal telephone wires evolved into an abstract mathematical theory with remarkably broad applications. Indeed, Shannon has been credited with laying the foundations for our modern information-based society.\nInterestingly, Shannon’s notion of entropy has deep connections to the concept of entropy in statistical mechanics and thermodynamics. In 1957, E. T. Jaynes famously formalized this connection in his influential paper (Jaynes 1957), where he wrote:\n\n“The mere fact that the same mathematical expression \\(-\\sum p_i \\log{p_i}\\) [for entropy] occurs both in statistical mechanics and in information theory does not in itself establish any connection between these fields. This can be done only by finding new viewpoints from which thermodynamic entropy and information-theory entropy appear as the same concept. In this paper we suggest a reinterpretation of statistical mechanics which accomplishes this, so that information theory can be applied to the problem of justification of statistical mechanics.”\n\nEven my undergraduate thermodynamics textbook devoted an entire chapter to Shannon’s information theory, emphasizing how these mathematical ideas provide a unifying perspective across seemingly different domains.\nWe will begin by surveying the most basic quantities of information theory: surprisal, entropy, Kullback–Leibler (KL) divergence, and mutual information. Surprisal is a quantity attached to a random outcome that is inversely linked to its probability: a likely outcome is not surprising, while an unlikely one is highly surprising. When we average surprisal over all possible random outcomes, we obtain the entropy, which quantifies the overall uncertainty in a probability distribution. The KL divergence measures how one probability distribution differs from another, capturing the “distance” between them. Mutual information can be viewed as a special kind of KL divergence applied to two random variables, \\(X\\) and \\(Y\\): it measures how much observing \\(X\\) reduces the uncertainty (entropy) in \\(Y\\) on average. This last quantity connects directly to our discussion of LLMs: it captures precisely how much the input prompt narrows down the output distribution.\nThis post is the first in a series on information. In future posts, we will explore other ways the concept of information appears—for example, through \\(\\sigma\\)-algebras—and apply these ideas to a range of problems, from gambling strategies and games of chance (the historical origin of mathematical probability theory), to options pricing in mathematical finance, and to probabilistic models in machine learning. I have discussed information theory previously in a chapter of my book; while some material overlaps with that chapter, this series also introduces many new perspectives and examples.\nIf you’d like to follow along with the code examples in this post, please see the dropdown code block below for the usual imports and setup.\n\n\nCode\n# Import probability distributions, integration, and plotting libraries\nfrom scipy.stats import norm, multivariate_normal, beta, poisson, binom, entropy\nfrom scipy.integrate import quad\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib.gridspec as gridspec\n\n\n# Set custom matplotlib style (user must use their own style file)\nplt.style.use(\"../../aux-files/custom-theme.mplstyle\")\n\n# Define color palette for plots\nyellow = \"#FFC300\"\nblue = \"#3399FF\"\npurple = \"#AA77CC\"\n\n\nclass RV:\n    \"\"\"\n    A class representing a random variable (discrete or continuous), with optional support for conditional densities.\n\n    Attributes\n    ----------\n    support : array-like or None\n        The support of the random variable (e.g., possible values for discrete, grid for continuous).\n    density : callable\n        The marginal density or mass function.\n    cond_density : callable or None\n        The conditional density function, if provided.\n    cond_support : array-like or None\n        The support for the conditional variable, if applicable.\n    density_array : np.ndarray or None\n        The marginal density evaluated on the support, if support is array-like.\n    _cond_density_array : dict or None\n        Precomputed conditional densities, if available.\n    \"\"\"\n\n    def __init__(\n        self,\n        support=None,\n        density=None,\n        cond_density=None,\n        cond_support=None,\n    ):\n        \"\"\"\n        Initialize an RV object.\n\n        Parameters\n        ----------\n        support : array-like or None\n            The support of the random variable (e.g., possible values for discrete, grid for continuous).\n        density : callable\n            The marginal density or mass function. Should accept a value (or array of values) and return the density/mass.\n        cond_density : callable, optional\n            The conditional density function f(x|y). Should accept (x, y) and return the density of x given y.\n        cond_support : array-like or None, optional\n            The support for the conditional variable, if applicable.\n        \"\"\"\n        self.support = support\n        self.density = density\n        self.cond_density = cond_density\n        self.cond_support = cond_support\n\n        # Precompute the marginal density array if possible\n        if support is not None and density is not None:\n            self.density_array = np.array([density(x) for x in support])\n        else:\n            self.density_array = None\n\n        # Precompute the conditional density array as a dictionary, if possible\n        if (\n            support is not None\n            and density is not None\n            and cond_density is not None\n            and cond_support is not None\n        ):\n            self._cond_density_array = {\n                y: cond_density(support, y) for y in cond_support\n            }\n        else:\n            self._cond_density_array = None\n\n    def pdf(self, x):\n        \"\"\"\n        Evaluate the marginal density or mass function at x.\n        \"\"\"\n        return self.density(x)\n\n    def pmf(self, x):\n        \"\"\"\n        Alias for pdf, for discrete random variables.\n        \"\"\"\n        return self.pdf(x)\n\n    def set_cond_density(self, cond_density):\n        \"\"\"\n        Set the conditional density function f(x|y).\n        \"\"\"\n        self.cond_density = cond_density\n\n    def cond_pdf(self, x, y):\n        \"\"\"\n        Evaluate the conditional density f(x|y).\n        Raises a ValueError if the conditional density function is not set.\n        \"\"\"\n        if self.cond_density is None:\n            raise ValueError(\"Conditional density function not set.\")\n        return self.cond_density(x, y)\n\n    def cond_density_array(self, y):\n        \"\"\"\n        Get the conditional density array f(x|y) for a fixed y.\n        Raises a ValueError if the conditional density array is not precomputed.\n        \"\"\"\n        if self._cond_density_array is None:\n            raise ValueError(\"Conditional density array not precomputed.\")\n        return self._cond_density_array[y]"
  },
  {
    "objectID": "posts/info-1/index.html#flows-of-information",
    "href": "posts/info-1/index.html#flows-of-information",
    "title": "Entropy & information",
    "section": "Flows of information",
    "text": "Flows of information\nWe begin by building a mathematical gadget—a kind of probabilistic framework—that models the “flow of information” between two random variables \\(X\\) and \\(Y\\) (or random vectors, or random objects, or …). Such flows are exactly what information theory calls communication channels, and they include many of the predictive probabilistic models in machine learning where information flows from input \\(X\\) to output \\(Y\\). Once these flows have been identified in this section, in the rest of the post we seek to understand how information flowing from \\(X\\) influences and shapes the distribution of \\(Y\\).\nThe simplest flow between \\(X\\) and \\(Y\\) is a functional one, expressed as an equation \\[\ng(X)=Y,\n\\tag{1}\\]\nwhere \\(g\\) is a function. With \\(X\\) as input and \\(Y\\) as output, each \\(X=x\\) produces a unique output \\(y = g(x)\\). Such flows underlie deterministic models. In the case that \\(X\\) and \\(Y\\) take values on the real line \\(\\mathbb{R}\\), we might visualize the situation like this:\n\n\n\n\n\nNote that each value of \\(x\\) along the input (left) axis determines a unique value of \\(y\\) along the output (right) axis.\nOn the other hand, we might suppose that information flows from \\(X\\) to \\(Y\\) in a stochastic fashion, in which \\(X=x\\) no longer determines a single \\(y\\), but instead induces a distribution over possible \\(Y\\) values. This is precisely what a conditional distribution \\(P(Y= y\\mid X=x)\\) captures: given an observed value \\(X=x\\), we have a probability distribution on \\(y\\)’s. We can think of this as a function of the form\n\\[\nx \\mapsto P(Y= y \\mid X=x),\n\\tag{2}\\]\nwhere \\(y\\) plays the role of a variable rather than a fixed quantity, so that \\(P(Y= y \\mid X=x)\\) is a probability distribution and not just a single probability. So this function is rather special: its input is a value \\(x\\), while its output is an entire probability distribution. Mathematicians call such objects Markov kernels. A figure will make this more concrete: in contrast to the deterministic flows drawn above, in the case that \\(X\\) and \\(Y\\) take values on the real line \\(\\mathbb{R}\\), we visualize a stochastic flow as follows, where each value of \\(x\\) is mapped to a probability distribution on \\(y\\)’s:\n\n\n\n\n\nIn our model of a stochastic flow, we therefore use conditional distributions, understood as functions on the conditioning variable.\nIn practice, we often model such flows with a family \\(P(Y=y;\\theta)\\) of distributions parameterized by a vector \\(\\theta\\). The stochastic flow from \\(X\\) to \\(Y\\) is then implemented as a function \\(x\\mapsto \\theta(x)\\) from observations of \\(X\\) to parameters \\(\\theta\\), and the conditional distribution is then defined as\n\\[\nP(Y=y \\mid X=x) = P(Y=y ; \\theta=\\theta(x)).\n\\]\nLinear regression (with known variance \\(\\sigma^2\\)) is a familiar example: here \\(P(Y=y;\\theta)\\) has the normal density\n\\[\nf(y;\\theta) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left[ - \\frac{1}{2\\sigma^2}(y-\\theta)^2 \\right],\n\\]\nand with parameter mapping\n\\[\nx\\mapsto \\theta(x) = \\beta_0 + \\beta_1x,\n\\]\nfor some model coefficients \\(\\beta_0\\) and \\(\\beta_1\\). Concisely, the stochastic flow from \\(X\\) to \\(Y\\) in a linear regression model is completely described by specifying\n\\[\n(Y\\mid X=x) \\sim \\mathcal{N}(\\beta_0+\\beta_1x, \\sigma^2).\n\\]\nWe will return to an information-theoretic treatment of linear regression (and other) models in a later post.\nFor now, let’s see all this in action with real distributions in a real-world context. Suppose that \\(X\\) is the number of hours that a randomly chosen student studies for an upcoming exam, restricted to discrete values \\(X = 1,2,3,4,5,6\\). Let’s say that most students study only 2 or 3 hours, with its full distribution (mass function \\(f(x)\\)) shown below:\n\n\nCode\n# Compute the probability mass function for X (hours studied) using a Poisson distribution with mean 3\nfx_array = poisson.pmf(range(1, 7), mu=3)\nfx_array /= fx_array.sum()  # Normalize so probabilities sum to 1\n\n# Define a function to look up the probability for a given value of X\nfx = lambda x: fx_array[x - 1]\n\n# Create an RV object for X, specifying its support and marginal mass function\nX = RV(support=range(1, 7), density=fx)\n\n# Create a new matplotlib figure and axis with a specified size\n_, ax = plt.subplots(figsize=(6, 4))\n\n# Plot a bar chart of the probability mass function for X\nax.bar(X.support, X.density_array, width=0.4, zorder=2)\n\n# Label the x-axis as \"hours studied (x)\"\nax.set_xlabel(r\"hours studied ($x$)\")\n\n# Label the y-axis as \"probability\"\nax.set_ylabel(\"probability\")\n\n# Set the plot title to indicate this is the marginal mass function f(x)\nax.set_title(r\"marginal mass $f(x)$\")\n\n# Set the x-axis ticks to match the possible values of X\nax.set_xticks(X.support)\n\n# Adjust layout for better appearance and display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe might reasonably believe that \\(X\\) is predictive of \\(Y\\), the exam score of a randomly chosen student, taking continuous values in the interval \\([0,1]\\), understood as percentages. The corresponding marginal density \\(f(y)\\) is shown below:\n\n\nCode\n# Define the conditional density function fy_given_x(y, x) as a Beta(x, 3) distribution\nfy_given_x = lambda y, x: beta.pdf(y, a=x, b=3)\n\n# Define the marginal density function fy(y) as a mixture over x, weighted by fx(x)\nfy = lambda y: sum([fy_given_x(y, x) * fx(x) for x in range(1, 7)])\n\n# Create an RV object for Y, specifying its support, marginal density, and conditional density\nY = RV(\n    support=np.linspace(0, 1, num=250),  # Grid of possible y values (test scores)\n    density=fy,  # Marginal density function for Y\n    cond_density=fy_given_x,  # Conditional density function fy_given_x(y, x)\n    cond_support=range(1, 7),  # Possible values of x (hours studied)\n)\n\n# Create a new matplotlib figure and axis for plotting the marginal density of Y\n_, ax = plt.subplots(figsize=(6, 4))\n\n# Plot the marginal density fy(y) over the grid of y values\nax.plot(Y.support, Y.density_array)\n\n# Shade the area under the density curve for visual emphasis\nax.fill_between(Y.support, Y.density_array, zorder=2, alpha=0.1)\n\n# Format the x-axis labels as percentages (since y is a proportion)\nax.xaxis.set_major_formatter(PercentFormatter(xmax=1))\n\n# Set the plot title and axis labels\nax.set_title(r\"marginal density $f(y)$\")\nax.set_xlabel(\"test score ($y$)\")\nax.set_ylabel(\"probability density\")\n\n# Adjust layout for better appearance and display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nTogether, \\(X\\) and \\(Y\\) have a joint mass/density function \\(f(x,y)\\), visualized in the following ridgeline plot, where each of the horizontal density curves shows \\(f(x,y)\\) as a function of \\(y\\), for fixed \\(x=1,2,3,4,5,6\\).\n\n\nCode\n# Create a new matplotlib figure and axis with a specified size\n_, ax = plt.subplots(figsize=(6, 5))\n\n# Create a custom colormap for conditional distributions\nconditional_stops = [blue, purple, yellow]\nconditional_cmap = LinearSegmentedColormap.from_list(\n    \"conditional_cmap\", conditional_stops\n)\n\n# Generate a list of colors for each value of x using the custom colormap\nconditional_colors = [conditional_cmap(i / 5) for i in range(6)]\n\n# Loop over each possible value of x\nfor x in X.support:\n    # Compute the joint density values for each x, scaled for visualization\n    # This is f(y|x) * f(x), scaled for the ridgeline effect\n    joint_vals = 1.7 * Y.cond_density_array(x) * X.pdf(x)\n\n    # Fill the area between the baseline (x) and the curve (x + joint_vals) for ridgeline effect\n    ax.fill_between(\n        Y.support,\n        x,\n        x + joint_vals,\n        color=conditional_colors[x - 1],\n        zorder=2,\n        alpha=0.1,\n    )\n\n    # Plot the top edge of the density curve for each x\n    ax.plot(Y.support, x + joint_vals, color=conditional_colors[x - 1], zorder=2)\n\n# Label the y-axis as \"hours studied (x)\"\nax.set_ylabel(r\"hours studied ($x$)\")\n\n# Label the x-axis as \"test score (y)\"\nax.set_xlabel(r\"test score ($y$)\")\n\n# Format the x-axis labels as percentages\nax.xaxis.set_major_formatter(PercentFormatter(xmax=1))\n\n# Set the plot title to indicate this is the joint mass/density f(x, y)\nax.set_title(r\"joint mass/density $f(x,y)$\")\n\n# Adjust layout for better appearance and display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nDividing the joint mass/density \\(f(x,y)\\) by the marginal mass \\(f(x)\\) yields the conditional densities \\(f(y|x)\\). These are just the same density curves in the ridgeline plot above, normalized so that they integrate to \\(1\\) over \\([0,1]\\). They are shown in:\n\n\nCode\n# Create a new matplotlib figure and axis with a specified size\n_, ax = plt.subplots(figsize=(6, 4))\n\n# Loop over each possible value of x (hours studied)\nfor x in X.support:\n    # Plot the conditional density f(y|x) for each x as a Beta(x, 3) distribution\n    ax.plot(\n        Y.support,\n        Y.cond_density_array(x),\n        color=conditional_colors[x - 1],\n        label=x\n    )\n\n# Add a legend indicating the value of x for each curve\nax.legend(title=r\"hours studied ($x$)\", loc=\"center left\", bbox_to_anchor=(1, 0.5))\n\n# Format the x-axis labels as percentages (since y is a proportion)\nax.xaxis.set_major_formatter(PercentFormatter(xmax=1))\n\n# Set the plot title to indicate these are conditional densities f(y|x)\nax.set_title(r\"conditional densities $f(y|x)$\")\n\n# Label the axes\nax.set_xlabel(r\"test score ($y$)\")\nax.set_ylabel(\"probability density\")\n\n# Adjust layout for better appearance and display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn our information-theoretic terminology, the conditional density\n\\[\nx\\mapsto f(y|x),\n\\]\nthought of as a function of \\(x\\), models the stochastic flow of information from \\(X\\) to \\(Y\\).\nThe marginal density \\(f(y)\\) shows moderate uncertainty—it’s somewhat peaked, but not sharply. An exam score randomly drawn from \\(Y\\) will be mildly uncertain, mildly surprising. The exact amount of uncertainty in \\(Y\\) will be measured through its entropy, denoted \\(H(Y)\\), introduced in the next section. In contrast, the conditional densities \\(f(y|x)\\) exhibit less uncertainty compared to the marginal, especially for values of \\(x\\) closer to \\(6\\). The uncertainty remaining in \\(Y\\) after observing \\(X=x\\) is measured by the conditional entropy, denoted \\(H(Y\\mid X=x)\\). Averaging this conditional entropy over \\(X\\) yields the quantity\n\\[\nH(Y\\mid X) \\overset{\\text{def}}{=}E_{x\\sim f(x)}(H(Y\\mid X=x)),\n\\]\nthe average amount of uncertainty in \\(Y\\), given \\(X\\). Then, it is a general observation that\n\\[\nH(Y) \\geq H(Y\\mid X)\n\\]\nfor any pair of random variables \\(X\\) and \\(Y\\), reflecting the obvious fact that no additional information will ever increase the uncertainty in \\(Y\\). Thus, the quantity\n\\[\nI(X,Y) \\overset{\\text{def}}{=}H(Y) - H(Y\\mid X)\n\\]\nis a nonnegative proxy for the amount of information transmitted from \\(X\\) to \\(Y\\): if it is large, then the gap between \\(H(Y)\\) and \\(H(Y\\mid X)\\) is wide, indicating that observations of \\(X\\) greatly reduce the uncertainty in \\(Y\\). We understand this as a “large amount of information” is transmitted from \\(X\\) to \\(Y\\). Conversely, when \\(I(X,Y)\\) is small, observations of \\(X\\) reveal little about \\(Y\\); in the extreme case \\(I(X,Y)=0\\), the two are independent. The quantity \\(I(X,Y)\\) is exactly the mutual information between \\(X\\) and \\(Y\\), introduced in the next section."
  },
  {
    "objectID": "posts/info-1/index.html#surprisal-and-entropy",
    "href": "posts/info-1/index.html#surprisal-and-entropy",
    "title": "Entropy & information",
    "section": "Surprisal and entropy",
    "text": "Surprisal and entropy\nAs mentioned in the introduction, entropy measures the uncertainty in the outcome of a random variable. More precisely, it is the average surprisal of an observation. Surprisal varies inversely with probability: large probabilities yield small surprisals, and small probabilities yield large ones.\nThis inverse relationship is given by the function \\(s = -\\log{p}\\), linking a probability \\(p\\in [0,1]\\) with a surprisal \\(s\\in [0,\\infty)\\). The graph of this relationship is shown in:We write \\(\\log\\) for the base-\\(e\\) logarithm.\n\n\nCode\n# Create a grid of probability values from 0.01 to 1 (avoiding 0 to prevent log(0))\nmesh = np.linspace(0.01, 1, num=100)\n\n# Create a new matplotlib figure and axis with a specified size\n_, ax = plt.subplots(figsize=(4, 3))\n\n# Plot the surprisal function s = -log(p) as a function of probability p\nax.plot(mesh, -np.log(mesh), color=yellow)\n\n# Label the x-axis as probability (p)\nax.set_xlabel(r\"probability ($p$)\")\n\n# Label the y-axis as surprisal (s)\nax.set_ylabel(r\"surprisal ($s$)\")\n\n# Adjust layout for better appearance and display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAt first glance, many functions might seem equally capable of expressing this inverse relationship between probability and surprisal—so why the choice of base-\\(e\\) logarithm? It turns out that if one starts from a few natural axioms that any reasonable notion of surprisal should satisfy, then you can prove all such surprisal functions must be proportional to negative logarithms (see, for example, the discussion in Section 9 in Rioul 2021). The choice of base \\(e\\) is then somewhat arbitrary, akin to choosing units. Another popular choice is base \\(2\\), which aligns naturally with bit strings in coding theory. In base \\(e\\), information content is measured in so-called natural units, or nats; in base \\(2\\), it is measured in binary units, or bits. (See Section 10 in the aforementioned reference Rioul 2021 for more on units.)\nThis link between surprisals and probabilities may be extended to a link between surprisal and probability densities in the case that the probabilities are continuous. Since it is inconvenient to continually distinguish between mass and density functions in all definitions and theorems, we will follow the convention in measure-theoretic probability theory and refer to all probability mass and density functions as densities and denote them all by \\(f\\). In this scheme, a probability mass function really is a density function relative to the counting measure.\nWith this convention in mind, the following definition applies to both discrete and continuous random variables:\n\n\n\n\n\n\n\nDefinition 1 Let \\(X\\) and \\(Y\\) be two random variables with density functions \\(f(x)\\) and \\(f(y)\\), respectively.\n\nThe surprisal of an observed value \\(X=x\\) is the quantity \\[\n  s(x) = -\\log{f(x)}.\n  \\]\nThe conditional surprisal of an observed value \\(Y=y\\), given \\(X=x\\), is the quantity \\[\n  s(y|x) = -\\log{f(y|x)},\n  \\] where \\(f(y|x)\\) is the conditional density of \\(Y\\) given \\(X\\).\n\n\n\n\n\nFor a simple example of the relationship between discrete probabilities and surprisals, let’s bring back our random variable \\(X\\) from the previous section, which tallied the number of hours a randomly chosen student studied for the upcoming exam:\n\n\nCode\n# Create a figure with two subplots side by side\nfig, axes = plt.subplots(ncols=2, figsize=(6, 3))\n\n# Plot the marginal probability mass function f(x) as a bar chart on the first subplot\naxes[0].bar(X.support, X.density_array, width=0.4, zorder=2)\naxes[0].set_ylabel(r\"probability mass\")  # Label y-axis\naxes[0].set_title(r\"marginal density $f(x)$\")  # Set subplot title\naxes[0].set_xticks(X.support)  # Set x-ticks to match possible values of x\n\n# Plot the marginal surprisal s(x) = -log(f(x)) as a bar chart on the second subplot\naxes[1].bar(X.support, -np.log(X.density_array), width=0.4, zorder=2)\naxes[1].set_ylabel(r\"surprisal mass\")  # Label y-axis\naxes[1].set_title(r\"marginal surprisal $s(x)$\")  # Set subplot title\naxes[1].set_xticks(X.support)  # Set x-ticks to match possible values of x\n\n# Add a shared x-axis label for both subplots\nfig.supxlabel(r\"hours studied ($x$)\")\n\n# Adjust layout for better appearance and display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nBecause a probability density (mass) function of a discrete random variable must take values in \\([0,1]\\), its surprisal function is never negative. However, the probability density function of a continuous random variable may take on values larger than \\(1\\), which means that the associated surprisal density function can be negative. This can be seen for the continuous random variable \\(Y\\) from the previous section, whose density can exceed \\(1\\) in regions of high concentration—hence its surprisal can dip below zero.\n\n\nCode\n# Create a figure with two subplots side by side\nfig, axes = plt.subplots(ncols=2, figsize=(6, 3))\n\n# Plot the marginal probability density function f(y) on the first subplot\naxes[0].plot(Y.support, Y.density_array)\naxes[0].xaxis.set_major_formatter(\n    PercentFormatter(xmax=1)\n)  # Format x-axis as percentages\naxes[0].set_title(r\"marginal density $f(y)$\")  # Set subplot title\naxes[0].set_ylabel(\"probability density\")  # Label y-axis\n\n# Plot the marginal surprisal s(y) = -log(f(y)) on the second subplot\naxes[1].plot(Y.support[:-1], -np.log(Y.density_array[:-1]))\naxes[1].xaxis.set_major_formatter(\n    PercentFormatter(xmax=1)\n)  # Format x-axis as percentages\naxes[1].set_title(r\"marginal surprisal $s(y)$\")  # Set subplot title\naxes[1].set_ylabel(\"surprisal density\")  # Label y-axis\n\n# Add a shared x-axis label for both subplots\nfig.supxlabel(\"test score ($y$)\")\n\n# Adjust layout for better appearance and display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nHaving defined surprisal for individual outcomes, entropy emerges naturally as its average—capturing the typical “surprise” we can expect.\n\n\n\n\n\n\n\nDefinition 2 Let \\(X\\) and \\(Y\\) be two random variables with density functions \\(f(x)\\) and \\(f(y)\\), respectively.\n\nThe entropy of \\(X\\) is the quantity \\[\n  H(X) = E_{x\\sim f(x)}(s(x)).\n  \\]\nThe conditional entropy of \\(Y\\), given an observed value \\(X=x\\), is the quantity \\[\n  H(Y\\mid X=x) = E_{y\\sim f(y|x)}(s(y\\mid x)),\n  \\] where \\(f(y|x)\\) is the conditional density of \\(Y\\) given \\(X\\).\nThe conditional entropy of \\(Y\\), given \\(X\\), is the quantity\n\n\\[\nH(Y\\mid X) = E_{x\\sim f(x)}(H(Y\\mid X=x)).\n\\]\n\n\n\n\nIn the case that \\(X\\) is discrete, then the entropy \\(H(X)\\) is a sum of either a finite or countably infinite number of terms:\n\\[\nH(X) = \\sum_{x\\in \\mathbb{R}} f(x)s(x) = - \\sum_{x\\in \\mathbb{R}} f(x) \\log{f(x)}.\n\\]\nIf \\(X\\) is continuous, then the entropy is an integral:\n\\[\nH(X) = \\int f(x) s(x) \\ dx = - \\int f(x) \\log{f(x)} \\ dx,\n\\]\nwhere, by convention, \\(\\int\\) denotes integration over \\(\\mathbb{R}\\). In the literature, the entropy of a continuous random variable is often called differential entropy.\nThe stats submodule of the SciPy library contains a convenient method called entropy for computing entropies of discrete random variables. We use it to compute the entropy \\(H(X)\\), where \\(X\\) is the “hours studied” random variable:\n\nprint(f\"The probability mass function f(x) of X is:\\n\")\nfor x in X.support:\n    # Print the probability mass for each possible value of X (1 through n)\n    print(f\"    f({x}) =\", round(X.pdf(x), 3))\n\n# Compute and print the entropy H(X) using scipy's entropy function\nprint(f\"\\nThe entropy H(X) is {entropy(X.density_array):.3f}.\")\n\nThe probability mass function f(x) of X is:\n\n    f(1) = 0.163\n    f(2) = 0.244\n    f(3) = 0.244\n    f(4) = 0.183\n    f(5) = 0.11\n    f(6) = 0.055\n\nThe entropy H(X) is 1.698.\n\n\nWe can use the quad method in the integrate submodule of SciPy to compute differential entropies. For the “exam score” random variable \\(Y\\), we compute:\n\n# Compute the differential entropy H(Y) by integrating -f(y) * log(f(y)) over [0, 1]\ndiff_entropy, _ = quad(func=lambda y: -Y.pdf(y) * np.log(Y.pdf(y)), a=0, b=1)\n\n# Print the computed differential entropy value\nprint(f\"The differential entropy H(Y) is {diff_entropy:.3f}.\")\n\nThe differential entropy H(Y) is -0.131.\n\n\nNotice that \\(H(Y)\\) turns out to be negative—a reminder that differential entropy behaves quite differently from its discrete cousin."
  },
  {
    "objectID": "posts/info-1/index.html#kullbackleibler-divergence-and-mutual-information",
    "href": "posts/info-1/index.html#kullbackleibler-divergence-and-mutual-information",
    "title": "Entropy & information",
    "section": "Kullback–Leibler divergence and mutual information",
    "text": "Kullback–Leibler divergence and mutual information\nIn this section, we develop an information-theoretic way to measure how “far apart” two probability distributions are. By way of motivation, we consider two probability measures on a single finite probability space \\(\\Omega\\), so that the two measures have mass functions \\(f(\\omega)\\) and \\(g(\\omega)\\). The metric we’ll use is the mean logarithmic relative magnitude, a measure that captures not the absolute difference between probabilities, but how one probability scales relative to another. To define it, we first define the absolute relative magnitude of the probability \\(f(\\omega)\\) to the probability \\(g(\\omega)\\) as the ratio \\(f(\\omega)/g(\\omega)\\). Then, logarithmic relative magnitude refers to the base-\\(e\\) logarithm of the absolute relative magnitude:\n\\[\n\\log\\left( \\frac{f(\\omega)}{g(\\omega)} \\right).\n\\]\nIntuitively, this logarithm tells us the “order of magnitude” difference between \\(f(\\omega)\\) and \\(g(\\omega)\\). If \\(f(\\omega)\\approx e^k\\) and \\(g(\\omega)\\approx e^l\\), then the log ratio is roughly \\(k-l\\).\nPerhaps the most obvious immediate benefit of introducing the logarithm is that it yields a workable number when \\(f(\\omega)\\) and \\(g(\\omega)\\) each have widely different magnitudes. For example, let’s suppose that the mass functions \\(f(\\omega)\\) and \\(g(\\omega)\\) are given by\n\\[\nf(\\omega) = \\binom{10}{\\omega} (0.4)^\\omega(0.6)^{10-\\omega} \\quad \\text{and} \\quad g(\\omega) = \\binom{10}{\\omega} (0.9)^\\omega(0.1)^{10-\\omega}\n\\]\nfor \\(\\omega\\in \\{0,1,\\ldots,10\\}\\). These are the mass functions of a \\(\\mathcal{B}in(10,0.4)\\) and \\(\\mathcal{B}in(10,0.9)\\) random variable, respectively. We then plot histograms for these mass functions, along with histograms of the absolute and logarithmic relative magnitudes:\n\n\nCode\n# Define the possible values of omega (0 through 10)\nomegas = np.arange(0, 11)\n\n# Compute the probability mass functions for two Binomial distributions:\n# p: Binomial(n=10, p=0.4)\n# q: Binomial(n=10, p=0.9)\np = binom(n=10, p=0.4).pmf(omegas)\nq = binom(n=10, p=0.9).pmf(omegas)\n\n# Titles for each subplot\ntitles = [\n    \"$f(\\\\omega)$\",  # PMF of first distribution\n    \"$g(\\\\omega)$\",  # PMF of second distribution\n    \"$\\\\frac{f(\\\\omega)}{g(\\\\omega)}$\",  # Ratio of PMFs\n    \"$\\\\log\\\\left(\\\\frac{f(\\\\omega)}{g(\\\\omega)}\\\\right)$\",  # Log-ratio of PMFs\n]\n\n# Data to plot in each subplot\nprobs = [p, q, p / q, np.log(p / q)]\n\n# Y-axis limits for each subplot for better visualization\nylims = [(0, 0.4), (0, 0.4), (-50, 0.75e8), (-10, 20)]\n\n# Create a 2x2 grid of subplots\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 5), sharex=True)\n\n# Loop over each subplot, plotting the corresponding data\nfor title, prob, ylim, axis in zip(titles, probs, ylims, axes.flatten()):\n    axis.bar(omegas, prob, width=0.4, zorder=2)  # Bar plot for each omega\n    axis.set_xticks(ticks=omegas)  # Set x-ticks to omega values\n    axis.set_ylim(ylim)  # Set y-axis limits\n    axis.set_title(title)  # Set subplot title\n\n# Add a shared x-axis label for all subplots\nfig.supxlabel(\"$\\\\omega$\")\n\n# Adjust layout for better appearance and spacing\nplt.tight_layout()\nplt.subplots_adjust(hspace=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe second row makes the point vividly: the absolute relative magnitudes span such wildly different scales that the plot is almost useless, and numerical computations would be unstable. The logarithmic version, by contrast, stays well-behaved and informative.\nWe obtain a single-number summary of the logarithmic relative magnitudes by taking their mean with respect to the mass function \\(f(\\omega)\\), giving us the number\n\\[\nE_{\\omega\\sim f(\\omega)} \\left[\\log\\left( \\frac{f(\\omega)}{g(\\omega)} \\right)\\right] = \\sum_{\\omega\\in \\Omega} f(\\omega) \\log\\left( \\frac{f(\\omega)}{g(\\omega)} \\right).\n\\tag{3}\\]\nObserve that we could have instead computed the mean relative to \\(g(\\omega)\\), giving us the number\n\\[\nE_{\\omega\\sim g(\\omega)} \\left[\\log\\left( \\frac{f(\\omega)}{g(\\omega)} \\right)\\right] = \\sum_{\\omega\\in \\Omega} g(\\omega) \\log\\left( \\frac{f(\\omega)}{g(\\omega)} \\right).\n\\tag{4}\\]\nBut notice that\n\\[\nE_{\\omega\\sim g(\\omega)} \\left[\\log\\left( \\frac{f(\\omega)}{g(\\omega)} \\right)\\right] = - E_{\\omega\\sim g(\\omega)} \\left[\\log\\left( \\frac{g(\\omega)}{f(\\omega)} \\right)\\right],\n\\]\nwhere the right-hand side is the negative of a number of the form (3). So, at least up to sign, it doesn’t really matter which of the two numbers (3) or (4) that we use to develop our theory. Our choice of (3) has the benefit of making the KL divergence nonnegative when the distributions are discrete.\nThese considerations lead us to:\n\n\n\n\n\n\n\nDefinition 3 Let \\(X\\) and \\(Y\\) be two random variables with density functions \\(f(x)\\) and \\(g(x)\\), respectively. The Kullback–Leibler divergence (or just KL divergence) from \\(X\\) to \\(Y\\), denoted \\(D(X \\parallel Y)\\), is the mean logarithmic relative magnitude:\n\\[\nD(X \\parallel Y) = E_{x\\sim f(x)} \\left[ \\log \\left( \\frac{f(x)}{g(x)}\\right) \\right].\n\\]\n\n\n\n\nTechnically, \\(D(X\\parallel Y)\\) is defined only when \\(f(x)=0\\) implies \\(g(x)=0\\) for all \\(x\\)—a condition known as absolute continuity in measure theory. If \\(X\\) and \\(Y\\) are continuous, then \\(D(X\\parallel Y)\\) is often called the differential KL divergence.\nFor some examples of differential KL divergences, let’s consider the conditional random variables \\(Y\\mid X=x\\) from the previous section, which give the exam score \\(Y\\) of a randomly chosen student if they had studied \\(X=x\\) hours (for \\(x=1,2,\\ldots,6\\)). In the figure below, we plot the densities \\(f(y\\mid x)\\) of the conditional distributions and compute the five differential KL divergences\n\\[\nD\\left( (Y\\mid X=1) \\parallel (Y\\mid X=x) \\right)\n\\]\nfor \\(x=2,3,4,5,6\\).\n\n\nCode\n# Define the integrand for KL divergence between two Beta distributions:\ndef integrand(y, x1, x2):\n    return Y.cond_pdf(y, x1) * np.log(Y.cond_pdf(y, x1) / Y.cond_pdf(y, x2))\n\n\n# Compute KL divergence D((Y|X=1) || (Y|X=x)) for x = 2, 3, 4, 5, 6\nKL_div = {x: quad(func=integrand, args=(1, x), a=0, b=1)[0] for x in X.support[1:]}\n\n# Set up a 2x6 grid for custom subplot arrangement\nfig = plt.figure(figsize=(8, 5))\ngs = gridspec.GridSpec(2, 6, figure=fig)\nax1 = fig.add_subplot(gs[0, 0:2])\nax2 = fig.add_subplot(gs[0, 2:4])\nax3 = fig.add_subplot(gs[0, 4:6])\nax4 = fig.add_subplot(gs[1, 1:3])\nax5 = fig.add_subplot(gs[1, 3:5])\naxes = [ax1, ax2, ax3, ax4, ax5]\n\n# For each subplot, plot the two conditional densities and annotate with KL divergence\nfor x, ax in zip(X.support[1:], axes):\n    # Plot f(y|x=1) in blue\n    ax.plot(Y.support, Y.cond_density_array(1), color=blue, zorder=2, label=\"x = 1\")\n    ax.fill_between(Y.support, Y.cond_density_array(1), zorder=2, color=blue, alpha=0.1)\n\n    # Plot f(y|x) in yellow\n    ax.plot(\n        Y.support, Y.cond_density_array(x), color=yellow, zorder=2, label=f\"x = {x}\"\n    )\n    ax.fill_between(\n        Y.support, Y.cond_density_array(x), zorder=2, color=yellow, alpha=0.1\n    )\n\n    # Annotate with the computed KL divergence\n    ax.set_title(f\"KL div. = {KL_div[x]:.3f}\")\n    ax.set_ylim(0, 4)\n    ax.legend(loc=\"upper right\")\n\n# Adjust layout and spacing for better appearance\nplt.tight_layout()\nplt.subplots_adjust(hspace=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nEach subplot contains the blue density curve \\(f(y\\mid x=1)\\), along with a yellow density curve \\(f(y\\mid x)\\) for \\(x=2,3,4,5,6\\). As \\(x\\) gets larger, we can see visually that the densities become more unalike; this increasing dissimilarity is reflected in larger KL divergences as \\(x\\) gets larger.\nAt the other end of the spectrum, we have \\(D(X \\parallel Y) = 0\\) when \\(X\\) and \\(Y\\) are identically distributed. And, at least when the variables are discrete, it is a basic but important fact that we always have \\(D(X \\parallel Y)\\geq 0\\), with equality if and only if \\(X\\) and \\(Y\\) are identically distributed; this is referred to as Gibbs’ inequality (see here for a proof). So, the KL divergence has several properties that make it a good measure for the “distance” between two probability distributions. However, note that this distance is not symmetric, in the sense that we have\n\\[\nD(X\\parallel Y) \\neq D(Y \\parallel X)\n\\]\nin general.\nKL divergence measures how one distribution differs from another. To study relationships between random variables, we apply it to their joint and marginal distributions. If \\(X\\) and \\(Y\\) are independent, their joint density factors as the product of their marginals, \\(f(x,y)=f(x)f(y)\\). Thus, a measure of the “information flow” between \\(X\\) and \\(Y\\) is the distance—in the sense of KL divergence—from the true joint density \\(f(x,y)\\) to the product densities \\(f(x)f(y)\\). This leads us to:\n\n\n\n\n\n\n\nDefinition 4 Let \\(X\\) and \\(Y\\) be two random variables with density functions \\(f(x)\\) and \\(f(y)\\). The mutual information shared between \\(X\\) and \\(Y\\), denoted \\(I(X,Y)\\), is the quantity\n\\[\nI(X,Y) = E_{(x,y)\\sim f(x,y)} \\left[ \\log \\left( \\frac{f(x,y)}{f(x)f(y)} \\right) \\right].\n\\tag{5}\\]\n\n\n\n\nThe product \\(f(x)f(y)\\) is the density of some probability distribution on \\(\\mathbb{R}^2\\), which would coincide with the true joint probability distribution (with density \\(f(x,y)\\)) if the variables were independent. So, the mutual information is the KL divergence between two probability distributions on \\(\\mathbb{R}^2\\), and we have \\(I(X,Y)=0\\) when \\(X\\) and \\(Y\\) are independent.\nAs an example, we return once more to our random variables \\(X\\) and \\(Y\\), the “hours studied” discrete variable and the “exam score” continuous variable. In this case, the joint distribution of \\(X\\) and \\(Y\\) is a mixed discrete-continuous one, so the formula (5) gives\n\\[\nI(X,Y) = \\sum_{x=1}^6 \\int_0^1 f(x,y) \\log\\left(\\frac{f(x,y)}{f(x)f(y)} \\right) \\ dy,\n\\]\nwhere \\(f(x,y)\\) is the true joint mass-density function and \\(f(x)\\) and \\(f(y)\\) are the marginal mass and densities, respectively. We implement this formula directly in Python, using the quad method in the integrate submodule of SciPy for integration:\n\n# Compute the true joint density f(x, y) = f(x) * f(y|x)\ndef fxy(x, y):\n    return X.pdf(x) * Y.cond_pdf(y, x)\n\n\n# Define the integrand for mutual information:\n# f(x, y) * log(f(x, y) / (f(x) * f(y)))\ndef integrand(x, y):\n    return fxy(x, y) * np.log(fxy(x, y) / (X.pdf(x) * Y.pdf(y)))\n\n\n# For each x, create a function of y for integration over y in [0, 1]\nfuncs = [lambda y, x=x: integrand(x, y) for x in range(1, 7)]\n\n# Compute the mutual information by summing the integrals over y for each x\nmutual_info = sum([quad(func, a=0, b=1)[0] for func in funcs])\n\n# Print the computed mutual information I(X, Y)\nprint(f\"The mutual information I(X,Y) is {mutual_info:.3f}.\")\n\nThe mutual information I(X,Y) is 0.201.\n\n\nUsing the definitions of marginal and conditional entropies given in Definition 2, one easily proves that the mutual information \\(I(X,Y)\\) may be computed as described in:\n\n\n\n\n\n\n\nTheorem 1 (Mututal information is entropy) Let \\(X\\) and \\(Y\\) be two random variables. Then\n\\[\nI(X,Y) = H(Y) - H(Y\\mid X).\n\\tag{6}\\]\n\n\n\n\nThus, the mutual information measures the amount of entropy in \\(Y\\) that is “leftover” after having observed \\(X\\). In other words, it quantifies how much knowing \\(X\\) reduces uncertainty about \\(Y\\).\nWe end this section by using formula (6) to re-do our computation of the mutual information \\(I(X,Y)\\) from above. We get the same answer:\n\n# the differential entropy H(Y) is stored in `diff_entropy`\n\n# For each x, define a function of y for the conditional entropy integrand: -f(y|x) * log(f(y|x))\nfuncs = [lambda y, x=x: -Y.cond_pdf(y, x) * np.log(Y.cond_pdf(y, x)) for x in X.support]\n\n# Compute the conditional entropy H(Y|X=x) for each x by integrating over y in [0, 1]\ncond_entropies = [quad(func, a=0, b=1)[0] for func in funcs]\n\n# Compute H(Y) - sum_x f(x) * H(Y|X=x), which equals the mutual information I(X, Y)\ndiff_entropy - sum([cond_entropies[x - 1] * X.pdf(x) for x in X.support])\n\n# Print the previously computed mutual information for comparison\nprint(f\"The mutual information I(X,Y) is {mutual_info:.3f}.\")\n\nThe mutual information I(X,Y) is 0.201."
  },
  {
    "objectID": "posts/info-1/index.html#mutual-information-of-jointly-normal-random-variables",
    "href": "posts/info-1/index.html#mutual-information-of-jointly-normal-random-variables",
    "title": "Entropy & information",
    "section": "Mutual information of jointly normal random variables",
    "text": "Mutual information of jointly normal random variables\nUseful intuition for the mutual information \\(I(X,Y)\\) arises from the simple case of jointly normal variables, where a closed-form expression can be obtained. As a first step toward this formula, we compute the differential entropy of a single normal random variable:For background on normal random vectors, see here.\n\n\n\n\n\n\n\nTheorem 2 (Entropy of a normal random variable) If \\(X\\sim \\mathcal{N}(\\mu,\\sigma^2)\\), then\n\\[\nH(X) = \\frac{1}{2}\\log(2\\pi e \\sigma^2),\n\\]\nwhere \\(e\\) is the base of the natural logarithm.\n\n\n\n\n\n\n\n\n\n\nNoteProof.\n\n\n\n\n\nLetting \\(f(x)\\) be the density of \\(X\\), we compute:\n\\[\n\\begin{align*}\nH(X) &= -\\int f(x) \\log{f(x)} \\ dx \\\\\n&= - \\int f(x) \\log\\left\\{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left[ -\\frac{1}{2\\sigma^2}(x-\\mu)^2 \\right]\\right\\} \\ dx \\\\\n&= - \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) \\int f(x) \\ dx + \\frac{1}{2\\sigma^2}\\int f(x)(x-\\mu)^2 \\ dx \\\\\n&= \\frac{1}{2}\\log(2\\pi \\sigma^2) + \\frac{1}{2} \\\\\n&= \\frac{1}{2}\\log(2\\pi e \\sigma^2)\n\\end{align*}\n\\]\nwhere we’ve used \\(\\int f(x) \\ dx =1\\) and \\(\\int f(x)(x-\\mu)^2 \\ dx = \\sigma^2\\).\n\n\n\nIt is well known that the conditional distributions of a normal random vector are themselves normal; we include here the result for a \\(2\\)-dimensional normal random vector. The proof follows directly from standard properties of multivariate normal distributions and is omitted for brevity.\n\n\n\n\n\n\n\nTheorem 3 (Conditional distributions of normal vectors are normal) Let \\((X,Y) \\sim \\mathcal{N}_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) be a \\(2\\)-dimensional normal random vector with\n\\[\n\\boldsymbol{\\mu}= \\begin{bmatrix} \\mu_X \\\\ \\mu_Y \\end{bmatrix} \\quad \\text{and} \\quad \\boldsymbol{\\Sigma}= \\begin{bmatrix}\n\\sigma_X^2 & \\rho \\sigma_X \\sigma_Y \\\\\n\\rho \\sigma_X \\sigma_Y & \\sigma_Y^2\n\\end{bmatrix},\n\\]\nwhere \\(X \\sim \\mathcal{N}(\\mu_X,\\sigma_X^2)\\), \\(Y\\sim \\mathcal{N}(\\mu_Y,\\sigma_Y^2)\\), and \\(\\rho\\) is the correlation of \\(X\\) and \\(Y\\). Then\n\\[\n(Y \\mid X=x) \\sim \\mathcal{N}\\left(\\mu_Y + (x-\\mu_X) \\frac{\\rho \\sigma_Y}{\\sigma_X}, \\ \\sigma_Y^2(1-\\rho^2) \\right)\n\\]\nfor all \\(x\\).\n\n\n\n\nThe next result contains the formula for the mutual information of two jointly normal random variables; its proof is an easy application of our previous results. Notice the mutual information only depends on the correlation between the variables, as intuition might suggest.\n\n\n\n\n\n\n\nTheorem 4 (Mutual information of jointly normal variables) Let \\((X,Y) \\sim \\mathcal{N}_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) be a \\(2\\)-dimensional normal random vector. Then\n\\[\nI(X,Y) = -\\frac{1}{2} \\log \\left(1-\\rho^2 \\right),\n\\]\nwhere \\(\\rho\\) is the correlation of \\(X\\) and \\(Y\\).\n\n\n\n\n\n\n\n\n\n\nNoteProof.\n\n\n\n\n\nFrom Theorem 2 and Theorem 3, we get that\n\\[\nH(Y\\mid X=x) = \\frac{1}{2}\\log\\left(2\\pi e \\sigma_Y^2(1-\\rho^2)\\right),\n\\]\nwhere \\(\\sigma_Y\\) is the standard deviation of \\(Y\\). Since this does not depend on \\(x\\), we have\n\\[\nH(Y\\mid X) = E_{x\\sim f(x)}\\left(H(Y\\mid X=x) \\right) = \\frac{1}{2}\\log\\left(2\\pi e \\sigma_Y^2(1-\\rho^2)\\right),\n\\]\nwhere \\(f(x)\\) is the marginal density of \\(X\\). But another application of Theorem 2 gives\n\\[\nH(Y) = \\frac{1}{2}\\log(2\\pi e \\sigma_Y^2),\n\\]\nand so by Theorem 1 we have\n\\[\n\\begin{align*}\nI(X,Y) &= H(Y) - H(Y\\mid X) \\\\\n&= \\frac{1}{2}\\log(2\\pi e \\sigma_Y^2) - \\frac{1}{2}\\log\\left(2\\pi e \\sigma_Y^2(1-\\rho^2)\\right) \\\\\n&= -\\frac{1}{2} \\log \\left(1-\\rho^2 \\right).\n\\end{align*}\n\\]\n\n\n\nHence, the larger the correlation between the marginal normals, the larger the mutual information. To see this in action with a concrete example, let’s suppose that we have \\((X,Y) \\sim \\mathcal{N}_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), where\n\\[\n\\boldsymbol{\\mu}= \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\quad \\text{and} \\quad \\boldsymbol{\\Sigma}= \\begin{bmatrix}\n1 & 2\\rho  \\\\\n2\\rho  & 4\n\\end{bmatrix},\n\\]\nand \\(\\rho\\) is the correlation of \\(X\\) and \\(Y\\) (hence the marginal standard deviations are \\(\\sigma_X = 1\\) and \\(\\sigma_Y=2\\)). In the second and third plots below, we have selected two correlations \\(\\rho=0.5,0.85\\) and computed the corresponding mutual information \\(I(X,Y)\\). The isoprobability contours of the joint normal density \\(f(x,y)\\) are shown in yellow, while the conditional normal densities \\(f(y|x)\\) are shown in blue for each of \\(x=-1, 0, 1\\). For comparison, the marginal density \\(f(y)\\) has been shown in the first plot.\n\n\nCode\n# Define a function to plot contours of a bivariate normal distribution\ndef plot_multivar_norm(ax, muX, muY, sigmaX, sigmaY, x, y, labels=False):\n    # Construct the covariance matrix using the specified correlation rho\n    Sigma = np.array(\n        [[sigmaX**2, rho * sigmaX * sigmaY], [rho * sigmaX * sigmaY, sigmaY**2]]\n    )\n    Mu = np.array([muX, muY])  # Mean vector\n    U = multivariate_normal(mean=Mu, cov=Sigma)  # Multivariate normal object\n    grid = np.dstack((x, y))  # Create a grid for evaluation\n    z = U.pdf(grid)  # Evaluate the PDF on the grid\n    contour = ax.contour(x, y, z, colors=yellow, alpha=0.3)  # Plot contours\n    if labels:\n        ax.clabel(contour, inline=True, fontsize=8)  # Optionally label contours\n\n\n# Define a function to plot the conditional density f(y|x) for a given x_obs\ndef plot_conditional(\n    ax, muX, muY, sigmaX, sigmaY, rho, y_mesh, x_obs, magnification_factor=1\n):\n    # Compute conditional mean and standard deviation for Y|X=x_obs\n    mu = muY + (x_obs - muX) * rho * sigmaY / sigmaX\n    sigma = sigmaY * np.sqrt(1 - rho**2)\n    # Compute and scale the conditional normal density\n    x = magnification_factor * norm(loc=mu, scale=sigma).pdf(y_mesh)\n    # Plot the conditional density horizontally, shifted to align with x_obs\n    ax.plot(-x + x_obs, y_mesh, color=blue)\n    ax.fill_betweenx(y_mesh, -x + x_obs, x_obs, color=blue, alpha=0.4)\n\n\n# Set parameters for the bivariate normal distribution\nmuX = 0\nmuY = 0\nsigmaX = 1\nsigmaY = 2\nrhos = [0.5, 0.85]  # Correlation values to illustrate\nx_obs = [-1, 0, 1]  # Observed x values for conditional plots\nx, y = np.mgrid[-2.1:2.1:0.01, -5:5:0.01]  # Grid for contour plot\ny_mesh = np.linspace(-5, 5, num=250)  # Grid for conditional densities\n\n# Create a figure with three subplots: one for the marginal, two for different correlations\nfig, axes = plt.subplots(\n    ncols=3, figsize=(8, 4), sharey=True, gridspec_kw={\"width_ratios\": [1, 4, 4]}\n)\n\n# Plot the marginal density of Y on the first subplot (as a horizontal density)\nmagnification_factor = 2.5\nx_marginal = magnification_factor * norm(scale=sigmaY).pdf(y_mesh)\naxes[0].plot(-x_marginal, y_mesh, color=blue)\naxes[0].set_xlim(-1, 0)\naxes[0].fill_betweenx(y_mesh, -x_marginal, 0, color=blue, alpha=0.4)\naxes[0].yaxis.tick_right()\naxes[0].spines[\"left\"].set_visible(False)\naxes[0].spines[\"right\"].set_visible(True)\naxes[0].spines[\"bottom\"].set_visible(False)\naxes[0].set_xticks([])\n\n# For each correlation value, plot the joint contours and conditional densities\nfor rho, ax in zip(rhos, axes[1:]):\n    plot_multivar_norm(ax, muX, muY, sigmaX, sigmaY, x, y)\n    for x_ob in x_obs:\n        plot_conditional(\n            ax,\n            muX,\n            muY,\n            sigmaX,\n            sigmaY,\n            rho,\n            y_mesh,\n            x_obs=x_ob,\n            magnification_factor=3,\n        )\n    # Compute and display the mutual information for this correlation\n    info = -(1 / 2) * np.log(1 - rho**2)\n    ax.set_title(rf\"$\\rho ={rho}$, $I(X,Y)= {info:0.3f}$\")\n    ax.set_xlabel(r\"$x$\")\n    ax.set_xlim(-2.2, 2.2)\n    ax.set_xticks(range(-2, 3))\n\n# Label the y-axis on the first subplot\naxes[0].set_ylabel(r\"$y$\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nCompared to the marginal distribution of \\(Y\\), the conditional distributions become increasingly concentrated as \\(\\rho\\) increases. This stronger concentration reflects the reduced uncertainty in \\(Y\\) when \\(X\\) is observed. This example illustrates the intuition behind mutual information: greater correlation implies stronger dependence, smaller conditional entropy, and thus higher mutual information."
  },
  {
    "objectID": "posts/info-1/index.html#conclusion",
    "href": "posts/info-1/index.html#conclusion",
    "title": "Entropy & information",
    "section": "Conclusion",
    "text": "Conclusion\nWe began by examining how information flows between random variables, distinguishing deterministic flows (where input uniquely determines output) from stochastic flows (where input induces a probability distribution over outputs). This distinction captures the fundamental difference between classical predictive models and modern probabilistic ones like large language models.\nTo quantify information and uncertainty, we introduced the core concepts of information theory: surprisal, entropy, KL divergence, and mutual information. Surprisal measures how unexpected an outcome is, inversely related to its probability through the logarithm. Entropy emerges as the average surprisal—capturing the overall uncertainty in a probability distribution. For our exam score example, we computed \\(H(X) \\approx 1.698\\) for hours studied and \\(H(Y) \\approx -0.131\\) for exam scores, illustrating how differential entropy can be negative for continuous variables.\nThe KL divergence \\(D(X \\parallel Y)\\) quantifies how one distribution differs from another by measuring the mean logarithmic relative magnitude between their densities. While not a true metric (it’s asymmetric), it provides a principled way to measure distributional distance. This led naturally to mutual information \\(I(X,Y)\\), which applies KL divergence to the joint and marginal distributions, measuring how much knowing one variable reduces uncertainty about the other.\nThe relationship \\(I(X,Y) = H(Y) - H(Y \\mid X)\\) reveals mutual information as the reduction in entropy: the gap between uncertainty before and after observing additional information. For our student example, we found \\(I(X,Y) \\approx 0.201\\), indicating that knowing study hours provides modest but measurable information about exam performance. In the jointly normal case, we obtained the elegant formula \\(I(X,Y) = -\\frac{1}{2}\\log(1-\\rho^2)\\), showing mutual information depends only on correlation.\nThese information-theoretic quantities provide a unified mathematical framework for understanding uncertainty, dependence, and information flow in probabilistic systems. In future posts, we’ll extend these ideas through the lens of \\(\\sigma\\)-algebras and apply them to gambling strategies, options pricing in mathematical finance, and the analysis of probabilistic models in machine learning. The concepts developed here—viewing information as reduction in entropy—will prove essential for understanding learning over time and decision-making under uncertainty."
  },
  {
    "objectID": "posts/info-2/index.html",
    "href": "posts/info-2/index.html",
    "title": "Algebras & information",
    "section": "",
    "text": "How do we mathematically represent what someone knows? This question sits at the heart of probability theory, information theory, and decision-making under uncertainty. When an experiment produces an outcome but we don’t observe it directly, our knowledge is partial: we might know some facts about the outcome while remaining uncertain about others. This partial knowledge needs a precise mathematical framework.\nConsider a simple scenario: three coin flips produce a binary sequence like \\(011\\) or \\(101\\) where a \\(0\\) indicates that a tail was produced and \\(1\\) a head, but you don’t see the full result. If someone tells you only the outcome of the first flip, you’ve gained information, but how much? If they then reveal the second flip, you learn more, but again, how much? These questions about information content and uncertainty reduction require both a qualitative structure (what can you distinguish?) and a quantitative measure (how uncertain are you?).\nThe qualitative structure is captured by algebras of sets, which formalize the collection of events an observer can decide about. If you know the first flip, you can decide whether the full sequence starts with \\(0\\) or \\(1\\), partitioning the eight possible outcomes into two groups. Knowing the first two flips refines this partition into four groups, and so on. Each partition represents a distinct “information state,” and the nested structure\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3\n\\]\ncaptures progressive learning over time.\nBut algebras alone don’t quantify uncertainty; they only describe logical structure. To measure how much information each partition contains, we need entropy, along with the related notions of conditional entropy and mutual information. As partitions refine, conditional entropy decreases (less residual uncertainty) while mutual information increases (more shared information between algebras).\nIn a previous post, we introduced entropy, conditional entropy, and mutual information for probability distributions on finite sets. Here, we develop the same ideas from a measure-theoretic perspective, showing how information naturally emerges from the algebraic structure of events. This dual perspective (probabilistic and set-theoretic) provides both conceptual clarity and the foundation for extensions to infinite spaces, stochastic processes, and dynamical systems.\nWe’ll work primarily with a concrete example: three coin flips and the nested algebras they generate. Through explicit computations in R, we’ll see how entropy quantifies information at each stage of learning. The mathematical machinery we develop (algebras, partitions, and their correspondence) forms the backbone of modern probability theory. These ideas have powerful applications in gambling problems and games of chance, where players accumulate information over time, and in options pricing in mathematical finance, where nested filtrations model traders’ evolving information and determine fair prices for derivative securities. We will explore these latter themes in future posts.\nThe idea that algebras (and their \\(\\sigma\\)-algebra generalizations) can represent “information states” is familiar to probability theorists. For example, these ideas appear throughout (Billingsley 1995), which also discusses gambling strategies, while (Björk 2020) provides a perspective from mathematical finance. Information-theoretic measures like entropy are applied to \\(\\sigma\\)-algebras in ergodic theory and other areas; for an introduction, see (Walters 1982).\nIf you’d like to follow along with the code examples in this post, please see the dropdown code block below for the usual imports and setup.\n\n\nCode\n# Load required packages\nlibrary(\"ggplot2\") # For plotting\nlibrary(\"knitr\") # For table formatting (used with kable())\nlibrary(\"dplyr\") # For data manipulation\nlibrary(\"latex2exp\") # For LaTeX rendering in plots\n\n# Source custom theme file\n# Note: Replace this path with your own theme file location\nsource(\"../../aux-files/custom-theme.R\")\n\n# Extract custom colors from theme\n# Note: These color definitions depend on your custom theme file\nyellow &lt;- custom_colors[[\"yellow\"]]\nblue &lt;- custom_colors[[\"blue\"]]\n\n# Apply custom ggplot2 theme\ntheme_set(custom_theme())"
  },
  {
    "objectID": "posts/info-2/index.html#introduction",
    "href": "posts/info-2/index.html#introduction",
    "title": "Algebras & information",
    "section": "",
    "text": "How do we mathematically represent what someone knows? This question sits at the heart of probability theory, information theory, and decision-making under uncertainty. When an experiment produces an outcome but we don’t observe it directly, our knowledge is partial: we might know some facts about the outcome while remaining uncertain about others. This partial knowledge needs a precise mathematical framework.\nConsider a simple scenario: three coin flips produce a binary sequence like \\(011\\) or \\(101\\) where a \\(0\\) indicates that a tail was produced and \\(1\\) a head, but you don’t see the full result. If someone tells you only the outcome of the first flip, you’ve gained information, but how much? If they then reveal the second flip, you learn more, but again, how much? These questions about information content and uncertainty reduction require both a qualitative structure (what can you distinguish?) and a quantitative measure (how uncertain are you?).\nThe qualitative structure is captured by algebras of sets, which formalize the collection of events an observer can decide about. If you know the first flip, you can decide whether the full sequence starts with \\(0\\) or \\(1\\), partitioning the eight possible outcomes into two groups. Knowing the first two flips refines this partition into four groups, and so on. Each partition represents a distinct “information state,” and the nested structure\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3\n\\]\ncaptures progressive learning over time.\nBut algebras alone don’t quantify uncertainty; they only describe logical structure. To measure how much information each partition contains, we need entropy, along with the related notions of conditional entropy and mutual information. As partitions refine, conditional entropy decreases (less residual uncertainty) while mutual information increases (more shared information between algebras).\nIn a previous post, we introduced entropy, conditional entropy, and mutual information for probability distributions on finite sets. Here, we develop the same ideas from a measure-theoretic perspective, showing how information naturally emerges from the algebraic structure of events. This dual perspective (probabilistic and set-theoretic) provides both conceptual clarity and the foundation for extensions to infinite spaces, stochastic processes, and dynamical systems.\nWe’ll work primarily with a concrete example: three coin flips and the nested algebras they generate. Through explicit computations in R, we’ll see how entropy quantifies information at each stage of learning. The mathematical machinery we develop (algebras, partitions, and their correspondence) forms the backbone of modern probability theory. These ideas have powerful applications in gambling problems and games of chance, where players accumulate information over time, and in options pricing in mathematical finance, where nested filtrations model traders’ evolving information and determine fair prices for derivative securities. We will explore these latter themes in future posts.\nThe idea that algebras (and their \\(\\sigma\\)-algebra generalizations) can represent “information states” is familiar to probability theorists. For example, these ideas appear throughout (Billingsley 1995), which also discusses gambling strategies, while (Björk 2020) provides a perspective from mathematical finance. Information-theoretic measures like entropy are applied to \\(\\sigma\\)-algebras in ergodic theory and other areas; for an introduction, see (Walters 1982).\nIf you’d like to follow along with the code examples in this post, please see the dropdown code block below for the usual imports and setup.\n\n\nCode\n# Load required packages\nlibrary(\"ggplot2\") # For plotting\nlibrary(\"knitr\") # For table formatting (used with kable())\nlibrary(\"dplyr\") # For data manipulation\nlibrary(\"latex2exp\") # For LaTeX rendering in plots\n\n# Source custom theme file\n# Note: Replace this path with your own theme file location\nsource(\"../../aux-files/custom-theme.R\")\n\n# Extract custom colors from theme\n# Note: These color definitions depend on your custom theme file\nyellow &lt;- custom_colors[[\"yellow\"]]\nblue &lt;- custom_colors[[\"blue\"]]\n\n# Apply custom ggplot2 theme\ntheme_set(custom_theme())"
  },
  {
    "objectID": "posts/info-2/index.html#algebras-as-information-carriers",
    "href": "posts/info-2/index.html#algebras-as-information-carriers",
    "title": "Algebras & information",
    "section": "Algebras as information carriers",
    "text": "Algebras as information carriers\nAn observer monitors the result of some experiment. The set of all possible results is collected into a sample space \\(\\Omega\\), whose elements \\(\\omega\\) are called outcomes or sample points. Suppose that the experiment is finished, so that a definite \\(\\omega_0\\) in \\(\\Omega\\) is produced, but that the exact identity of \\(\\omega_0\\) remains unknown to the observer. However, suppose that the observer has access to a set \\(\\mathcal{F}\\) of subsets of \\(\\Omega\\) with the following property:\n\n\n\n\n\n\nFor each \\(A\\in \\mathcal{F}\\), the observer can decide whether \\(\\omega_0\\in A\\) or not.\n\n\n\nThe observer’s decisions reflect the information they possess, so we can treat the class \\(\\mathcal{F}\\) as a proxy for that information. Sets in \\(\\mathcal{F}\\) are called (decidable) events. If the observer confirms that \\(\\omega_0\\in A\\) for some \\(A\\in \\mathcal{F}\\), then we say that “the event \\(A\\) has occurred”.\nIt is never in question that \\(\\omega_0\\in \\Omega\\), since the same space \\(\\Omega\\) contains all possible outcomes, so it is natural to include \\(\\Omega\\) in the class \\(\\mathcal{F}\\) of decidable events. But what other properties must \\(\\mathcal{F}\\) have?\n\nIf the observer can determine whether an event \\(A\\) has occurred, then they obviously can determine whether \\(A\\) did not occur. This means that if \\(A\\in \\mathcal{F}\\), then \\(A^c \\in \\mathcal{F}\\) too. \nIf the observer can determine separately whether events \\(A\\) and \\(B\\) have occurred, then they can determine whether \\(A\\) or \\(B\\) has occurred. Since \\(A\\cup B\\) consists of those outcomes that are either in \\(A\\) or \\(B\\), this means that \\(A\\cup B\\) must be in \\(\\mathcal{F}\\) when both \\(A\\) and \\(B\\) are.\nSimilarly, if the observer can determine separately whether events \\(A\\) and \\(B\\) have occurred, then they can determine whether \\(A\\) and \\(B\\) has occurred. This implies that \\(\\mathcal{F}\\) must contain the intersection \\(A\\cap B\\) whenever it contains both \\(A\\) and \\(B\\).\n\nHere, \\(A^c\\) is the absolute complement of \\(A\\), consisting of those \\(\\omega\\in \\Omega\\) that are not in \\(A\\).These three properties capture what it means for the observer’s collection of events \\(\\mathcal{F}\\) to represent a logically coherent state of information. They lead us naturally to the formal definition of an algebra of sets. Actually, we need not explicitly require that \\(\\mathcal{F}\\) contains all (binary) intersections of events, since this follows from properties (1) and (2). Note also that \\(\\mathcal{F}\\) must contain the empty set, since it contains \\(\\Omega\\) and is closed under complements.\n\n\n\n\n\n\n\nDefinition 1 Let \\(\\mathcal{F}\\) be a set of subsets of a set \\(\\Omega\\). Then \\(\\mathcal{F}\\) is called an algebra of sets (in \\(\\Omega\\)) if it has the following properties:\n\nIt contains \\(\\Omega\\).\nIt is closed under (absolute) complementation:\n\n\\[\n  A\\in \\mathcal{F}\\quad \\Rightarrow \\quad A^c\\in \\mathcal{F}.\n  \\]\n\nIt is closed under binary unions:\n\n\\[\n  A_1, A_2\\in \\mathcal{F}\\quad\\Rightarrow \\quad A_1 \\cup A_2 \\in \\mathcal{F}.\n  \\]\n\n\n\n\nWe have thus argued that our class \\(\\mathcal{F}\\) of decidable events—encoding our observer’s information—must be an algebra of sets in the sample space \\(\\Omega\\).\nAs we indicated in the margin note above, algebras of sets are also closed under binary intersections. Moreover, an easy inductive proof shows that algebras are also closed under all finite unions and intersections, not just binary ones. In this post we’ll stick to finite sample spaces, but in the infinite case one usually strengthens these closure properties to include countable unions, yielding \\(\\sigma\\)-algebras of sets."
  },
  {
    "objectID": "posts/info-2/index.html#example-coin-tosses",
    "href": "posts/info-2/index.html#example-coin-tosses",
    "title": "Algebras & information",
    "section": "Example: coin tosses",
    "text": "Example: coin tosses\nIn practice, the information encoded in \\(\\mathcal{F}\\) is often most naturally described by a partition that generates it. A partition is a collection of sets\n\\[\n\\{A_1,A_2,\\ldots,A_n\\}\n\\]\nwhere each \\(A_k \\subset \\Omega\\), the sets \\(A_k\\)​ are mutually disjoint, and their union exhausts the sample space: \\(\\Omega = \\bigcup_{k=1}^n A_k\\)​.\nThe intuition is simple: an observer’s information typically comes from knowing the answer to some question. Each possible answer groups together certain outcomes, and these groups form a partition of \\(\\Omega\\). The algebra \\(\\mathcal{F}\\) is then the smallest algebra containing all sets in this partition. We will study partitions in more detail in the next section, but for now we see how it works in a simple example.\nWe consider the sample space \\(\\Omega\\) consisting of all binary sequences of length 3:\n\\[\n\\Omega = \\{000, 001, 010, 011, 100, 101, 110, 111\\}.\n\\]\nWe would visualize this sample space as follows:\n\n\n\n\n\nWe imagine that a binary sequence \\(\\omega\\) in \\(\\Omega\\) represents the result of flipping a coin three times in succession, with \\(0\\) when a tail occurs, and \\(1\\) when a head is shown. We let \\(X_i(\\omega)\\) denote the result of the \\(i\\)-th flip in \\(\\omega\\) (so \\(X_i(\\omega)=0\\) or \\(1\\)), and we write\n\\[\nY = X_1 + X_2 + X_3.\n\\]\nThe four functions \\(X_1\\), \\(X_2\\), \\(X_3\\), and \\(Y\\) are, of course, random variables on \\(\\Omega\\). If the coin has probability \\(\\theta\\) of showing heads, then \\(X_i \\sim \\mathcal{B}er(\\theta)\\) for each \\(i\\), and \\(Y\\sim \\mathcal{B}in(3,\\theta)\\).\nWe will be carrying out some basic computations later, so let’s implement our scenario in the machine, by coding a data frame in which each row represents an outcome \\(\\omega\\):\n\n\nCode\n# Create the sample space Omega as a data frame\n# Each row represents one of the 8 possible outcomes (binary sequences of length 3)\nOmega &lt;- data.frame(\n  X1 = c(0, 0, 0, 0, 1, 1, 1, 1), # First coin flip: 0 = tail, 1 = head\n  X2 = c(0, 0, 1, 1, 0, 0, 1, 1), # Second coin flip\n  X3 = c(0, 1, 0, 1, 0, 1, 0, 1) # Third coin flip\n)\n\n# Add random variable Y: total number of heads across all three flips\nOmega &lt;- Omega %&gt;%\n  mutate(Y = X1 + X2 + X3)\n\n# Display the sample space as a formatted table\nkable(\n  Omega,\n  caption = \"sample space $\\\\Omega$ and random variables $X_1$, $X_2$, $X_3$, and $Y$\"\n)\n\n\n\nsample space \\(\\Omega\\) and random variables \\(X_1\\), \\(X_2\\), \\(X_3\\), and \\(Y\\)\n\n\nX1\nX2\nX3\nY\n\n\n\n\n0\n0\n0\n0\n\n\n0\n0\n1\n1\n\n\n0\n1\n0\n1\n\n\n0\n1\n1\n2\n\n\n1\n0\n0\n1\n\n\n1\n0\n1\n2\n\n\n1\n1\n0\n2\n\n\n1\n1\n1\n3\n\n\n\n\n\n\nNow, suppose that the coin was flipped three times, producing a definite outcome \\(\\omega_0\\) in \\(\\Omega\\), but that its exact identity is unknown to our observer. The only information available is the answer to the question: “What is the outcome of the first flip?”. This information allows the observer to partition the sample space \\(\\Omega\\) and generate an algebra \\(\\mathcal{F}_1\\), just as we described at the beginning of this section.\nThis partition consists of the sets\n\\[\nA_1 = \\{000,001,010,011\\} \\quad \\text{and} \\quad A_2 = \\{100,101,110,111\\},\n\\]\nwhere \\(A_1\\) contains outcomes corresponding to the answer “tail” and \\(A_2\\) those corresponding to “head.” As you can easily check, the set\n\\[\n\\mathcal{F}_1 = \\{\\emptyset, A_1, A_2, \\Omega\\}\n\\]\nis an algebra of sets in \\(\\Omega\\). We would visualize this partition as follows:\n\n\n\n\n\nLet’s insert a column in our data frame tracking the membership of each outcome \\(\\omega\\) (i.e., row) in \\(A_1\\) or \\(A_2\\):\n\n\nCode\n# Group outcomes by the result of the first flip (X1)\n# and assign a group ID to track which partition set (A1 or A2) each outcome belongs to\nOmega &lt;- Omega %&gt;%\n  group_by(X1) %&gt;%\n  mutate(Ai = cur_group_id()) %&gt;% # Ai = 1 for A1 (X1=0), Ai = 2 for A2 (X1=1)\n  ungroup() %&gt;%\n  as.data.frame()\n\n# Display the updated sample space showing partition membership\nkable(Omega, caption = \"column Ai records the index $i$ of $A_i$\")\n\n\n\ncolumn Ai records the index \\(i\\) of \\(A_i\\)\n\n\nX1\nX2\nX3\nY\nAi\n\n\n\n\n0\n0\n0\n0\n1\n\n\n0\n0\n1\n1\n1\n\n\n0\n1\n0\n1\n1\n\n\n0\n1\n1\n2\n1\n\n\n1\n0\n0\n1\n2\n\n\n1\n0\n1\n2\n2\n\n\n1\n1\n0\n2\n2\n\n\n1\n1\n1\n3\n2\n\n\n\n\n\n\nNow, suppose the observer is also given the answer to the question: “What is the outcome of the second flip?” Combining this with the information from the first flip, the observer can now distinguish outcomes based on the results of the first two flips. This additional information generates a finer partition of \\(\\Omega\\) and produces a second algebra \\(\\mathcal{F}_2\\), which contains the first:\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2.\n\\]\nInclusions of algebras represent refinements of information.\nThe new partition consists of the sets\n\\[\nB_1 = \\{000,001\\}, \\quad B_2 = \\{010,011\\}, \\quad B_3 = \\{100,101\\}, \\quad B_4 = \\{110,111\\},\n\\]\nwhere \\(B_1\\) corresponds to the answer “first and second flips are both tails”, \\(B_2\\) to “first flip tail, second flip head”, and so on. The algebra that is generated by this partition is\n\\[\n\\mathcal{F}_2 = \\{\\emptyset, B_1, B_2, B_3, B_4, B_1 \\cup B_2, B_3 \\cup B_4, B_1\\cup B_3, B_1\\cup B_4, B_2\\cup B_3, B_2\\cup B_4, \\Omega \\}.\n\\tag{1}\\]\nAs expected, \\(\\mathcal{F}_2\\) contains \\(\\mathcal{F}_1\\) since \\(B_1\\cup B_2 = A_1\\) and \\(B_3\\cup B_4 = A_2\\), reflecting that the observer’s information has been refined. Along with the first partition induced by \\(\\mathcal{F}_1\\), we would visualize this finer partition as follows:\n\n\n\n\n\nLet’s insert our new algebra into the machine:\n\n\nCode\n# Group outcomes by the results of the first two flips (X1 and X2)\n# and assign a group ID to track which partition set (B1, B2, B3, or B4) each outcome belongs to\nOmega &lt;- Omega %&gt;%\n  group_by(X1, X2) %&gt;%\n  mutate(Bi = cur_group_id()) %&gt;% # Bi = 1 for B1, Bi = 2 for B2, etc.\n  ungroup() %&gt;%\n  as.data.frame()\n\n# Display the updated sample space showing partition membership for the refined algebra F2\nkable(Omega, caption = \"column Bi records the index $i$ of $B_i$\")\n\n\n\ncolumn Bi records the index \\(i\\) of \\(B_i\\)\n\n\nX1\nX2\nX3\nY\nAi\nBi\n\n\n\n\n0\n0\n0\n0\n1\n1\n\n\n0\n0\n1\n1\n1\n1\n\n\n0\n1\n0\n1\n1\n2\n\n\n0\n1\n1\n2\n1\n2\n\n\n1\n0\n0\n1\n2\n3\n\n\n1\n0\n1\n2\n2\n3\n\n\n1\n1\n0\n2\n2\n4\n\n\n1\n1\n1\n3\n2\n4\n\n\n\n\n\n\nFinally, suppose the observer is given the answer to the question: “What is the outcome of the third flip?” With this information, the observer can now distinguish outcomes based on all three flips, completely specifying \\(\\omega_0\\). This generates the finest partition of \\(\\Omega\\) and produces a third algebra \\(\\mathcal{F}_3\\), which refines the information in \\(\\mathcal{F}_2\\) (and thus also \\(\\mathcal{F}_1\\)):\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3.\n\\]\nThe new partition must contain the eight singletons\n\\[\n\\begin{gather*}\nC_1 = \\{000\\}, \\ C_2 = \\{001\\}, \\ C_3 = \\{010\\}, \\ C_4 = \\{011\\}, \\\\\nC_5 = \\{100\\}, \\ C_6 = \\{101\\}, \\ C_7 = \\{110\\}, \\ C_8 = \\{111\\},\n\\end{gather*}\n\\]\nwhere each \\(C_i\\) corresponds to the answer to the question, “Which sequence of heads and tails occurred in all three flips?”. Knowing which singleton contains \\(\\omega_0\\) completely identifies the outcome. To form an algebra, \\(\\mathcal{F}_3\\) must include not only these singletons but also all possible unions of them; in other words, \\(\\mathcal{F}_3\\) is the power set of \\(\\Omega\\), containing all subsets of the sample space. Along with the first two partitions induced by \\(\\mathcal{F}_1\\) and \\(\\mathcal{F}_2\\), we would visualize this finest partition as follows:\n\n\n\n\n\nWe insert one last column in our data frame for the \\(C_i\\)’s:\n\n\nCode\n# Group outcomes by all three flips (X1, X2, and X3)\n# Each combination of (X1, X2, X3) uniquely identifies one outcome\nOmega &lt;- Omega %&gt;%\n  group_by(X1, X2, X3) %&gt;%\n  mutate(Ci = cur_group_id()) %&gt;% # Ci = 1 for C1, Ci = 2 for C2, etc.\n  ungroup() %&gt;%\n  as.data.frame()\n\n# Display the updated sample space showing the finest partition (singletons)\n# At this level, each outcome is completely distinguishable\nkable(Omega, caption = \"column Ci records the index $i$ of $C_i$\")\n\n\n\ncolumn Ci records the index \\(i\\) of \\(C_i\\)\n\n\nX1\nX2\nX3\nY\nAi\nBi\nCi\n\n\n\n\n0\n0\n0\n0\n1\n1\n1\n\n\n0\n0\n1\n1\n1\n1\n2\n\n\n0\n1\n0\n1\n1\n2\n3\n\n\n0\n1\n1\n2\n1\n2\n4\n\n\n1\n0\n0\n1\n2\n3\n5\n\n\n1\n0\n1\n2\n2\n3\n6\n\n\n1\n1\n0\n2\n2\n4\n7\n\n\n1\n1\n1\n3\n2\n4\n8\n\n\n\n\n\n\nTo recap, the sets \\(A_k\\), \\(B_k\\), and \\(C_k\\) form partitions of \\(\\Omega\\), with each successive partition being finer than the last: each \\(A_k\\) is a union of \\(B_k\\)’s, and each \\(B_k\\) is a union of \\(C_k\\)’s. These sets are also the atoms of their respective algebras, meaning they do not properly contain any nonempty subset in the corresponding algebra.\nTwo outcomes \\(\\omega_1\\) and \\(\\omega_2\\) in the same \\(A_k\\) are indistinguishable using only the information in \\(\\mathcal{F}_1\\). Likewise, outcomes in the same \\(B_k\\) are indistinguishable with respect to \\(\\mathcal{F}_2\\). The \\(C_k\\), by contrast, correspond to complete information in \\(\\mathcal{F}_3\\), so no ambiguity remains.\nEach algebra in the chain of inclusions\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3\n\\]\ncaptures a distinct level of information about the outcome \\(\\omega_0\\). This nested structure provides a simple model of learning over time: as the experiment unfolds and the observer acquires new data, their algebra of decidable events expands, splitting sets of previously indistinguishable outcomes into finer partitions—until, at last, \\(\\mathcal{F}_3\\) represents complete knowledge of \\(\\omega_0\\)."
  },
  {
    "objectID": "posts/info-2/index.html#algebras-and-partitions",
    "href": "posts/info-2/index.html#algebras-and-partitions",
    "title": "Algebras & information",
    "section": "Algebras and partitions",
    "text": "Algebras and partitions\nIn this section, we nail down the ideas from the previous section on partitions and algebras and highlight four useful theorems connecting them. Some proofs get a bit technical, so feel free to skim and focus on the statements. For simplicity, we stick to finite sets \\(\\Omega\\), though many results hold for infinite sets too.\nSo, let \\(\\Omega\\) be a finite set and \\(\\mathcal{F}\\) an algebra of sets in \\(\\Omega\\). Indistinguishability of two elements \\(\\omega\\) and \\(\\omega'\\) of \\(\\Omega\\) (relative to \\(\\mathcal{F}\\)) occurs exactly when we have\n\\[\n\\omega \\in A \\quad \\Leftrightarrow \\quad \\omega' \\in A, \\quad \\forall A\\in \\mathcal{F}.\n\\tag{2}\\]\nElements \\(\\omega\\) and \\(\\omega'\\) related in this way will be called \\(\\mathcal{F}\\)-equivalent.\nThis notion of equivalence is evidently an equivalence relation in the technical sense, and thus we obtain a partition of \\(\\Omega\\) into the distinct equivalence classes. These equivalence classes are actually themselves contained in \\(\\mathcal{F}\\), since \\(\\mathcal{F}\\) is closed under intersections. In fact, if \\(E\\) is an equivalence class containing an outcome \\(\\omega\\in \\Omega\\), then \\(E\\) is the intersection of all \\(A\\in \\mathcal{F}\\) that contain \\(\\omega\\):\n\\[\nE = \\bigcap_{\\omega \\in A \\in \\mathcal{F}} A.\n\\]\n(Verification of this equality is left to the reader.) But even more than being elements of \\(\\mathcal{F}\\), these equivalence classes are atomic in the sense of the following definition.\n\n\n\n\n\n\n\nDefinition 2 Let \\(\\mathcal{F}\\) be an algebra of sets in a finite set \\(\\Omega\\). A set \\(A\\in \\mathcal{F}\\) is called an atom if it is non-reducible, in the sense that \\(A\\) contains no nonempty subset of \\(\\mathcal{F}\\) other than itself.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1 (Atomic theorm) Let \\(\\mathcal{F}\\) be an algebra of sets in a finite set \\(\\Omega\\).\n\nThere exists a partition \\(\\mathcal{A}\\) of \\(\\Omega\\) consisting entirely of atoms in \\(\\mathcal{F}\\). In fact, \\(\\mathcal{A}\\) may be chosen to consist of the distinct \\(\\mathcal{F}\\)-equivalence classes.\nIf \\(\\mathcal{A}\\) is a partition of \\(\\Omega\\) consisting of atoms in \\(\\mathcal{F}\\), then every nonempty set in \\(\\mathcal{F}\\) is a union of atoms in \\(\\mathcal{A}\\).\nLet \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) be two partitions of \\(\\Omega\\) consisting of atoms in \\(\\mathcal{F}\\). Then \\(\\mathcal{A}= \\mathcal{B}\\).\n\n\n\n\n\n\n\n\n\n\n\nNoteProof.\n\n\n\n\n\n(1): Since the distinct \\(\\mathcal{F}\\)-equivalence classes partition \\(\\Omega\\), we just need to show that a given equivalence class is non-reducible. So, let \\(E\\) be an \\(\\mathcal{F}\\)-equivalence class, and suppose that \\(A\\in \\mathcal{F}\\) is a nonempty subset of \\(E\\). Choose \\(\\omega'\\in A\\) and \\(\\omega \\in E\\). Then \\(\\omega'\\in E\\) as well, and so \\(\\omega\\) and \\(\\omega'\\) are \\(\\mathcal{F}\\)-equivalent. But then \\(\\omega'\\in A\\) implies \\(\\omega\\in A\\), and since \\(\\omega\\) was an arbitrarily chosen element of \\(E\\), we must have \\(E\\subset A\\). But \\(A\\) began as a subset of \\(E\\), and so \\(E=A\\).\n(2): Since the atoms in \\(\\mathcal{A}\\) partition \\(\\Omega\\), it will suffice to show that every nonempty set \\(S\\in \\mathcal{F}\\) is either disjoint from a given atom in \\(A\\in\\mathcal{A}\\), or \\(S\\) contains \\(A\\) as a subset. But if \\(S\\cap A\\) is nonempty, then we must have \\(S\\cap A = A\\) since \\(S\\cap A\\in \\mathcal{F}\\) and \\(A\\) is non-reducible. But then \\(A\\subset S\\), as desired.\n(3): First, choose an atom \\(A\\in \\mathcal{A}\\). Since \\(\\mathcal{B}\\) is a partition of \\(\\Omega\\), there must exist a \\(B\\in \\mathcal{B}\\) such that \\(A\\cap B\\) is nonempty. But then\n\\[\nA = A\\cap B =B,\n\\]\nwhere the first equalities follow from the fact that \\(A\\cap B\\) is in \\(\\mathcal{F}\\) and non-reducibility of \\(A\\) and \\(B\\). Thus \\(A\\in \\mathcal{B}\\), which proves \\(\\mathcal{A}\\subset \\mathcal{B}\\). A symmetric argument proves the opposite containment, and hence \\(\\mathcal{A}= \\mathcal{B}\\).\n\n\n\nSo, every algebra of sets \\(\\mathcal{F}\\) in a finite set \\(\\Omega\\) naturally gives rise to a unique partition \\(\\mathcal{A}\\) of \\(\\Omega\\) consisting of atoms in \\(\\mathcal{F}\\). Conversely, as we saw in the coin-flip example, a given partition \\(\\mathcal{A}\\) of \\(\\Omega\\) can be used to generate an algebra, which we denote by \\(\\sigma(\\mathcal{A})\\). By construction, \\(\\sigma(\\mathcal{A})\\) is the smallest algebra in \\(\\Omega\\) that contains all sets in \\(\\mathcal{A}\\), i.e., \\(\\mathcal{A}\\subset \\sigma(\\mathcal{A})\\).\n\n\n\n\n\n\n\nTheorem 2 (Generating algebras via atoms) Let \\(\\mathcal{A}\\) be a partition of a finite set \\(\\Omega\\).\n\nEvery set in \\(\\sigma(\\mathcal{A})\\) is a union of sets in \\(\\mathcal{A}\\).\nThe sets in \\(\\mathcal{A}\\) are atoms in the algebra \\(\\sigma(\\mathcal{A})\\).\n\n\n\n\n\nTo see this theorem in action, I encourage you to scroll back and look at the algebra \\(\\mathcal{F}_2\\) in (1) in the previous section, generated by the partition \\(\\{B_1,B_2,B_3,B_4\\}\\). Notice that \\(\\mathcal{F}_2\\) does indeed consist of all unions of the \\(B_k\\)’s.\n\n\n\n\n\nNoteProof.\n\n\n\n\n\n(1): Since \\(\\sigma(\\mathcal{A})\\) is an algebra, it must contain the set \\(\\mathcal{F}\\) of all unions of sets in \\(\\mathcal{A}\\), so we have\n\\[\n\\mathcal{A}\\subset \\mathcal{F}\\subset \\sigma(\\mathcal{A}).\n\\]\nThe point is that \\(\\mathcal{F}\\) is already an algebra, and hence \\(\\mathcal{F}= \\sigma(\\mathcal{A})\\). To prove this, note that \\(\\Omega \\in \\mathcal{F}\\) since the union of all sets in \\(\\mathcal{A}\\) is \\(\\Omega\\). Clearly \\(\\mathcal{F}\\) is closed under unions, so we need only show that it is closed under absolute complements. But because the sets in \\(\\mathcal{A}\\) are pairwise disjoint and their union is \\(\\Omega\\), the complement of a union of sets in \\(\\mathcal{A}\\) is another set of the same form, and hence \\(\\mathcal{F}\\) is indeed closed under unions. This proves the claim \\(\\mathcal{F}= \\sigma(\\mathcal{A})\\), and thereby establishes the first statement.\n(2): Let \\(A\\in \\mathcal{A}\\) and suppose \\(S\\in \\mathcal{F}\\) is a nonempty subset of \\(A\\). But then \\(S\\) is a union of sets in \\(\\mathcal{A}\\), and since the sets in \\(\\mathcal{A}\\) are pairwise disjoint, we must have \\(S=A\\).\n\n\n\nThe mapping \\(\\mathcal{A}\\mapsto \\sigma(\\mathcal{A})\\) associating every partition \\(\\mathcal{A}\\) of a finite set \\(\\Omega\\) with the algebra \\(\\sigma(\\mathcal{A})\\) is injective. Indeed, if \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) are two partitions such that \\(\\sigma(\\mathcal{A}) = \\sigma(\\mathcal{B})\\), then Theorem 2 shows that \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) are both partitions of the same algebra and both consist of atoms; then Theorem 1 shows \\(\\mathcal{A}= \\mathcal{B}\\). On the other hand, if \\(\\mathcal{F}\\) is an algebra of sets in \\(\\Omega\\), then a partition \\(\\mathcal{A}\\) of \\(\\Omega\\) consisting of atoms in \\(\\mathcal{F}\\) exists, by Theorem 1, and all sets in \\(\\mathcal{F}\\) are unions of atoms. But then \\(\\mathcal{F}= \\sigma(\\mathcal{A})\\) by Theorem 2. We have thus proved:\n\n\n\n\n\n\n\nTheorem 3 (Algebras \\(=\\) Partitions) Let \\(\\Omega\\) be a finite set. The mapping \\(\\mathcal{A}\\mapsto \\sigma(\\mathcal{A})\\) is a bijection from the class of all partitions of \\(\\Omega\\) onto the class of all algebras of sets in \\(\\Omega\\).\n\n\n\n\nFinally, if \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\) are two algebras of sets in \\(\\Omega\\), we say \\(\\mathcal{F}\\) refines \\(\\mathcal{G}\\) if \\(\\mathcal{G}\\subset \\mathcal{F}\\). As we saw in the coin-flip example, refinements of algebras corresponds to refinement of information.\n\n\n\n\n\n\n\nTheorem 4 (Refinement Theorem) Let \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\) be two algebras of sets in a finite set \\(\\Omega\\) with associated partitions \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) consisting of atoms, respectively. Then \\(\\mathcal{F}\\) refines \\(\\mathcal{G}\\) if and only if every set \\(B\\in \\mathcal{B}\\) is a union of sets in \\(\\mathcal{A}\\).\n\n\n\n\n\n\n\n\n\n\nNoteProof.\n\n\n\n\n\nFirst, supposing that \\(\\mathcal{F}\\) refines \\(\\mathcal{G}\\), we have\n\\[\n\\mathcal{B}\\subset \\sigma(\\mathcal{B}) = \\mathcal{G}\\subset \\mathcal{F}= \\sigma(\\mathcal{A}),\n\\]\nwhere the two equalities follow from Theorem 3. The desired conclusion then follows from part (1) of Theorem 2. Conversely, if every \\(B\\in \\mathcal{B}\\) is a union of sets in \\(\\mathcal{A}\\), then we get the subset containment in\n\\[\n\\mathcal{B}\\subset \\sigma(\\mathcal{A}) = \\mathcal{F},\n\\]\nwhile the equality again follows from Theorem 3. But then\n\\[\n\\mathcal{G}= \\sigma(\\mathcal{B}) \\subset \\mathcal{F},\n\\]\nwhere the equality follows (one more time) from Theorem 3 and the subset containment from the fact \\(\\mathcal{F}\\) is an algebra containing \\(\\mathcal{B}\\). Thus \\(\\mathcal{F}\\) refines \\(\\mathcal{G}\\).\n\n\n\nThe correspondence between algebras and partitions gives us two complementary perspectives on information. The algebraic view emphasizes logical operations: we can decide membership in events, take complements, form unions and intersections. The partition view emphasizes distinguishability: outcomes in the same partition set are indistinguishable, while outcomes in different sets can be told apart. Theorem 3 shows these perspectives are equivalent—every algebra uniquely determines a partition of atoms, and every partition uniquely generates an algebra. Moreover, Theorem 4 shows that refinement of information has a natural interpretation in both frameworks: \\(\\mathcal{F}\\) refines \\(\\mathcal{G}\\) (as algebras) precisely when the atoms of \\(\\mathcal{F}\\) subdivide the atoms of \\(\\mathcal{G}\\) (as partitions). This duality between set-theoretic structure and equivalence classes will prove essential when we introduce entropy, where the partition view naturally connects to probability distributions over distinguishable outcomes."
  },
  {
    "objectID": "posts/info-2/index.html#algebras-and-entropy",
    "href": "posts/info-2/index.html#algebras-and-entropy",
    "title": "Algebras & information",
    "section": "Algebras and entropy",
    "text": "Algebras and entropy\nHaving established the correspondence between algebras and partitions, we now turn to quantifying the information content they represent. While the nested structure\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3\n\\]\nthat we explored in our coin-flip example shows that information is refined—that uncertainty decreases—we have not yet measured how much uncertainty remains at each stage, or how much uncertainty is removed by each refinement. To do this, we introduce the concept of entropy associated with an algebra of sets, which is a natural extension of the Shannon entropy of a random variable that we studied in a previous post.\nWe continue to work with a finite sample space \\(\\Omega\\) and an algebra \\(\\mathcal{F}\\) of sets in \\(\\Omega\\). To quantify uncertainty, we need a probability measure \\(P\\) on \\(\\Omega\\), which assigns a probability \\(P(\\{\\omega\\})\\) to each outcome \\(\\omega\\in \\Omega\\). The probability of an event \\(A\\in \\mathcal{F}\\) is then given by\n\\[\nP(A) = \\sum_{\\omega \\in A} P(\\{\\omega\\}).\n\\]\nTogether, the triple \\((\\Omega,\\mathcal{F},P)\\) is called a finite probability space. An algebra of sets \\(\\mathcal{G}\\) in \\(\\Omega\\) is called a sub-\\(\\sigma\\)-algebra of \\(\\mathcal{F}\\) if \\(\\mathcal{G}\\subset \\mathcal{F}\\).\nIn our coin-flip example, we can define a probability measure \\(P\\) on \\(\\Omega\\) by assuming that the coin has probability \\(\\theta\\) of showing heads on any given flip, independently of previous flips. Then the probability of an outcome \\(\\omega\\) with \\(Y(\\omega)\\) heads is\n\\[\nP(\\{\\omega\\}) = \\theta^{Y(\\omega)} (1-\\theta)^{3-Y(\\omega)}.\n\\]\nIf we suppose that \\(\\theta = 0.6\\), then the probabilties on \\(\\Omega\\) are as follows:\n\n\nCode\ntheta &lt;- 0.6 # Probability of heads (coin bias parameter)\n\n# Calculate the probabilities of each outcome in Omega\n# Under independence, P({ω}) = θ^(# heads) × (1-θ)^(# tails)\nOmega &lt;- Omega %&gt;%\n  mutate(\n    p = (theta^Y) * ((1 - theta)^(3 - Y)) # Probability based on binomial model\n  )\n\n# Display the sample space with probabilities as a formatted table\n# Each row shows an outcome ω and its probability P({ω})\nkable(\n  Omega,\n  caption = \"sample space $\\\\Omega$ with probabilities $P(\\\\{\\\\omega\\\\})$ for each outcome $\\\\omega$\"\n)\n\n\n\nsample space \\(\\Omega\\) with probabilities \\(P(\\{\\omega\\})\\) for each outcome \\(\\omega\\)\n\n\nX1\nX2\nX3\nY\nAi\nBi\nCi\np\n\n\n\n\n0\n0\n0\n0\n1\n1\n1\n0.064\n\n\n0\n0\n1\n1\n1\n1\n2\n0.096\n\n\n0\n1\n0\n1\n1\n2\n3\n0.096\n\n\n0\n1\n1\n2\n1\n2\n4\n0.144\n\n\n1\n0\n0\n1\n2\n3\n5\n0.096\n\n\n1\n0\n1\n2\n2\n3\n6\n0.144\n\n\n1\n1\n0\n2\n2\n4\n7\n0.144\n\n\n1\n1\n1\n3\n2\n4\n8\n0.216\n\n\n\n\n\n\nWith finite probability spaces defined, we can now introduce the entropy of an algebra of sets, which measures the uncertainty associated with the information it encodes. The definition is a straightforward extension of Shannon entropy, applied to the partition of \\(\\Omega\\) formed by the atoms of the algebra.\n\n\n\n\n\n\n\nDefinition 3 Let \\((\\Omega,\\mathcal{F},P)\\) be a finite probability space, let \\(\\mathcal{G}\\) be a sub-\\(\\sigma\\)-algebra of \\(\\mathcal{F}\\), and let \\(\\{B_1,B_2,\\ldots,B_n\\}\\) be the partition of \\(\\Omega\\) by atoms of \\(\\mathcal{G}\\). We then define the entropy of \\(\\mathcal{G}\\) to be the quantity\n\\[\nH(\\mathcal{G}) = -\\sum_{i=1}^n P(B_i) \\log{P(B_i)}.\n\\tag{3}\\]\n\n\n\n\nFor those readers who are familiar with abstract integration theory, we note that the formula (3) can be expressed as a Lebesgue integral. Indeed, if we write \\(\\pi: \\Omega \\to \\Omega/\\mathcal{G}\\) for the natural projection map onto the quotient space \\(\\Omega/\\mathcal{G}\\) whose points are the atoms of \\(\\mathcal{G}\\), then the entropy of \\(\\mathcal{G}\\) is given by the Lebesgue integral\n\\[\nH(\\mathcal{G}) = \\int_{\\Omega/\\mathcal{G}} s(B) \\ \\pi_\\ast P(dB),\n\\]\nwhere \\(s:\\Omega/\\mathcal{G}\\to \\mathbb{R}\\) is the surprisal function given by\n\\[\ns(B) = -\\log{P(B)},\n\\]\nand \\(\\pi_\\ast P\\) is the pushforward measure of \\(P\\) under \\(\\pi\\) defined on the power set of the quotient \\(\\Omega/\\mathcal{G}\\). By the substitution rule, we also have\n\\[\nH(\\mathcal{G}) = \\int_\\Omega (s\\circ \\pi)(\\omega) \\ P(d\\omega),\n\\]\nso the entropy is nothing but the expected value \\(E(s\\circ \\pi)\\) of the random variable \\(s\\circ \\pi\\) on \\(\\Omega\\). This abstraction becomes essential for infinite sample spaces, but we won’t need it here.\nReturning to our coin-flip example, we can now compute the entropies of the three algebras \\(\\mathcal{F}_1\\), \\(\\mathcal{F}_2\\), and \\(\\mathcal{F}_3\\) that we constructed. First, we summarize the probabilities of the atoms in each algebra:\n\n\nCode\n# Calculate probabilities on the atoms of F1\n# For algebra F1 with partition {A1, A2}, sum the probabilities of outcomes in each atom\nF1_probs &lt;- Omega %&gt;%\n  group_by(Ai) %&gt;% # Group by atom membership (Ai = 1 or 2)\n  summarise(p = sum(p)) # Sum probabilities of all outcomes in each atom\n\n# Display the probabilities for atoms of F1 as a formatted table\nkable(\n  F1_probs,\n  caption = \"probabilities $P(A_i)$ for the atoms $A_i$ of algebra $\\\\mathcal{F}_1$\"\n)\n\n\n\nprobabilities \\(P(A_i)\\) for the atoms \\(A_i\\) of algebra \\(\\mathcal{F}_1\\)\n\n\nAi\np\n\n\n\n\n1\n0.4\n\n\n2\n0.6\n\n\n\n\n\nCode\n# Calculate probabilities on the atoms of F2\n# For algebra F2 with partition {B1, B2, B3, B4}, sum probabilities for each atom\nF2_probs &lt;- Omega %&gt;%\n  group_by(Bi) %&gt;% # Group by atom membership (Bi = 1, 2, 3, or 4)\n  summarise(p = sum(p)) # Sum probabilities of all outcomes in each atom\n\n# Display the probabilities for atoms of F2 as a formatted table\nkable(\n  F2_probs,\n  caption = \"probabilities $P(B_i)$ for the atoms $B_i$ of algebra $\\\\mathcal{F}_2$\"\n)\n\n\n\nprobabilities \\(P(B_i)\\) for the atoms \\(B_i\\) of algebra \\(\\mathcal{F}_2\\)\n\n\nBi\np\n\n\n\n\n1\n0.16\n\n\n2\n0.24\n\n\n3\n0.24\n\n\n4\n0.36\n\n\n\n\n\nCode\n# Calculate probabilities on the atoms of F3\n# For algebra F3 with partition {C1, ..., C8} (singletons), extract individual outcome probabilities\nF3_probs &lt;- Omega %&gt;%\n  group_by(Ci) %&gt;% # Group by atom membership (Ci = 1 through 8)\n  summarise(p = sum(p)) # Sum probabilities (each atom contains exactly one outcome)\n\n# Display the probabilities for atoms of F3 as a formatted table\n# Since F3 is the power set, each atom Ci corresponds to a single outcome ω\nkable(\n  F3_probs,\n  caption = \"probabilities $P(C_i)$ for the atoms $C_i$ of algebra $\\\\mathcal{F}_3$\"\n)\n\n\n\nprobabilities \\(P(C_i)\\) for the atoms \\(C_i\\) of algebra \\(\\mathcal{F}_3\\)\n\n\nCi\np\n\n\n\n\n1\n0.064\n\n\n2\n0.096\n\n\n3\n0.096\n\n\n4\n0.144\n\n\n5\n0.096\n\n\n6\n0.144\n\n\n7\n0.144\n\n\n8\n0.216\n\n\n\n\n\n\nSince the algebra \\(\\mathcal{F}_3\\) is the power set on \\(\\Omega\\), notice that the probabilties of the atoms in \\(\\mathcal{F}_3\\) are just the probabilities of the individual outcomes in \\(\\Omega\\).\nWe now compute the entropies of the three algebras:\n\n\nCode\n# Calculate entropy using H = -sum(p * log(p))\n# This formula computes Shannon entropy, measuring uncertainty in the distribution\n\n# Calculate marginal entropy H(F1)\n# For algebra F1 with atoms {A1, A2}, compute entropy over the 2-element partition\nH_F1 &lt;- F1_probs %&gt;%\n  mutate(surprisal = -log(p)) %&gt;% # Surprisal: -log(p) for each atom's probability\n  summarise(H = sum(p * surprisal)) %&gt;% # Entropy: expected surprisal\n  pull(H) # Extract the scalar value\n\n# Calculate marginal entropy H(F2)\n# For algebra F2 with atoms {B1, B2, B3, B4}, compute entropy over the 4-element partition\nH_F2 &lt;- F2_probs %&gt;%\n  mutate(surprisal = -log(p)) %&gt;%\n  summarise(H = sum(p * surprisal)) %&gt;%\n  pull(H)\n\n# Calculate marginal entropy H(F3)\n# For algebra F3 with atoms {C1, ..., C8}, compute entropy over the 8-element partition\n# F3 corresponds to complete information (the power set of Omega)\nH_F3 &lt;- F3_probs %&gt;%\n  mutate(surprisal = -log(p)) %&gt;%\n  summarise(H = sum(p * surprisal)) %&gt;%\n  pull(H)\n\n# Create a tibble containing the three entropy values\n# Shows how entropy increases as algebras refine: H(F1) &lt; H(F2) &lt; H(F3)\nmarginal_entropies &lt;- tibble(\n  entropy = c(\"H(F1)\", \"H(F2)\", \"H(F3)\"), # Labels for each algebra\n  value = c(H_F1, H_F2, H_F3) # Corresponding entropy values in nats\n)\n\n# Display the marginal entropies as a formatted table\nkable(\n  marginal_entropies,\n  caption = \"marginal entropies for algebras $\\\\mathcal{F}_1$, $\\\\mathcal{F}_2$, and $\\\\mathcal{F}_3$\"\n)\n\n\n\nmarginal entropies for algebras \\(\\mathcal{F}_1\\), \\(\\mathcal{F}_2\\), and \\(\\mathcal{F}_3\\)\n\n\nentropy\nvalue\n\n\n\n\nH(F1)\n0.6730117\n\n\nH(F2)\n1.3460233\n\n\nH(F3)\n2.0190350\n\n\n\n\n\n\nThus, we see that the entropy increases as we refine our information. This means that uncertainty increases as our information becomes more detailed, which may seem counterintuitive at first glance. However, this is a natural consequence of the way entropy is defined: it measures the expected uncertainty over all possible outcomes, and as we refine our information, we are effectively considering a larger number of possible outcomes (the atoms of the finer algebra), each with its own associated probability. The increased granularity leads to a higher overall uncertainty when averaged across all outcomes.\nWe may visualize this increase in entropy as follows, where reading the plot from left to right corresponds to refining our information from \\(\\mathcal{F}_1\\) to \\(\\mathcal{F}_2\\) to \\(\\mathcal{F}_3\\):\n\n\nCode\n# Create a bar chart visualizing how entropy increases across the filtration\n# Each bar represents the entropy H(Fi) for i = 1, 2, 3\nggplot(marginal_entropies, aes(x = entropy, y = value)) +\n  geom_col(fill = yellow, width = 0.4) + # Yellow bars with reduced width for clarity\n  labs(\n    title = \"entropy increases under refinement\", # Title describes the key pattern\n    x = NULL, # Remove x-axis label (entropy names are self-explanatory)\n    y = \"entropy\" # Label y-axis with the quantity being measured\n  )\n\n\n\n\n\n\n\n\n\nHaving defined the entropy of an algebra, we can now introduce two related concepts: conditional entropy and mutual information. These quantities measure, respectively, the uncertainty remaining in one algebra given knowledge of another, and the amount of information shared between two algebras.\nTo define the conditional entropy, we need two sub-\\(\\sigma\\)-algebras \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\) of a finite probability space \\((\\Omega,\\mathcal{F},P)\\). If the decompositions of \\(\\Omega\\) into the atoms of \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\) are\n\\[\n\\{B_1,B_2,\\ldots,B_n\\} \\quad \\text{and} \\quad \\{C_1,C_2,\\ldots,C_m\\},\n\\]\nrespectively, then the decomposition of \\(\\Omega\\) into the atoms of the algebra \\(\\mathcal{G}\\vee \\mathcal{H}\\), the smallest algebra containing both \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\), is given by the nonempty sets among \\(B_i \\cap C_j\\) for each of \\(i=1,2,\\ldots,n\\) and \\(j=1,2,\\ldots,m\\).\n\n\n\n\n\n\n\nDefinition 4 Let \\((\\Omega,\\mathcal{F},P)\\) be a finite probability space, and let \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\) be two sub-\\(\\sigma\\)-algebras of \\(\\mathcal{F}\\). The conditional entropy of \\(\\mathcal{G}\\) given \\(\\mathcal{H}\\) is the quantity\n\\[\nH(\\mathcal{G}\\mid \\mathcal{H}) = H(\\mathcal{G}\\vee \\mathcal{H}) - H(\\mathcal{H}).\n\\]\n\n\n\n\nThe intuition for this definition is as follows. The entropy \\(H(\\mathcal{H})\\) measures the uncertainty in \\(\\mathcal{H}\\), while \\(H(\\mathcal{G}\\vee \\mathcal{H})\\) measures the uncertainty in the combined information of \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\). The difference between these two quantities, \\(H(\\mathcal{G}\\mid \\mathcal{H})\\), thus represents the uncertainty that remains in \\(\\mathcal{G}\\) once we know everything in \\(\\mathcal{H}\\).\nIn our coin-flip example, three conditional entropies are of interest: \\(H(\\mathcal{F}_3\\mid \\mathcal{F}_i)\\), for each of \\(i=1,2,3\\). Since we have the nested sequence\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3,\n\\]\nnote that\n\\[\nH(\\mathcal{F}_3 \\mid \\mathcal{F}_1) = H(\\mathcal{F}_1 \\vee \\mathcal{F}_3) - H(\\mathcal{F}_1) = H(\\mathcal{F}_3) - H(\\mathcal{F}_1),\n\\tag{4}\\]\nand similarly for \\(H(\\mathcal{F}_3 \\mid \\mathcal{F}_2)\\) and \\(H(\\mathcal{F}_3 \\mid \\mathcal{F}_3)\\). In particular, we have that \\(H(\\mathcal{F}_3 \\mid \\mathcal{F}_3) =0\\), which squares with our intuition that, given the information in \\(\\mathcal{F}_3\\), the uncertainty remaining in \\(\\mathcal{F}_3\\) is zero.\nWe compute all three conditional entropies:\n\n\nCode\n# Create a tibble containing conditional entropy values for the filtration\n# For nested algebras F1 ⊂ F2 ⊂ F3, conditional entropy H(F3 | Fi) = H(F3) - H(Fi)\n# This measures the residual uncertainty in F3 given knowledge of Fi\nconditional_entropies &lt;- tibble(\n  entropy = c(\"H(F3 | F1)\", \"H(F3 | F2)\", \"H(F3 | F3)\"), # Labels for each conditional entropy\n  value = c(H_F3 - H_F1, H_F3 - H_F2, H_F3 - H_F3) # Values: H(F3 | Fi) for each i\n)\n\n# Display the conditional entropies as a formatted table\n# Shows how residual uncertainty in F3 decreases as we refine from F1 to F2 to F3\nkable(\n  conditional_entropies,\n  caption = \"conditional entropies for the filtration $\\\\mathcal{F}_1 \\\\subset \\\\mathcal{F}_2 \\\\subset \\\\mathcal{F}_3$\"\n)\n\n\n\nconditional entropies for the filtration \\(\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3\\)\n\n\nentropy\nvalue\n\n\n\n\nH(F3 | F1)\n1.3460233\n\n\nH(F3 | F2)\n0.6730117\n\n\nH(F3 | F3)\n0.0000000\n\n\n\n\n\n\nThese values show that as we compute \\(H(\\mathcal{F}_3 \\mid \\mathcal{F}_i)\\) along the sequence\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3,\n\\]\nthe conditional entropies decrease. This reflects the fact that as we refine our information (moving from \\(\\mathcal{F}_1\\) to \\(\\mathcal{F}_2\\) to \\(\\mathcal{F}_3\\)), the uncertainty remaining in \\(\\mathcal{F}_3\\) given knowledge of the other algebras diminishes. This aligns perfectly with our intuition: more information leads to less uncertainty.\nFinally, we define the mutual information between two sub-\\(\\sigma\\)-algebras \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\) of a finite probability space \\((\\Omega,\\mathcal{F},P)\\). This quantity measures the amount of information shared between \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\), quantifying how much knowing one reduces uncertainty about the other.\n\n\n\n\n\n\n\nDefinition 5 Let \\((\\Omega,\\mathcal{F},P)\\) be a finite probability space, and let \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\) be two sub-\\(\\sigma\\)-algebras of \\(\\mathcal{F}\\). The mutual information between \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\) is the quantity\n\\[\nI(\\mathcal{G},\\mathcal{H}) = H(\\mathcal{G}) -H(\\mathcal{G}\\mid \\mathcal{H}).\n\\tag{5}\\]\n\n\n\n\nThe intuition for this definition is as follows. The entropy \\(H(\\mathcal{G})\\) measures the uncertainty in \\(\\mathcal{G}\\). The conditional entropy \\(H(\\mathcal{G}\\mid \\mathcal{H})\\) measures the uncertainty remaining in \\(\\mathcal{G}\\) once we know everything in \\(\\mathcal{H}\\). The difference between these two quantities, \\(I(\\mathcal{G},\\mathcal{H})\\), thus represents the reduction in uncertainty about \\(\\mathcal{G}\\) due to knowledge of \\(\\mathcal{H}\\)—in other words, the information that \\(\\mathcal{G}\\) and \\(\\mathcal{H}\\) share.\nReturning once more to our coin-flip example, we compute the three mutual informations \\(I(\\mathcal{F}_3,\\mathcal{F}_i)\\) for \\(i=1,2,3\\). From (4), we have\n\\[\nI(\\mathcal{F}_3, \\mathcal{F}_1) = H(\\mathcal{F}_3) - H(\\mathcal{F}_3 \\mid \\mathcal{F}_1) = H(\\mathcal{F}_3) - (H(\\mathcal{F}_3) - H(\\mathcal{F}_1)) = H(\\mathcal{F}_1),\n\\]\nand similarly for \\(I(\\mathcal{F}_3,\\mathcal{F}_2)\\) and \\(I(\\mathcal{F}_3,\\mathcal{F}_3)\\). We compute all three mutual informations:\n\n\nCode\n# Create a tibble containing mutual information values for each algebra in the filtration\n# For nested algebras F1 ⊂ F2 ⊂ F3, the mutual information I(F3, Fi) equals H(Fi)\n# because Fi contains all the information about F3 that Fi itself possesses\nmutual_infos &lt;- tibble(\n  entropy = c(\"I(F3, F1)\", \"I(F3, F2)\", \"I(F3, F3)\"),  # Labels for each mutual information\n  value = c(H_F1, H_F2, H_F3)  # Values: I(F3, Fi) = H(Fi) for nested algebras\n)\n\n# Display the mutual informations as a formatted table\n# Shows how shared information between F3 and each Fi increases with refinement\nkable(\n  mutual_infos,\n  caption = \"mutual informations for the filtration $\\\\mathcal{F}_1 \\\\subset \\\\mathcal{F}_2 \\\\subset \\\\mathcal{F}_3$\"\n)\n\n\n\nmutual informations for the filtration \\(\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3\\)\n\n\nentropy\nvalue\n\n\n\n\nI(F3, F1)\n0.6730117\n\n\nI(F3, F2)\n1.3460233\n\n\nI(F3, F3)\n2.0190350\n\n\n\n\n\n\nThus, as we compute \\(I(\\mathcal{F}_3,\\mathcal{F}_i)\\) along the sequence\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3,\n\\]\nwe see that the mutual informations increase. This reflects the fact that as we refine our information (moving from \\(\\mathcal{F}_1\\) to \\(\\mathcal{F}_2\\) to \\(\\mathcal{F}_3\\)), the amount of information shared between \\(\\mathcal{F}_3\\) and the other algebras grows. This again aligns perfectly with our intuition: more information leads to greater shared knowledge.\nFrom (5), we have\n\\[\nH(\\mathcal{F}_3) = I(\\mathcal{F}_3,\\mathcal{F}_i) + H(\\mathcal{F}_3 \\mid \\mathcal{F}_i),\n\\]\nfor each \\(i=1,2,3\\), and so the total entropy in \\(\\mathcal{F}_3\\) decomposes into the sum of mutual information and conditional entropy. We visualize this decomposition for each of the three algebras in our filtration:\n\n\nCode\n# Create a data frame decomposing the entropy H(F3) into mutual information and conditional entropy\n# For each algebra F_i (i=1,2,3), we have: H(F3) = I(F3, F_i) + H(F3 | F_i)\ndecomposition &lt;- tibble(\n  algebra = rep(c(\"i=1\", \"i=2\", \"i=3\"), each = 2), # Three algebras, each with two components\n  component = rep(c(\"mutual information\", \"conditional entropy\"), 3), # Two components per algebra\n  entropy = c(H_F1, H_F3 - H_F1, H_F2, H_F3 - H_F2, H_F3, 0) # Values for each component\n)\n\n# Create a stacked bar chart showing the decomposition\n# Each bar represents H(F3) for a different conditioning algebra F_i\nggplot(decomposition, aes(x = algebra, y = entropy, fill = component)) +\n  geom_col(width = 0.4) + # Stacked column chart\n  scale_fill_manual(\n    name = NULL,\n    values = c(\"mutual information\" = blue, \"conditional entropy\" = yellow)\n  ) +\n  labs(\n    title = TeX(\"entropy decomposition $H(F_3) = I(F_3, F_i) + H(F_3 | F_i)$\"),\n    x = NULL\n  )\n\n\n\n\n\n\n\n\n\nThe decomposition \\(H(\\mathcal{F}_3) = I(\\mathcal{F}_3, \\mathcal{F}_i) + H(\\mathcal{F}_3 \\mid \\mathcal{F}_i)\\) reveals a fundamental trade-off: as we refine our information from \\(\\mathcal{F}_1\\) to \\(\\mathcal{F}_2\\) to \\(\\mathcal{F}_3\\), mutual information increases (capturing more shared knowledge with the complete outcome) while conditional entropy decreases (leaving less residual uncertainty). The total entropy \\(H(\\mathcal{F}_3)\\) remains constant, but its composition shifts as information accumulates."
  },
  {
    "objectID": "posts/info-2/index.html#conclusion",
    "href": "posts/info-2/index.html#conclusion",
    "title": "Algebras & information",
    "section": "Conclusion",
    "text": "Conclusion\nWe began this post by asking how to represent an observer’s information mathematically. The answer emerged through two complementary perspectives: algebras of sets, which capture the logical operations available to the observer, and partitions, which describe which outcomes can be distinguished from one another. Theorem 3 showed these views are equivalent—every algebra corresponds to a unique partition of atoms, and vice versa.\nTo quantify information, we introduced entropy as a measure of uncertainty associated with an algebra. The entropy \\(H(\\mathcal{G})\\) tells us how much uncertainty remains when the observer’s knowledge is captured by \\(\\mathcal{G}\\). Our coin-flip example illustrated how entropy increases as partitions refine: knowing the first flip yields \\(H(\\mathcal{F}_1) \\approx 0.67\\), knowing the first two flips yields \\(H(\\mathcal{F}_2) \\approx 1.35\\), and complete knowledge yields \\(H(\\mathcal{F}_3) \\approx 2.02\\).\nWe also introduced conditional entropy \\(H(\\mathcal{G} \\mid \\mathcal{H})\\), measuring residual uncertainty in \\(\\mathcal{G}\\) given knowledge of \\(\\mathcal{H}\\), and mutual information \\(I(\\mathcal{G}, \\mathcal{H})\\), measuring information shared between algebras. These quantities satisfy the fundamental relationship\n\\[\nH(\\mathcal{G}) = I(\\mathcal{G}, \\mathcal{H}) + H(\\mathcal{G} \\mid \\mathcal{H}),\n\\]\ndecomposing total uncertainty into shared information and residual uncertainty.\nThe framework of nested algebras representing evolving information has powerful applications beyond theoretical probability. In future posts, we’ll explore how these ideas illuminate gambling strategies and games of chance, where players accumulate information over time and make decisions under uncertainty. We’ll also see how the same mathematical structure underpins options pricing in mathematical finance, where nested filtrations model the information available to traders at different times, and conditional expectations with respect to these algebras determine fair prices for derivative securities. The interplay between information, uncertainty, and value in these contexts provides a rich testing ground for the concepts developed here."
  },
  {
    "objectID": "posts/gmm-1/index.html",
    "href": "posts/gmm-1/index.html",
    "title": "Gaussian mixture models I: a first look",
    "section": "",
    "text": "A mixture of gaussians represents one of the most elegant and practically useful concepts in probability theory and machine learning. At its core, this is a probabilistic model that assumes observed data arises from a combination of several underlying normal distributions, each contributing to the overall distribution with different weights and parameters. Rather than forcing all data points to conform to a single bell curve, a gaussian mixture allows for multiple clusters, each governed by its own mean, variance, and relative importance in the mixture. This creates a framework for modeling complex, multi-modal distributions that frequently appear in real-world scenarios. Consider, for example, the heights of adults in a population that includes both men and women, or the distribution of pixel intensities in an image containing multiple distinct objects. A single gaussian would poorly capture the true underlying structure, while a mixture model can naturally accommodate the multiple peaks and varying spreads that characterize such data. The beauty of this approach lies not only in its flexibility but also in its mathematical tractability, making it both theoretically sound and computationally feasible for a wide range of applications.\nFor example, suppose we were presented with the following data distribution, displayed here as a histogram:\n\n\n\n\n\n\n\n\n\nThree clusters are immediately evident: the first with a peak around \\(x=10\\), the second around \\(x=15\\), and the third near \\(x=21\\). The presence of three clusters with well-defined peaks suggests that the data might be modeled well by a mixture of three gaussians.\nIndeed, this synthetic dataset contains 1,000 observations sampled from a three-component mixture of gaussians, also called a gaussian mixture model or GMM. In this introductory post, I’ll explain the mathematical and graphical structure of the GMM that generated this data. Since I fixed the GMM parameters before sampling, we know the true underlying model. In a follow-up post, I’ll tackle the inverse problem: given only the dataset, how can we estimate the parameters and recover the original GMM?"
  },
  {
    "objectID": "posts/gmm-1/index.html#introduction",
    "href": "posts/gmm-1/index.html#introduction",
    "title": "Gaussian mixture models I: a first look",
    "section": "",
    "text": "A mixture of gaussians represents one of the most elegant and practically useful concepts in probability theory and machine learning. At its core, this is a probabilistic model that assumes observed data arises from a combination of several underlying normal distributions, each contributing to the overall distribution with different weights and parameters. Rather than forcing all data points to conform to a single bell curve, a gaussian mixture allows for multiple clusters, each governed by its own mean, variance, and relative importance in the mixture. This creates a framework for modeling complex, multi-modal distributions that frequently appear in real-world scenarios. Consider, for example, the heights of adults in a population that includes both men and women, or the distribution of pixel intensities in an image containing multiple distinct objects. A single gaussian would poorly capture the true underlying structure, while a mixture model can naturally accommodate the multiple peaks and varying spreads that characterize such data. The beauty of this approach lies not only in its flexibility but also in its mathematical tractability, making it both theoretically sound and computationally feasible for a wide range of applications.\nFor example, suppose we were presented with the following data distribution, displayed here as a histogram:\n\n\n\n\n\n\n\n\n\nThree clusters are immediately evident: the first with a peak around \\(x=10\\), the second around \\(x=15\\), and the third near \\(x=21\\). The presence of three clusters with well-defined peaks suggests that the data might be modeled well by a mixture of three gaussians.\nIndeed, this synthetic dataset contains 1,000 observations sampled from a three-component mixture of gaussians, also called a gaussian mixture model or GMM. In this introductory post, I’ll explain the mathematical and graphical structure of the GMM that generated this data. Since I fixed the GMM parameters before sampling, we know the true underlying model. In a follow-up post, I’ll tackle the inverse problem: given only the dataset, how can we estimate the parameters and recover the original GMM?"
  },
  {
    "objectID": "posts/gmm-1/index.html#conditional-pdfs-and-graphical-structures",
    "href": "posts/gmm-1/index.html#conditional-pdfs-and-graphical-structures",
    "title": "Gaussian mixture models I: a first look",
    "section": "Conditional PDFs and graphical structures",
    "text": "Conditional PDFs and graphical structures\nAs mentioned above, a GMM combines several individual gaussians, each called a component of the mixture. Generating a sample \\(x\\) from a GMM involves two steps:\n\nSelect a component from which to generate \\(x\\).\nSample \\(x\\) from the chosen component gaussian.\n\nThe key insight is that components need not be selected with equal probability. Each component has an associated weight that determines its selection probability in step 1.\nTo formalize the selection process, we introduce the categorical distribution. Suppose given non-negative real numbers \\(\\phi_1,\\phi_2,\\ldots,\\phi_m\\) that sum to \\(1\\):\n\\[\n\\phi_1 + \\phi_2 + \\cdots + \\phi_m =1.\n\\]\nWe say that a discrete random variable \\(Y\\) has a categorical distribution, written\n\\[\nY \\sim \\mathcal{C}at(\\phi_1,\\phi_2,\\ldots,\\phi_m),\n\\tag{1}\\]\nif its probability mass function has the form\n\\[\np(y) = \\begin{cases}\n\\phi_y & : y=1,2,\\ldots,m, \\\\\n0 & : \\text{otherwise}.\n\\end{cases}\n\\]\nWe can now describe GMM sampling more precisely. For a GMM with \\(m\\) components where the \\(i\\)-th component has weight \\(\\phi_i\\), and \\(Y\\) a categorical random variable as in Equation 1, the sampling scheme above becomes:\n\nSample \\(y\\) from the categorical distribution \\(Y\\).\nGiven \\(y\\), sample \\(x\\) from the \\(y\\)-th component gaussian.\n\nThis natural progression from component selection to data generation is often called forward sampling, for obvious reasons.\nWe can visualize this sampling process with a simple graphical representation:\n\n\n\n\n\nHere, \\(Y\\) represents the categorical random variable that encodes component weights, while \\(X\\) represents the random variable from which we sample our observation \\(x\\). The arrow captures the “forward” direction of the sampling process: first select a component, then generate data from it. This diagram illustrates the GMM as a simple example of a probabilistic graphical model (or PGM)—a topic I’ll explore in depth in future posts.\nThe mathematical relationship between these variables is straightforward. Given that component \\(y\\) has been selected, the conditional distribution of \\(X\\) is gaussian:\n\\[\n(X \\mid Y=y) \\sim \\mathcal{N}(\\mu_y,\\sigma_y^2).\n\\]\nThis conditional distribution corresponds exactly to the \\(y\\)-th component of our GMM. Notice that both the mean \\(\\mu_y\\)​ and standard deviation \\(\\sigma_y\\) are component-specific, allowing each gaussian in the mixture to have its own location and spread.\nA GMM thus has two different types of parameters:\n\nThe component weights \\(\\phi_1,\\phi_2,\\ldots,\\phi_m\\).\nFor each \\(y=1,2,\\ldots,m\\), the guassian parameters \\(\\mu_y\\) and \\(\\sigma_y\\).\n\nAs I mentioned in the introduction, in this post we assume that these parameters are fixed and known. Later, we will learn how to estimate them from data."
  },
  {
    "objectID": "posts/gmm-1/index.html#marginal-pdfs",
    "href": "posts/gmm-1/index.html#marginal-pdfs",
    "title": "Gaussian mixture models I: a first look",
    "section": "Marginal PDFs",
    "text": "Marginal PDFs\nThe marginal distribution of \\(X\\) follows directly from the law of total probability: \\[\nf(x) = \\sum_{y=1}^m p(y)f(x|y)  = \\sum_{y=1}^m \\phi_y f(x|y),\n\\tag{2}\\]\nwhere the conditional densities are normal:\n\\[\nf(x|y) = \\frac{1}{\\sigma_y\\sqrt{2\\pi}} \\exp \\left[ - \\frac{1}{2} \\left( \\frac{x-\\mu_y}{\\sigma_y}\\right)^2\\right].\n\\]\nThus, the marginal density \\(f(x)\\) is a convex linear combination of normal densities.\nNow let’s examine the specific \\(3\\)-component GMM that generated our dataset. The component parameters are:\n\\[\n(\\mu_1,\\sigma_1) = (10, 1), \\quad (\\mu_2,\\sigma_2) = (15, 2), \\quad (\\mu_3,\\sigma_3) = (21, 0.5),\n\\]\nwith weights:\n\\[\n(\\phi_1,\\phi_2,\\phi_3) = (0.2, 0.7, 0.1).\n\\]\nNotice that the middle component (centered at 15) dominates with 70% weight, while the outer components contribute 20% and 10% respectively.\nLet’s implement and visualize the component gaussians and the marginal PDF in Python. First, let’s load the parameters:\n\nnorm_params = [\n  {\"loc\": 10, \"scale\": 1},\n  {\"loc\": 15, \"scale\": 2},\n  {\"loc\": 21, \"scale\": 0.5}\n]\nweights = [0.2, 0.7, 0.1]\n\nNow, we plot the individual components, each scaled by its weight:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nmesh = np.linspace(7, 23, num=200)\n\nfor y, (param, weight) in enumerate(zip(norm_params, weights)):\n  X = norm(**param)\n  plt.plot(mesh, weight * X.pdf(mesh), color=colors[y], label=f\"component {y+1}\")\n\nplt.xlabel(\"x\")\nplt.ylabel(\"density\")\nplt.title(\"gaussian components\")\nplt.legend()\nfig = plt.gcf()  # Get current figure\nfig.set_size_inches(6, 4)\nplt.tight_layout()\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\nThe marginal PDF implementation follows Equation 2 directly:\n\ndef marginal_pdf(x):  \n  return sum([weight * norm(**param).pdf(x) for weight, param in zip(weights, norm_params)])\n\nFinally, we plot the theoretical marginal PDF against the empirical histogram:\n\nimport seaborn as sns\n\ndata = np.load(\"../../data/gmm-data.npy\")\n\nsns.histplot(data=data, color=yellow, alpha=0.25, ec=grey, zorder=2, stat=\"density\", label=\"data\")\nplt.plot(mesh, marginal_pdf(mesh), color=blue, label=\"marginal PDF\")\nplt.xlabel(\"x\")\nplt.ylabel(\"density\")\nplt.legend()\nfig = plt.gcf()  # Get current figure\nfig.set_size_inches(6, 4)\nplt.tight_layout()\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\nThe close agreement between our theoretical marginal PDF and the empirical histogram confirms that our synthetic dataset faithfully represents the underlying GMM. Notice how the theoretical curve captures all three modes: the subtle left peak around \\(x=10\\), the dominant central peak near \\(x=15\\), and the sharp right peak at \\(x=21\\). The varying heights and widths of these peaks directly reflect the component weights and standard deviations we specified."
  },
  {
    "objectID": "posts/gmm-1/index.html#sampling",
    "href": "posts/gmm-1/index.html#sampling",
    "title": "Gaussian mixture models I: a first look",
    "section": "Sampling",
    "text": "Sampling\nHaving established the theoretical foundation and visualized the marginal PDF, let’s now examine the process behind generating our synthetic dataset.\nBut this is easy. We first sample 1,000 values of \\(y\\) from the categorical random variable\n\\[\nY \\sim \\mathcal{C}at(0.2, 0.7, 0.1),\n\\]\nand then for each \\(y\\) we sample from the gaussian \\(X|Y=y\\). We check that our new generated sample coincides with the original dataset:\n\nnp.random.seed(42) # Needed to replicate the original dataset\n\nn_samples = 1000\ny_samples = np.random.choice([0, 1, 2], size=n_samples, p=weights)\nx_samples = np.array([norm(**norm_params[y]).rvs() for y in y_samples])\n\nprint(\"Are the datasets equal? \" + (\"Yes.\" if np.array_equal(data, x_samples) else \"No.\"))\n\nAre the datasets equal? No."
  },
  {
    "objectID": "posts/gmm-1/index.html#conclusion",
    "href": "posts/gmm-1/index.html#conclusion",
    "title": "Gaussian mixture models I: a first look",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we’ve built the GMM from the ground up, starting with its definition, exploring its conditional and marginal structure, and even sampling from it directly. But in practice, the real challenge is the inverse problem we mentioned earlier: we are usually given a dataset and want to fit a GMM by estimating its parameters. How do we tease apart the hidden components, estimate their weights, and recover the underlying mixture? That puzzle leads directly to one of the most beautiful iterative methods in statistics and machine learning: the expectation–maximization algorithm, which will be the focus of the next post in this series."
  },
  {
    "objectID": "posts/sigalg/index.html",
    "href": "posts/sigalg/index.html",
    "title": "Introducing SigAlg: Measure-Theoretic Probability in Python",
    "section": "",
    "text": "For the past four-ish months, I’ve been working on a Python library called SigAlg (named after a \\(\\sigma\\)-algebra, duh), designed to provide mathematically faithful implementations of measure-theoretic probability. I’ve recently made early versions available on PyPI, made the GitHub repo public, and put up a documentation site that is around 90% done. I’m still very much in the early stages of development, but if I keep obsessing over perfection, it’ll never be released. So it’s time to kick this baby bird out of the nest and hope it doesn’t fall to its death.\nWhere did this thing come from?\nI have recently taken a liking to mathematical finance. (To the extent that I now listen to finance-oriented podcasts and pay some attention to the markets. I know—as a recovering pure mathematician, I feel gross just saying that. No offense finance people.) The issue is, that I come from a very different mathematical upbringing, so I knew next to nothing about the deeper parts of stochastic process theory, stochastic calculus, and all that stuff. So, as I’m grinding my way through some textbooks playing catch-up, I found myself wanting to experiment with code, but I couldn’t find any libraries that implemented the concepts in a way that felt natural and mathematically faithful. There’s a really big jump between, say, a \\(\\sigma\\)-algebra that you manipulate in your head and push around on a sheet of paper, and the code that implements it using existing Python libraries. Sure, you can perform a group_by on a pd.DataFrame that will partition your “sample space” into the “atoms” of your \\(\\sigma\\)-algebra, but I wanted a semantically meaningful API that feels like you’re writing mathematics on paper. That leads us here. I give you SigAlg.\n\nI will introduce you to SigAlg in this post, which is mostly an extended version of the random walk example on the landing page of the documentation site. Future blog posts will cover more advanced topics."
  },
  {
    "objectID": "posts/sigalg/index.html#introduction",
    "href": "posts/sigalg/index.html#introduction",
    "title": "Introducing SigAlg: Measure-Theoretic Probability in Python",
    "section": "",
    "text": "For the past four-ish months, I’ve been working on a Python library called SigAlg (named after a \\(\\sigma\\)-algebra, duh), designed to provide mathematically faithful implementations of measure-theoretic probability. I’ve recently made early versions available on PyPI, made the GitHub repo public, and put up a documentation site that is around 90% done. I’m still very much in the early stages of development, but if I keep obsessing over perfection, it’ll never be released. So it’s time to kick this baby bird out of the nest and hope it doesn’t fall to its death.\nWhere did this thing come from?\nI have recently taken a liking to mathematical finance. (To the extent that I now listen to finance-oriented podcasts and pay some attention to the markets. I know—as a recovering pure mathematician, I feel gross just saying that. No offense finance people.) The issue is, that I come from a very different mathematical upbringing, so I knew next to nothing about the deeper parts of stochastic process theory, stochastic calculus, and all that stuff. So, as I’m grinding my way through some textbooks playing catch-up, I found myself wanting to experiment with code, but I couldn’t find any libraries that implemented the concepts in a way that felt natural and mathematically faithful. There’s a really big jump between, say, a \\(\\sigma\\)-algebra that you manipulate in your head and push around on a sheet of paper, and the code that implements it using existing Python libraries. Sure, you can perform a group_by on a pd.DataFrame that will partition your “sample space” into the “atoms” of your \\(\\sigma\\)-algebra, but I wanted a semantically meaningful API that feels like you’re writing mathematics on paper. That leads us here. I give you SigAlg.\n\nI will introduce you to SigAlg in this post, which is mostly an extended version of the random walk example on the landing page of the documentation site. Future blog posts will cover more advanced topics."
  },
  {
    "objectID": "posts/sigalg/index.html#generalities-on-stochastic-processes",
    "href": "posts/sigalg/index.html#generalities-on-stochastic-processes",
    "title": "Introducing SigAlg: Measure-Theoretic Probability in Python",
    "section": "Generalities on stochastic processes",
    "text": "Generalities on stochastic processes\nFirst, before any code, let’s make sure we agree on definitions and terminology. Allow me to put on my math professor hat.\nGiven a nonempty set \\(T\\subset \\mathbb{R}\\), our definition of a stochastic process requires us to consider the function space \\(\\mathbb{R}^T\\), which consists of all \\(\\mathbb{R}\\)-valued functions on \\(T\\).\n\n\n\n\n\n\n\nDefinition 1 Let \\(\\Omega\\) be a probability space and let \\(T\\subset \\mathbb{R}\\) be a nonempty subset. A stochastic process, with index set \\(T\\), is a function\n\\[\nX: \\Omega \\to U\n\\]\nwhere \\(U\\) is a subset of the function space \\(\\mathbb{R}^T\\). We require \\(X\\) be such that, for each \\(t \\in T\\), the function \\[\nX_t : \\Omega \\to \\mathbb{R}, \\quad \\omega \\mapsto X(\\omega)(t),\n\\]\n is a random variable on \\(\\Omega\\).\n\n\n\n\nThe requirement that “\\(X_t\\) is a random variable” means that \\(X_t\\) is a measurable function with respect to the Borel algebra on \\(\\mathbb{R}\\) and the given \\(\\sigma\\)-algebra on \\(\\Omega\\). If this makes sense, then good—if not, the reader can simply ignore this technical detail.The index set \\(T\\) is often conceptualized as time, and when \\(T\\) is countable we say that \\(X\\) is a discrete-time stochastic process, while when \\(T\\) is an interval in \\(\\mathbb{R}\\) (or \\(\\mathbb{R}\\) itself) we say that \\(X\\) is a continuous-time stochastic process. Similarly, if each random variable \\(X_t\\) is discrete, we say that \\(X\\) is a discrete-state stochastic process, while if each \\(X_t\\) is continuous, we say that \\(X\\) is a continuous-state stochastic process.\nNote that a stochastic process \\(X\\) and the collection of random variables \\(\\{X_t\\}_{t\\in T}\\) that it determines are equivalent objects. To be more precise, note that obviously \\(X\\) determines each random variable \\(X_t\\), but conversely, given a collection of random variables \\(\\{X_t\\}_{t\\in T}\\), we can construct a stochastic process \\(X\\) via the formula\n\\[\nX(\\omega)(t) = X_t(\\omega), \\quad \\forall t \\in T, \\ \\omega \\in \\Omega.\n\\]\nFixing \\(\\omega\\in \\Omega\\), the function\n\\[\nX(\\omega) : T \\to \\mathbb{R}\n\\]\nis called a trajectory (or sample path) of the stochastic process. It is these trajectories that are often the central objects of study in applications of stochastic processes.\nTechnically, in order for the law of \\(X\\) to be well-defined, \\(X\\) needs to be a measurable function with respect to a \\(\\sigma\\)-algebra on \\(U\\). This is indeed the case. See, for example, Lemma 4.1 in [1].\nIf \\(X: \\Omega \\to U\\) is a random variable as in the definition above, and if \\(P\\) is the probability measure on \\(\\Omega\\), then \\(X\\) has a well-defined law, which is a probability measure \\(P_X\\) on \\(U\\) given by\n\\[\nP_X(A) \\stackrel{\\text{def}}{=} P(X\\in A),\n\\tag{1}\\]\nfor all measurable sets \\(A \\subset U\\). It is an important fact (see Proposition 4.2 in [1]) that this law is uniquely determined by the finite-dimensional distributions of \\(X\\), which are the probability measures on \\(\\mathbb{R}^n\\) of the form\n\\[\nP_{X_{t_1}, \\ldots, X_{t_n}}(B) \\stackrel{\\text{def}}{=} P\\big((X_{t_1}, \\ldots, X_{t_n}) \\in B\\big),\n\\]\nwhere \\(n\\) ranges over all positive integers, \\(t_1, \\ldots, t_n\\) range over all indices in \\(T\\), and \\(B\\subset \\mathbb{R}^n\\) is a Borel set. Note that these are the laws of the random vectors\n\\[\n\\Omega\\to \\mathbb{R}^n, \\quad \\omega \\mapsto (X_{t_1}(\\omega), \\ldots, X_{t_n}(\\omega)).\n\\]\nSpecializing this fact, in the case of a finite index set \\(T = \\{t_1, \\ldots, t_n\\}\\), we conclude that the law (1) of \\(X\\) is completely determined by the (single!) random vector \\((X_{t_1}, \\ldots, X_{t_n})\\), since all the other finite-dimensional distributions can be obtained by marginalizing the law of this one."
  },
  {
    "objectID": "posts/sigalg/index.html#independent-and-identically-distributed-processes",
    "href": "posts/sigalg/index.html#independent-and-identically-distributed-processes",
    "title": "Introducing SigAlg: Measure-Theoretic Probability in Python",
    "section": "Independent and identically distributed processes",
    "text": "Independent and identically distributed processes\nAfter all the generalities and abstractions in the previous section, we now bring everything down to earth and discuss what might be the simplest examples of stochastic processes:\n\n\n\n\n\n\n\nDefinition 2 Let \\(X:\\Omega \\to U \\subset \\mathbb{R}^T\\) be a \\(T\\)-indexed stochastic process on a probability space \\(\\Omega\\). Then \\(X\\) is called an independent and identically distributed (IID) process if the random variables \\(\\{X_t\\}_{t\\in T}\\) are independent and identically distributed.\n\n\n\n\nThe finite-dimensional distributions of an IID process are given by the formula\n\\[\nP_{X_{t_1}, \\ldots, X_{t_n}}(B) = \\prod_{i=1}^n P(X_{t_i} \\in B_i),\n\\tag{2}\\]\nwhere we assume \\(B = B_1 \\times \\cdots \\times B_n \\subset \\mathbb{R}^n\\) is a product of Borel sets.\nA simple example of an IID process is a sequence of Bernoulli random variables indexed by a finite set \\(T = \\{1,2,\\ldots,n\\}\\), which can be used to model a sequence of \\(n\\) coin flips where \\(X_t = 1\\) represents heads and \\(X_t = 0\\) represents tails on the \\(t\\)-th flip. Such a process can be implemented using the SigAlg library and its interface with SciPy:\nIn constructing the Time instance T modeling the index set \\(T = \\{1,2,3\\}\\), note that the length parameter is 2, which is the length of time spanned by the interval from 1 to 3. In particular, the length parameter is not the length in the sense of the Python magic method len(), which would return the number of time points, 3.\n\nfrom scipy.stats import bernoulli\nfrom sigalg.core import Time\nfrom sigalg.processes import IIDProcess\n\n# Construct time index [1, 2, 3]. See the aside in the margin ---&gt;\nT = Time.discrete(start=1, length=2)\n\n# Set the parameters for the IID process\nX = IIDProcess(\n    distribution=bernoulli(p=0.7),\n    support=[0, 1],\n    time=T,\n)\n\n# Generate all possible trajectories\nX.from_enumeration()\n\nprint(X)\n\nStochastic process 'X':\ntime        1  2  3\ntrajectory         \n0           0  0  0\n1           0  0  1\n2           0  1  0\n3           0  1  1\n4           1  0  0\n5           1  0  1\n6           1  1  0\n7           1  1  1\n\n\nThe printout shows an IID process \\(X\\) on the index set \\(T = \\{1,2,3\\}\\), where each \\(X_t\\sim \\mathcal{B}er(0.7)\\) is a Bernoulli random variable with parameter \\(p=0.7\\). The three time-indexed columns correspond to the values of the three random variables \\(X_1, X_2, X_3\\). SigAlg models the domain \\(\\Omega\\) as the trajectory indices, so \\(\\Omega = \\{0,1,\\ldots,7\\}\\). This may be verified by checking the domain attribute of our process:\n\nprint(X.domain)\n\nSample space 'Omega':\n[0, 1, 2, 3, 4, 5, 6, 7]\n\n\nThus, the rows of the printout may be conceptualized as the trajectories\n\\[\nX(\\omega) = (X_1(\\omega), X_2(\\omega), X_3(\\omega)) \\in \\{0,1\\}^3\n\\]\nas \\(\\omega\\) ranges over the domain \\(\\Omega = \\{0,1,\\ldots,7\\}\\). Since each \\(X_t\\) has only two possible outcomes, the entire process has \\(2^3 = 8\\) possible trajectories, all being listed in the printout.\nEach trajectory of the process has a probability computed via the formula (2) above. For example, for the trajectory \\(X(3)=(0,1,1)\\) we have\n\\[\nP_X\\big( (0, 1,1) \\big) = P(X_1 = 0) P(X_2 = 1) P(X_3 = 1) = 0.3 \\cdot 0.7^2 = 0.147.\n\\]\nThe probabilities of each trajectory can be accessed via the probability_measure attribute of the process. In this printout, note that trajectory 3 does indeed have the correct probability:\n\nP_X = X.probability_measure.with_name(\"P_X\")\n\nprint(P_X)\n\nProbability measure 'P_X':\n            probability\ntrajectory             \n0                 0.027\n1                 0.063\n2                 0.063\n3                 0.147\n4                 0.063\n5                 0.147\n6                 0.147\n7                 0.343\n\n\nTechnically, the domain \\(\\Omega\\) is supposed to have a \\(\\sigma\\)-algebra with respect to which the process \\(X\\) is measurable, but this is not enforced in SigAlg. However, \\(\\Omega\\) does have the natural filtration induced by \\(X\\), which is modeled in SigAlg. In our case, this filtration is a nested sequence of \\(\\sigma\\)-algebras\n\\[\n\\mathcal{F}_1 \\subset \\mathcal{F}_2 \\subset \\mathcal{F}_3\n\\]\nwhere\n\\[\n\\mathcal{F}_t = \\sigma(X_1, X_2, \\ldots, X_t), \\quad t = 1,2,3,\n\\]\nIn general, a containment \\(\\mathcal{F}\\subset \\mathcal{G}\\) of \\(\\sigma\\)-algebras on a common (finite!) sample space \\(\\Omega\\) is equivalent to the statement that every atom of \\(\\mathcal{F}\\) is a union of atoms of \\(\\mathcal{G}\\), which, in turn, is the same as saying that every atom of \\(\\mathcal{G}\\) is a subset of some atom of \\(\\mathcal{F}\\). In the present case, the reader will see this relationship in the “information flow” diagram below.\nis the \\(\\sigma\\)-algebra generated by the random vector \\((X_1, X_2, \\ldots, X_t)\\). This is the coarest \\(\\sigma\\)-algebra with respect to which \\(X_1, X_2, \\ldots, X_t\\) are all measurable. But, since \\(\\Omega\\) is finite, \\(\\sigma\\)-algebras can be completely described in terms of their atoms, which are the minimal non-empty sets in the \\(\\sigma\\)-algebra (and which partition \\(\\Omega\\)). The atoms of the \\(\\sigma\\)-algebra \\(\\mathcal{F}_t\\) are simply the level sets of the random vector \\((X_1,X_2,\\ldots,X_t)\\) as a function on \\(\\Omega\\).\nIn SigAlg, \\(\\sigma\\)-algebras are stored internally by associating to each sample point \\(\\omega\\in \\Omega\\) an atom identifier that specifies which atom the sample point belongs to. Since the atoms of the \\(\\sigma\\)-algebras \\(\\mathcal{F}_t\\) are the level sets of \\((X_1,X_2,\\ldots,X_t)\\), a convenient set of atom IDs are the values of these random vectors themselves.\nFor example, here are the atom IDs of \\(\\mathcal{F}_1\\):\n\nimport pandas as pd\n\n# Get the natural filtration\nF = X.natural_filtration\n\n# Grab the F1 sigma-algebra\nF1 = F[1]\n\n# Display the atom IDs alongside the process data.\n# Almost all objects in SigAlg have `data` attributes that return Pandas objects.\ndisplay_df = pd.concat([X.data, F1.data.rename(\"F1 atom ID\")], axis=1)\n\nprint(display_df)\n\n            1  2  3  F1 atom ID\ntrajectory                     \n0           0  0  0           0\n1           0  0  1           0\n2           0  1  0           0\n3           0  1  1           0\n4           1  0  0           1\n5           1  0  1           1\n6           1  1  0           1\n7           1  1  1           1\n\n\nIndeed, notice that the atom IDs are simply the values of the random variable \\(X_1(\\omega)\\in \\{0,1\\}\\), as \\(\\omega\\) ranges over \\(\\Omega = \\{0,1,\\ldots,7\\}\\).\nHere are the atom IDs of \\(\\mathcal{F}_2\\):\n\nF2 = F[2]\ndisplay_df = pd.concat([display_df, F2.data.rename(\"F2 atom ID\")], axis=1)\n\nprint(display_df)\n\n            1  2  3  F1 atom ID F2 atom ID\ntrajectory                                \n0           0  0  0           0     (0, 0)\n1           0  0  1           0     (0, 0)\n2           0  1  0           0     (0, 1)\n3           0  1  1           0     (0, 1)\n4           1  0  0           1     (1, 0)\n5           1  0  1           1     (1, 0)\n6           1  1  0           1     (1, 1)\n7           1  1  1           1     (1, 1)\n\n\nAgain, note that the atom IDs are the values of the random vector \\((X_1,X_2)\\), as \\(\\omega\\) ranges over \\(\\Omega = \\{0,1,\\ldots,7\\}\\).\nFor the third \\(\\sigma\\)-algebra \\(\\mathcal{F}_3\\), we have:\n\nF3 = F[3]\ndisplay_df = pd.concat([display_df, F3.data.rename(\"F3 atom ID\")], axis=1)\n\nprint(display_df)\n\n            1  2  3  F1 atom ID F2 atom ID F3 atom ID\ntrajectory                                           \n0           0  0  0           0     (0, 0)  (0, 0, 0)\n1           0  0  1           0     (0, 0)  (0, 0, 1)\n2           0  1  0           0     (0, 1)  (0, 1, 0)\n3           0  1  1           0     (0, 1)  (0, 1, 1)\n4           1  0  0           1     (1, 0)  (1, 0, 0)\n5           1  0  1           1     (1, 0)  (1, 0, 1)\n6           1  1  0           1     (1, 1)  (1, 1, 0)\n7           1  1  1           1     (1, 1)  (1, 1, 1)\n\n\nIn this last case, note that each \\(\\omega\\) has its own unique atom ID, which means that the \\(\\sigma\\)-algebra \\(\\mathcal{F}_3\\) is the power set on \\(\\Omega\\), i.e., the finest \\(\\sigma\\)-algebra on \\(\\Omega\\).\nIn probability theory, these filtrations (increasing sequences of \\(\\sigma\\)-algebras) are intended to model “refinements of information” available to an observer over time. For our current IID process \\(X\\), the natural filtration might have the following concrete information-theoretic interpretation: Suppose that each \\(X_t\\) (for \\(t=1,2,3\\)) does indeed model the outcome of a (biased) coin flip, with \\(X_t=0\\) representing a tail, and \\(X_t=1\\) a head on the \\(t\\)-th flip. Now, suppose that all three flips have occured, so that a definite trajectory \\(X(\\omega)\\) of the process has been identified, which is the same as saying that a definite sample point \\(\\omega\\) has been identified, since the sample points and their trajectories are in one-to-one correspondence.\nHowever, suppose that our observer does not yet know which \\(\\omega\\) has been realized, and its true value is only progressively revealed over time, flip by flip.\nAt time \\(t=1\\), the observer sees the result of the first flip, \\(X_1(\\omega)\\). With this information, the observer would be able to identify which of the two atoms of \\(\\mathcal{F}_1\\) contains the sample point \\(\\omega\\). And conversely, if the observer knew which of these atoms contained \\(\\omega\\), then the observer would also know the value of \\(X_1(\\omega)\\). So, in this sense, the information contained in knowledge of the value \\(X_1(\\omega)\\) is equally well contained in the \\(\\sigma\\)-algebra \\(\\mathcal{F}_1\\).\nNote that while this information reduces some uncertainty about the identity of \\(\\omega\\), it does not reduce all of it. For within each atom of \\(\\mathcal{F}_1\\), the differenent trajectories \\(X(\\omega)\\) are indistinguishable, given only the information in \\(\\mathcal{F}_1\\).\nNow, at time \\(t=2\\), the observer sees the result of the second flip, \\(X_2(\\omega)\\). With this additional information, combined with knowledge of the first flip, the observer would be able to identify which of the four atoms of \\(\\mathcal{F}_2\\) contains the sample point \\(\\omega\\). But again, while this reduces more of the uncertainty about \\(\\omega\\), it does not eliminate it entirely, for the observer still cannot distinguish between the trajectories \\(X(\\omega)\\) within each of the four atoms of \\(\\mathcal{F}_2\\), given only the information in \\(\\mathcal{F}_2\\).\nFinally, at time \\(t=3\\), the observer sees the result of the third flip, \\(X_3(\\omega)\\). As in the previous two cases, with this new information, the observer can identify which of the eight atoms of \\(\\mathcal{F}_3\\) contains the sample point \\(\\omega\\). But since \\(\\mathcal{F}_3\\) is the power set, this amounts to complete information on \\(\\omega\\). The trajectory \\(X(\\omega)\\) is now fully determined.\nThis “flow” of information, refining the observer’s knowledge over time, is precisely what a filtration models. This flow can be visualized through SigAlg’s interface with Plotly, to create a Sankey-style figure:\n\n# These two lines are only necessary to render in Quarto markdown.\n# The reader may remove them.\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\nfrom sigalg.core import plot_information_flow\n\n# Reusable plotting parameters\nplot_config = dict(\n    height=400,\n    node_color=\"#FFC300\",\n    font_color=\"#FAF9F6\",\n    link_color=\"rgba(168, 165, 159, 0.5)\",\n    background_color=\"#121212\",\n    font_family=\"monospace\",\n)\n\nfig = plot_information_flow(\n    filtration=F,\n    title=\"Natural filtration of 'X'\",\n    show_atom_counts=False,\n    **plot_config,\n)\nfig.show()\n\n                            \n                                            \n\n\nAs we read the figure from left to right through time, notice that each atom is split into a union of atoms in the next \\(\\sigma\\)-algebra. This illustrates how the observer’s knowledge is refined over time as more information becomes available. If earlier atoms were not unions of later atoms, then one of the containments \\(\\mathcal{F}_1 \\subset \\mathcal{F}_2\\) or \\(\\mathcal{F}_2 \\subset \\mathcal{F}_3\\) would be violated, which means that the three \\(\\sigma\\)-algebras would not form a filtration. (Recall the most recent margin note above.)\nThe trajectories of our process \\(X\\) are short. Longer trajectories can be generated by increasing the length parameter in the Time instance, though care must be taken since the number of possible trajectories grows exponentially with the length of the trajectories. For long trajectories, it is often more practical to work with Monte Carlo simulations rather than exhaustive enumerations. In the next code block, we simulate a single trajectory of length 100 from the same IID Bernoulli process above and plot it:\n\nimport matplotlib.pyplot as plt\n\n# The reader may remove this line, or set their own custom style\nplt.style.use(\"../../aux-files/custom-theme.mplstyle\")\n\n# Set the time index of the process to a longer length\nT = Time.discrete(start=1, length=100)\nX.time = T\n\n# Simulate a single trajectory\nX.from_simulation(\n    n_trajectories=1,\n    random_state=42,\n)\n\n# Plot the trajectory\n_, ax = plt.subplots(figsize=(7, 2))\nX.plot_trajectories(ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\nThe trajectory is plotted by first plotting the points \\((t, X_t)\\) for each time \\(t=1,2,\\ldots,101\\), and then connecting these points with lines for visual clarity."
  },
  {
    "objectID": "posts/sigalg/index.html#random-walks",
    "href": "posts/sigalg/index.html#random-walks",
    "title": "Introducing SigAlg: Measure-Theoretic Probability in Python",
    "section": "Random walks",
    "text": "Random walks\nWith IID processes as building blocks, we can construct more complex stochastic processes such as random walks.\n\n\n\n\n\n\n\nDefinition 3 Let \\(p\\) be a real number with \\(0\\leq p \\leq 1\\), let \\(x_0\\) and \\(t_0\\) be integers, and let\n\\[\nT = \\{t_0, t_0+1, t_0+2, \\ldots, t_0 + n\\},\n\\]\nwhere \\(n\\) is either a positive integer or \\(\\infty\\). Then a random walk of length \\(n\\), with initial position \\(x_0\\) and initial time \\(t_0\\), is a stochastic process \\(X\\) with the following properties:\n\nWe have \\(X_{t_0}=x_0\\), the constant random variable.\nThe increments \\[\n\\Delta X_t \\stackrel{\\text{def}}{=} X_t - X_{t-1}, \\quad t \\in T, \\ t\\neq t_0,\n\\] are IID random variables.\nEach increment \\(\\Delta X_t\\) takes either the value \\(1\\) or \\(-1\\), the former with probability \\(p\\) and the latter with probability \\(q=1-p\\).\n\n\n\n\n\nWhen \\(p\\neq 0.5\\), the random walk is said to have drift, which we will discuss in more detail below. If \\(p=0.5\\), the random walk is said to be symmetric.\nThe reason for the name “random walk” is that each trajectory \\(X(\\omega)\\) can be visualized as a path on the integer number line starting at \\(x_0\\), where at each discrete time \\(t\\) a step is taken that moves either one unit to the right (when \\(\\Delta X_t=1\\), with probability \\(p\\)) or one unit to the left (when \\(\\Delta X_t=-1\\), with probability \\(q\\)). Over many steps, this results in a “wandering” behavior that resembles a random walk. The random variable \\(X_t\\) represents the position of the walk after \\(t\\) steps.\nThe SigAlg library provides the RandomWalk class to model these processes, but it is instructive to build a random walk from scratch using the IID processes in the previous section. To do this, note that each \\(X_t\\) is the cumulative sum of the increments up to time \\(t\\), plus the initial state \\(X_{t_0} = x_0\\):\n\\[\nX_t = X_{t_0} + \\sum_{s=t_0+1}^t \\Delta X_s.\n\\] Notice the formal similarity between this equation and the the Fundamental Theorem of Calculus: \\[f(t) = f(t_0) + \\int_{t_0}^tdf(s),\\] where we write \\(df(s) = f'(s) \\, ds\\).\nThus, by modeling the increments \\(\\Delta X_t\\) in SigAlg as an IID process, we can model a random walk \\(X\\) as the cumulative sum of these increments plus the starting point \\(x_0\\).\nTo begin, let’s suppose we want to model a random walk of length \\(n=10\\), with initial position \\(x_0=0\\), with initial time \\(t_0=0\\), and with probability \\(p=0.7\\) of stepping “to the right.” Then, note that\n\\[\n\\Delta X_t = 2B_t -1\n\\]\nfor \\(1 \\leq t \\leq 10\\), where \\(B_t \\sim \\mathcal{B}er(0.7)\\). Thus, in SigAlg, we write:\n\nfrom sigalg.core import RandomVariable\n\n# The increments start at time t=1 and stop at t=10\nT = Time.discrete(start=1, stop=10)\n\n# Define the IID Bernoulli process\nB = IIDProcess(\n    distribution=bernoulli(p=0.7),\n    time=T,\n    name=\"B\",\n).from_simulation(\n    n_trajectories=10,\n    random_state=42,\n)\n\n# Construct the increment process\nDelta_X = 2 * B - 1\n\n# Construct the random walk process by cumulative summation\nX = Delta_X.cumsum().with_name(\"X\")\n\n# Insert the initial state\nX.insert_rv(state=0, time=0, in_place=True)\n\nprint(X)\n\nStochastic process 'X':\ntime        0   1   2   3   4   5   6   7   8   9   10\ntrajectory                                            \n0            0  -1   0  -1   0   1   0  -1  -2  -1   0\n1            0   1   0   1   0   1   2   3   4   3   4\n2            0  -1   0  -1  -2  -3  -2  -1   0   1   2\n3            0  -1  -2  -1   0   1   2   3   4   5   6\n4            0   1   0  -1   0  -1  -2  -1   0   1   2\n5            0   1   2   1   2   1   0   1   2   3   4\n6            0   1   2   3   2   3   4   5   6   7   8\n7            0   1   2   1   2   3   4   5   6   7   6\n8            0   1   2   1   2   3   4   3   4   5   6\n9            0   1   2   3   4   5   6   7   8   9   8\n\n\nWe have simulated ten trajectories of the random walk. We see that the final position of trajectory 3 is 6, while the final position of trajectory 9 is 8. A plot of these simulated trajectories is shown below:\n\nyellow = \"#FFC300\"\n_, ax = plt.subplots(figsize=(7, 3))\nX.plot_trajectories(ax=ax, colors=[yellow], plot_kwargs={\"alpha\": 0.7})\nplt.show()\n\n\n\n\n\n\n\n\nLonger trajectories reveal the “drift” that we mentioned above. To make these longer trajectories, we use the built-in RandomWalk class in SigAlg, which (essentially) internally implements the same construction as above. The following code simulates ten trajectories of length 100 with the same parameters as our original random walk:\n\nfrom sigalg.processes import RandomWalk\n\nT = Time.discrete(length=100)\nX = RandomWalk(p=0.7, time=T).from_simulation(\n    n_trajectories=10,\n    random_state=42,\n)\n\n_, ax = plt.subplots(figsize=(7, 4))\nX.plot_trajectories(ax=ax, colors=[yellow], plot_kwargs={\"alpha\": 0.7})\nplt.show()\n\n\n\n\n\n\n\n\nWhile random, it is evident that the trajectories tend to drift upwards over time, since the expected increment is positive:\n\\[\nE(\\Delta X_t) = 2E(B_t) - 1 = 2 \\cdot 0.7 - 1 = 0.4 &gt; 0.\n\\]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "john myers, ph.d.",
    "section": "",
    "text": "I’m a Ph.D. mathematician and university professor with a background that spans both theoretical and applied mathematics. My undergraduate education was in applied and computational mathematics and in physics. In graduate school and through the first few years of my research career, I specialized in homological algebra and commutative ring theory, focusing on the bridge between modern algebra and geometry and topology. However, my interests have gradually shifted toward more practical domains, where I now work with probability theory, modeling, and machine learning. You can find my early mathematical research on my arXiv page.\n\n\n\n\n\nI am also the author of the open source textbook “Probability Theory with a View Toward Machine Learning,” and the creator of a brand new open source Python library, SigAlg, for measure-theoretic probability theory. Links to both are in the menu bar."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/multi-calc-sp-26.html",
    "href": "teaching/multi-calc-sp-26/multi-calc-sp-26.html",
    "title": "Multivariable Calculus",
    "section": "",
    "text": "This course is an introduction to multivariable calculus. We will cover the following topics:\n\nVectors and matrices: We will learn about vector algebra, dot products, matrix operations, and determinants.\nCalculus of functions of several variables: We will study limits, continuity, derivatives, and applications of all these concepts.\nMultiple integrals: We will explore double and triple integrals, as well as applications such as volume and surface area.\nVector calculus: We will cover vector fields, line integrals, surface integrals, Green’s theorem, Stokes’ theorem, and the divergence theorem.\n\n\nCourse information\n\n\n\nInstructor:\n\n\nJohn Myers\n\n\n\n\nOffice:\n\n\nMarano 175\n\n\n\n\nOffice hours:\n\n\n12-12:30 MWF, and by appointment\n\n\n\n\nSyllabus:\n\n\nlink\n\n\n\n\n\nSchedule, slides, exercises, and info\n\n\n\nweek\ndate\ntopics\nslides\nexercises\ninfo\n\n\n\n\nWeek 4\n02.20 fri\nSame as below ↓\nSame as below ↓\nSame as below ↓\n\n\n\n\n02.19 thu\nSec 8: The definition of the derivative\nSec 8 Slides\nSec 8 Exercises\nQuiz on Sec 5, 6, and 7\n\n\n\n02.18 wed\nSame as below ↓\nSame as below ↓\nSame as below ↓\n\n\n\n\n02.16 mon\nSec 7: Functions of multiple variables, part 3\nSec 7 Slides\nSec 7 Exercises\n\n\n\nWeek 3\n02.13 fri\nSec 6: Functions of multiple variables, part 2\nSec 6 Slides\nSec 6 Exercises\n\n\n\n\n02.12 thu\nSec 5: Functions of multiple variables, part 1\nSec 5 Slides\nSec 5 Exercises\nQuiz on Sec 2 and 3\n\n\n\n02.11 wed\nSec 4: Planes and linear spaces, part 2\nSec 4 Slides\nSec 4 Exercises\n\n\n\n\n02.09 mon\nSame as below ↓\nSame as below ↓\nSame as below ↓\n\n\n\nWeek 2\n02.06 fri\nSec 3: Planes and linear spaces, part 1\nSec 3 Slides\nSec 3 Exercises\n\n\n\n\n02.05 thu\nSame as below ↓\nSame as below ↓\nSame as below ↓\nQuiz on Sec 1Just vectors in \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\), no matrices\n\n\n\n02.04 wed\nSec 2: Vectors and matrices, part 2\nSec 2 Slides\nSec 2 Exercises\n\n\n\n\n02.02 mon\nSame as below ↓\nSame as below ↓\nSame as below ↓\n\n\n\nWeek 1\n01.30 fri\nSec 1: Vectors and matrices, part 1\nSec 1 Slides\nSame as below ↓\n\n\n\n\n01.29 thu\nAnother “remote day”: Watch the videos below if you haven’t already, continue working on exercises.\n\nSame as below ↓\n\n\n\n\n01.28 wed\nAnother “remote day”: Watch the videos below if you haven’t already, continue working on exercises.\n\nSame as below ↓\n\n\n\n\n01.26 mon\n“Remote day”: Watch the videos here, here, here, here, and here. Complete the exercises (see the link to the right).\n\nSec 1 Exercises"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/06-functions-of-multiple-variables-part-2.html#more-plotting-strategies-for-graphs",
    "href": "teaching/multi-calc-sp-26/slides/06-functions-of-multiple-variables-part-2.html#more-plotting-strategies-for-graphs",
    "title": "06 Functions of multiple variables, part 2",
    "section": "More plotting strategies for graphs",
    "text": "More plotting strategies for graphs\n\n\n\n\nStrategies for plotting graphs of \\(f: \\mathbb{R}^2 \\to \\mathbb{R}\\)\n\n\n\nIntersect the graph with horizontal planes of the form \\(z=c\\), for various values of \\(c\\).\n\nThe equation you solve is \\(c = f(x,y)\\).\nThese cross sections (also called level sets!) are curves in the \\(xy\\)-plane.\n\nIntersect the graph with vertical planes of the form \\(x=c\\), for various values of \\(c\\).\n\nThe equation you solve is \\(z=f(c,y)\\).\nThese cross sections are curves in the \\(yz\\)-plane.\n\nIntersect the graph with vertical planes of the form \\(y=c\\), for various values of \\(c\\).\n\nThe equation you solve is \\(z=f(x,c)\\).\nThese cross sections are curves in the \\(xz\\)-plane."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/06-functions-of-multiple-variables-part-2.html#exercise-1-practice-with-cross-sections",
    "href": "teaching/multi-calc-sp-26/slides/06-functions-of-multiple-variables-part-2.html#exercise-1-practice-with-cross-sections",
    "title": "06 Functions of multiple variables, part 2",
    "section": "Exercise 1: Practice with cross sections",
    "text": "Exercise 1: Practice with cross sections\n\n\n\nPlot the graphs of the following functions by using cross sections.\n\n\\(f(x,y) = x^2 - y^2\\)\n\\(g(x,y) = y\\sin{x}\\)\n\\(h(x,y) = -2x - y +5\\)\n\\(k(x,y) = y^3 + xy\\)"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/06-functions-of-multiple-variables-part-2.html#contour-plots",
    "href": "teaching/multi-calc-sp-26/slides/06-functions-of-multiple-variables-part-2.html#contour-plots",
    "title": "06 Functions of multiple variables, part 2",
    "section": "Contour plots",
    "text": "Contour plots\n\n\n\n\nPlacing all of the level sets of a function \\(f:\\mathbb{R}^2 \\to \\mathbb{R}\\) in the \\(xy\\)-plane gives us a contour plot of \\(f\\).\n\n\n\n\n\nSo, a contour plot is like a topographic map of the graph of \\(f\\).\nIn this context, the level sets are often called contours or contour lines."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/06-functions-of-multiple-variables-part-2.html#exercise-2-matching-contour-plots",
    "href": "teaching/multi-calc-sp-26/slides/06-functions-of-multiple-variables-part-2.html#exercise-2-matching-contour-plots",
    "title": "06 Functions of multiple variables, part 2",
    "section": "Exercise 2: Matching contour plots",
    "text": "Exercise 2: Matching contour plots\n\n\n\nMatch the graphs to the contour plots."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/06-functions-of-multiple-variables-part-2.html#exercise-3-sketch-a-contour-plot",
    "href": "teaching/multi-calc-sp-26/slides/06-functions-of-multiple-variables-part-2.html#exercise-3-sketch-a-contour-plot",
    "title": "06 Functions of multiple variables, part 2",
    "section": "Exercise 3: Sketch a contour plot",
    "text": "Exercise 3: Sketch a contour plot\n\n\n\nSketch a contour plot for the function \\(f(x,y) = xy\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-1-reminder-on-the-derivative",
    "href": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-1-reminder-on-the-derivative",
    "title": "08 The definition of the derivative",
    "section": "Exercise 1: Reminder on the derivative",
    "text": "Exercise 1: Reminder on the derivative\n\n\n\nConsider the function \\(f:\\mathbb{R} \\to \\mathbb{R}\\) defined by \\(f(x) = x^2\\).\n\nUse the derivative to approximate the change in the function \\(f(1.1) - f(1)\\).\nUse the derivative to approximate the change in the function \\(f(0.9) - f(1)\\).\nUse the derivative to approximate the function \\(f(x)\\) for \\(x\\) close to \\(1\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#from-calculus-i",
    "href": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#from-calculus-i",
    "title": "08 The definition of the derivative",
    "section": "From calculus I…",
    "text": "From calculus I…\n\nThe derivative of \\(f\\) at \\(x_0\\), as presented in single-variable calculus, is the number \\(f'(x_0)\\) such that \\[\nf'(x_0) = \\lim_{h \\to 0} \\frac{f(x_0+h) - f(x_0)}{h}.\n\\]\nAs long as \\(h\\) is small, we can drop the limit, replace “\\(=\\)” with “\\(\\approx\\)”, and rearrange to get \\[\nf(x_0 + h) - f(x_0) \\approx f'(x_0)h.\n\\]\nIn order for all of this to make sense, we need to think of \\(h\\) as: \\[\nh = (x_0 + h) - x_0.\n\\]\nIn this way of thinking, \\(h\\) is a “step vector” in the domain, i.e., a vector along the \\(x\\)-axis with its tail at the point \\(x_0\\) and its head at the point \\(x_0 + h\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#to-calculus-iii",
    "href": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#to-calculus-iii",
    "title": "08 The definition of the derivative",
    "section": "…to calculus III",
    "text": "…to calculus III\n\nFrom the previous slide: \\(f(x_0 + h) - f(x_0) \\approx f'(x_0)h\\).\n\n\n\n\n\nSingle-variable calculus\n\n\nLet \\(f:\\mathbb{R} \\to \\mathbb{R}\\) be a function and let \\(x_0\\) be a point in the domain of \\(f\\). The derivative \\(f'(x_0)\\) in single-variable calculus is:\n\nA NUMBER that…\n…you MULTIPLY against a step vector \\(h\\) in the domain…\n…to approximate the change in the function in the direction of \\(h\\), for which…\n…the approximation becomes exact, as the length of the step vector \\(h\\) goes to \\(0\\).\n\n\n\n\n\n\n\n\n\nMulti-variable calculus\n\n\nLet \\(f:\\mathbb{R}^n \\to \\mathbb{R}^m\\) be a function and let \\(\\mathbf{v}_0\\) be (the position vector of) a point in the domain of \\(f\\). The derivative \\(f'(\\mathbf{v}_0)\\) in multivariable calculus is:\n\nA MATRIX that…\n…you MULTIPLY against a step vector \\(\\mathbf{h}\\) in the domain…\n…to approximate the change in the function in the direction of \\(\\mathbf{h}\\), for which…\n…the approximation becomes exact, as the length of the step vector \\(\\mathbf{h}\\) goes to \\(0\\).\n\n\n\n\n\n\nSo… does that mean \\(f(\\mathbf{v}_0 + \\mathbf{h}) - f(\\mathbf{v}_0) \\approx f'(\\mathbf{v}_0)\\mathbf{h}\\)?\nABSOLUTELY!"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#derivatives-the-official-definition",
    "href": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#derivatives-the-official-definition",
    "title": "08 The definition of the derivative",
    "section": "Derivatives: the official definition",
    "text": "Derivatives: the official definition\n\nOne clever way to make a step vector \\(\\mathbf{h}\\) go to \\(0\\) is to consider a scalar multiple \\(\\lambda\\mathbf{h}\\), and let \\(\\lambda\\to 0\\).\nThen, our approximation becomes \\[\nf(\\mathbf{v}_0 + \\lambda\\mathbf{h}) - f(\\mathbf{v}_0) \\approx f'(\\mathbf{v}_0)(\\lambda\\mathbf{h}).\n\\]\nBut, because \\(f'(\\mathbf{v}_0)(\\lambda\\mathbf{h})\\) is the product of a matrix and a scalar multiple of a vector, the \\(\\lambda\\) can be pulled out: \\[\nf'(\\mathbf{v}_0)(\\lambda\\mathbf{h}) = \\lambda f'(\\mathbf{v}_0)\\mathbf{h}.\n\\]\nThen, our approximation becomes \\[\nf(\\mathbf{v}_0 + \\lambda\\mathbf{h}) - f(\\mathbf{v}_0) \\approx \\lambda f'(\\mathbf{v}_0)\\mathbf{h}.\n\\]\nDividing both sides by \\(\\lambda\\) and flipping sides gives \\[\nf'(\\mathbf{v}_0)\\mathbf{h} \\approx \\frac{f(\\mathbf{v}_0 + \\lambda\\mathbf{h}) - f(\\mathbf{v}_0)}{\\lambda}.\n\\]\nThis approximation is supposed to become exact as \\(\\lambda\\to0\\), which leads us to.\n\n\n\n\n\nDefinition.\n\n\nLet \\(f:\\mathbb{R}^n \\to \\mathbb{R}^m\\) be a function and let \\(\\mathbf{v}_0\\) be (the position vector of) a point in the domain of \\(f\\). The derivative of \\(f\\) at \\(\\mathbf{v}_0\\) is the \\(m\\times n\\) matrix \\(f'(\\mathbf{v}_0)\\) such that \\[\nf'(\\mathbf{v}_0)\\mathbf{h} = \\lim_{\\lambda \\to 0} \\frac{f(\\mathbf{v}_0 + \\lambda\\mathbf{h}) - f(\\mathbf{v}_0)}{\\lambda}\n\\] for all vectors \\(\\mathbf{h}\\) in \\(\\mathbb{R}^n\\). (Provided the limit exists.)\n\n\n\n\n\nA couple things:\n\nThe derivative is unique, if it exists.\nWe’ve defined the derivative to be a matrix. This doesn’t quite follow the standard definition. Technically, our derivative is what most people call the Jacobian matrix.\nThe function \\(f\\) will usually be given to you in terms of points, not vectors. You’ll need to get used to switching back and forth between points and vectors."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-2-computing-the-derivative-of-a-function-mathbbr2-to-mathbbr2",
    "href": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-2-computing-the-derivative-of-a-function-mathbbr2-to-mathbbr2",
    "title": "08 The definition of the derivative",
    "section": "Exercise 2: Computing the derivative of a function \\(\\mathbb{R}^2 \\to \\mathbb{R}^2\\)",
    "text": "Exercise 2: Computing the derivative of a function \\(\\mathbb{R}^2 \\to \\mathbb{R}^2\\)\n\n\n\nConsider the function\n\\[\nf:\\mathbb{R}^2 \\to \\mathbb{R}^2, \\quad f(x,y) = (y^2+x, y).\n\\]\nCompute the derivative \\(f'(x,y)\\) using the definition, following the steps below.\n\nIdentify the size of the derivative matrix \\(f'(x,y)\\).\nIdentify the position vector \\(\\mathbf{v}_0\\) and its size.\nIdentify the step vector \\(\\mathbf{h}\\) and its size.\nNow compute \\(f'(x,y)\\) using the above information and the definition of the derivative.\nUse your answer to approximate the change in the function \\(f(1.1, 0.9) - f(1,1)\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-3-computing-the-derivative-of-a-function-mathbbr-to-mathbbr2",
    "href": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-3-computing-the-derivative-of-a-function-mathbbr-to-mathbbr2",
    "title": "08 The definition of the derivative",
    "section": "Exercise 3: Computing the derivative of a function \\(\\mathbb{R} \\to \\mathbb{R}^2\\)",
    "text": "Exercise 3: Computing the derivative of a function \\(\\mathbb{R} \\to \\mathbb{R}^2\\)\n\n\n\nConsider the function\n\\[\nf:\\mathbb{R} \\to \\mathbb{R}^2, \\quad f(t) = (t^2, t^3),\n\\]\nCompute the derivative \\(f'(t)\\) using the definition, following the steps below.\n\nIdentify the size of the derivative matrix \\(f'(t)\\).\nIdentify the position vector \\(\\mathbf{v}_0\\) and its size.\nIdentify the step vector \\(\\mathbf{h}\\) and its size.\nNow compute \\(f'(t)\\) using the above information and the definition of the derivative.\nUse your answer to approximate the change in the function \\(f(1.9) - f(2)\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-4-computing-the-derivative-of-another-function-mathbbr-to-mathbbr2",
    "href": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-4-computing-the-derivative-of-another-function-mathbbr-to-mathbbr2",
    "title": "08 The definition of the derivative",
    "section": "Exercise 4: Computing the derivative of another function \\(\\mathbb{R} \\to \\mathbb{R}^2\\)",
    "text": "Exercise 4: Computing the derivative of another function \\(\\mathbb{R} \\to \\mathbb{R}^2\\)\n\n\n\nConsider the function\n\\[\nf:\\mathbb{R} \\to \\mathbb{R}^2, \\quad f(t) = (\\cos{t}, \\sin{t}),\n\\]\nCompute the derivative \\(f'(t)\\) using the definition, following the steps below.\n\nIdentify the size of the derivative matrix \\(f'(t)\\).\nIdentify the position vector \\(\\mathbf{v}_0\\) and its size.\nIdentify the step vector \\(\\mathbf{h}\\) and its size.\nNow compute \\(f'(t)\\) using the above information and the definition of the derivative.\nUse your answer to approximate the change in the function \\(f(1.7) - f(1.8)\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-5-computing-the-derivative-of-a-function-mathbbr2-to-mathbbr",
    "href": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-5-computing-the-derivative-of-a-function-mathbbr2-to-mathbbr",
    "title": "08 The definition of the derivative",
    "section": "Exercise 5: Computing the derivative of a function \\(\\mathbb{R}^2 \\to \\mathbb{R}\\)",
    "text": "Exercise 5: Computing the derivative of a function \\(\\mathbb{R}^2 \\to \\mathbb{R}\\)\n\n\n\nConsider the function\n\\[\nf:\\mathbb{R}^2 \\to \\mathbb{R}, \\quad f(x,y) = x^2 + y^2,\n\\]\nCompute the derivative \\(f'(x,y)\\) using the definition, following the steps below.\n\nIdentify the size of the derivative matrix \\(f'(x,y)\\).\nIdentify the position vector \\(\\mathbf{v}_0\\) and its size.\nIdentify the step vector \\(\\mathbf{h}\\) and its size.\nNow compute \\(f'(x,y)\\) using the above information and the definition of the derivative.\nUse your answer to approximate the change in the function \\(f(10.2, -4.3) - f(10.05,-4.15)\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#tangent-approximations",
    "href": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#tangent-approximations",
    "title": "08 The definition of the derivative",
    "section": "Tangent approximations",
    "text": "Tangent approximations\n\nLet \\(f:\\mathbb{R}^n \\to \\mathbb{R}^m\\) be a function and let \\(\\mathbf{v}_0\\) be (the position vector of) a point in the domain of \\(f\\).\nAs long as the step vector \\(\\mathbf{h}\\) is small, the derivative \\(f'(\\mathbf{v}_0)\\) was designed so that \\[\nf(\\mathbf{v}_0 + \\mathbf{h}) \\approx f'(\\mathbf{v}_0)\\mathbf{h} + f(\\mathbf{v}_0).\n\\]\nNow, if \\(\\mathbf{v}\\) is the position vector of a (variable) point in the domain of \\(f\\) that is close to \\(\\mathbf{v}_0\\), then we can write \\(\\mathbf{v} = \\mathbf{v}_0 + \\mathbf{h}\\) for some small step vector \\(\\mathbf{h}\\).\nThen, we can write \\[\nf(\\mathbf{v}) \\approx f'(\\mathbf{v}_0)(\\mathbf{v} - \\mathbf{v}_0) + f(\\mathbf{v}_0).\n\\]\nThe right-hand side of this approximation has a name:\n\n\n\n\n\nDefinition.\n\n\nLet \\(f:\\mathbb{R}^n \\to \\mathbb{R}^m\\) be a differentiable function and let \\(\\mathbf{v}_0\\) be (the position vector of) a point in the domain of \\(f\\). The tangent approximation to \\(f\\) at \\(\\mathbf{v}_0\\) is the function\n\\[\nL_{\\mathbf{v}_0}(\\mathbf{v}) = f'(\\mathbf{v}_0)(\\mathbf{v} - \\mathbf{v}_0) + f(\\mathbf{v}_0),\n\\]\nwhere \\(\\mathbf{v}\\) is the position vector of a (variable) point in the domain of \\(f\\).\n\n\n\n\n\nThis supposes that the derivative \\(f'(\\mathbf{v}_0)\\) exists, i.e., that \\(f\\) is differentiable at \\(\\mathbf{v}_0\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#tangent-spaces",
    "href": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#tangent-spaces",
    "title": "08 The definition of the derivative",
    "section": "Tangent spaces",
    "text": "Tangent spaces\n\n\n\n\nDefinition.\n\n\nLet \\(f:\\mathbb{R}^n \\to \\mathbb{R}^m\\) be a differentiable function and let \\(\\mathbf{v}_0\\) be (the position vector of) a point in the domain of \\(f\\). The tangent space to \\(f\\) at \\(\\mathbf{v}_0\\) is the range of the tangent approximation \\(L_{\\mathbf{v}_0}\\).\n\n\n\n\n\nA couple concrete cases to keep in mind:\n\nIf \\(f: \\mathbb{R} \\to \\mathbb{R}\\) and you plot the graph of \\(f\\) in the \\(xy\\)-plane, then the tangent space is nothing but the tangent line to the graph from single-variable calculus.\nIf \\(f: \\mathbb{R}^2 \\to \\mathbb{R}\\) and you plot the graph of \\(f\\) in \\(xyz\\)-space, then the tangent space is the tangent plane to the graph.\nIf \\(f: \\mathbb{R} \\to \\mathbb{R}^2\\) and you visualize the action of \\(f\\) as a physical transformation embedding the \\(t\\)-line into the \\(xy\\)-plane (like in the previous section!), then the tangent space is the tangent line to the curve at the point \\(f(\\mathbf{v}_0)\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-6-a-tangent-plane",
    "href": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-6-a-tangent-plane",
    "title": "08 The definition of the derivative",
    "section": "Exercise 6: A tangent plane",
    "text": "Exercise 6: A tangent plane\n\n\n\nConsider the function\n\\[\nf:\\mathbb{R}^2 \\to \\mathbb{R}, \\quad f(x,y) = x^2 + y^2,\n\\]\n\nCompute the tangent approximation to \\(f\\) at the point \\((x_0,y_0) = (1,1)\\).\nPlot the graph of \\(f\\) along with its tangent plane at the point \\((x_0,y_0) = (1,1)\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-7-a-tangent-line",
    "href": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-7-a-tangent-line",
    "title": "08 The definition of the derivative",
    "section": "Exercise 7: A tangent line",
    "text": "Exercise 7: A tangent line\n\n\n\nConsider the function\n\\[\nf:\\mathbb{R} \\to \\mathbb{R}^2, \\quad f(t) = (t^2, t^3),\n\\]\n\nCompute the tangent approximation to \\(f\\) at the point \\(t_0 = 1\\).\nVisualizing \\(f\\) as a physical transformation embedding the \\(t\\)-line into the \\(xy\\)-plane, plot the tangent line to the curve at the point \\(t_0 = 1\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-8-another-tangent-line",
    "href": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-8-another-tangent-line",
    "title": "08 The definition of the derivative",
    "section": "Exercise 8: Another tangent line",
    "text": "Exercise 8: Another tangent line\n\n\n\nConsider the function\n\\[\nf:\\mathbb{R} \\to \\mathbb{R}^2, \\quad f(t) = (\\cos{t}, \\sin{t}),\n\\]\n\nCompute the tangent approximation to \\(f\\) at the point \\(t_0 = 1\\).\nVisualizing \\(f\\) as a physical transformation embedding the \\(t\\)-line into the \\(xy\\)-plane, plot the tangent line to the curve at the point \\(t_0 = 1\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-8-a-strange-tangent-space",
    "href": "teaching/multi-calc-sp-26/slides/08-the-definition-of-the-derivative.html#exercise-8-a-strange-tangent-space",
    "title": "08 The definition of the derivative",
    "section": "Exercise 8: A strange tangent space",
    "text": "Exercise 8: A strange tangent space\n\n\n\nConsider the function\n\\[\nf:\\mathbb{R}^2 \\to \\mathbb{R}, \\quad f(x,y) = (y^3 +x, y),\n\\]\n\nCompute the tangent approximation to \\(f\\) at the point \\((x_0,y_0) = (1,1)\\).\nWhat is the tangent space to \\(f\\) at the point \\((x_0,y_0) = (1,1)\\)? Is it a line, a plane, or something else?"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#sets-of-points-and-vectors",
    "href": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#sets-of-points-and-vectors",
    "title": "02 Vectors and matrices, part 2",
    "section": "Sets of points and vectors",
    "text": "Sets of points and vectors\n\n\n\n\n\\(\\mathbb{R}^n\\) as a set of points\n\n\nWe write \\(\\mathbb{R}^n\\) to denote the set of all points in \\(n\\)-dimensional space.\n\nA point is represented by an ordered \\(n\\)-tuple of real numbers \\((x_1, x_2, \\dots, x_n)\\).\nThe numbers \\(x_1, x_2, \\dots, x_n\\) are called the coordinates of the point.\n\n\n\n\n\n\nWe might still think of points in \\(\\mathbb{R}^n\\) as points in an \\(n\\)-dimensional “hyper” space. But we can’t literally see them!\n\n\n\n\n\n\\(\\mathbb{R}^n\\) as a set of vectors\n\n\nWe write \\(\\mathbb{R}^n\\) to denote the set of all vectors in \\(n\\)-dimensional space.\n\nSuch a vector is represented as a \\(n\\)-dimensional column vectors written as \\[\n  \\begin{bmatrix}\n  x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n  \\end{bmatrix}\n  \\] where \\(x_1, x_2, \\dots, x_n\\) are real numbers.\nThe numbers \\(x_1, x_2, \\dots, x_n\\) are called the components of the vector.\n\n\n\n\n\n\nSimilarly, we can think of vectors in \\(\\mathbb{R}^n\\) as arrows pointing from the origin in \\(\\bbr^n\\) to the point \\((x_1, x_2, \\dots, x_n)\\) in \\(n\\)-dimensional space."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#algebra-norms-and-dot-products-in-n-space",
    "href": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#algebra-norms-and-dot-products-in-n-space",
    "title": "02 Vectors and matrices, part 2",
    "section": "Algebra, norms, and dot products in \\(n\\)-space",
    "text": "Algebra, norms, and dot products in \\(n\\)-space\n\nAll the definitions and properties of vector algebra, norms, and dot products that we discussed for \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\) also carry over to \\(\\mathbb{R}^n\\) with only minor modifications.\nFor exmaple, given two vectors \\[\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix},\\] their sum/difference is defined as \\[\\mathbf{u} \\pm \\mathbf{v} = \\begin{bmatrix} u_1 \\pm v_1 \\\\ u_2 \\pm v_2 \\\\ \\vdots \\\\ u_n \\pm v_n \\end{bmatrix}.\\]\nTheir norm is defined as \\[\\lVert \\mathbf{u} \\rVert = \\sqrt{u_1^2 + u_2^2 + \\dots + u_n^2}.\\]\nTheir inner product is defined as \\[\\langle \\mathbf{u}, \\mathbf{v} \\rangle = u_1v_1 + u_2v_2 + \\dots + u_nv_n.\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#standard-basis-vectors-in-n-space",
    "href": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#standard-basis-vectors-in-n-space",
    "title": "02 Vectors and matrices, part 2",
    "section": "Standard basis vectors in \\(n\\)-space",
    "text": "Standard basis vectors in \\(n\\)-space\n\n\n\n\nStandard basis vectors in \\(\\mathbb{R}^n\\)\n\n\nThe standard basis vectors in \\(\\mathbb{R}^n\\) are denoted by \\(\\mathbf{e}_1\\), \\(\\mathbf{e}_2\\), \\(\\dots\\), \\(\\mathbf{e}_n\\), and are defined as \\[\n\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\quad \\dots, \\quad \\mathbf{e}_n = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}.\n\\]\n\n\n\n\n\n\n\n\nTheorem: Resolving vectors into standard basis vectors\n\n\nAny vector\n\\[\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\end{bmatrix}\\]\nin \\(\\mathbb{R}^n\\) can be expressed as a linear combination of the standard basis vectors (i.e., a sum of scalar multiples of the basis vectors) as:\n\\[\\mathbf{u} = u_1 \\mathbf{e}_1 + u_2 \\mathbf{e}_2 + \\dots + u_n \\mathbf{e}_n.\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#exercise-1-practice-with-vectors-in-4-space",
    "href": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#exercise-1-practice-with-vectors-in-4-space",
    "title": "02 Vectors and matrices, part 2",
    "section": "Exercise 1: Practice with vectors in \\(4\\)-space",
    "text": "Exercise 1: Practice with vectors in \\(4\\)-space\n\n\n\na. Practice with vector algebra\n\n\nLet\n\\[\n\\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\\\ 3 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{v} = \\begin{bmatrix} 0 \\\\ -1 \\\\ 2 \\\\ 1 \\end{bmatrix}\n\\]\n\nCompute \\(\\mathbf{u} +\\mathbf{v}\\).\nCompute \\(2\\mathbf{v}\\).\n\n\n\n\n\n\n\nb. Practice with inner produts and norms\n\n\nWith the same vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) as in part (a):\n\nCompute \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle\\).\nCompute \\(\\lVert \\mathbf{u} \\rVert\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#definition-of-a-matrix",
    "href": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#definition-of-a-matrix",
    "title": "02 Vectors and matrices, part 2",
    "section": "Definition of a matrix",
    "text": "Definition of a matrix\n\n\n\n\nMatrices\n\n\nAn \\(m\\times n\\) matrix is a rectangular array of numbers with \\(m\\) rows and \\(n\\) columns, typically written as \\[\nA = \\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix}.\n\\]\n\nThe numbers in a matrix are called entries or elements.\nThe entry \\(a_{ij}\\) is said to be in the \\((i,j)\\)-th position of the matrix.\n\n\n\n\n\n\n\n\n\nMatrix algebra\n\n\n\nMatrix addition and subtraction is performed element-wise. However, both matrix must have the same number of rows and columns.\nScalar multiplication is performed by multiplying each entry of the matrix by the scalar.\nMatrix multiplication is defined when the number of columns of the first matrix equals the number of rows of the second matrix. The product is obtained by taking the dot product of rows and columns. (See the next exercise.)"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#exercise-2-practice-with-matrices",
    "href": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#exercise-2-practice-with-matrices",
    "title": "02 Vectors and matrices, part 2",
    "section": "Exercise 2: Practice with matrices",
    "text": "Exercise 2: Practice with matrices\n\n\n\na. Practice with matrix addition/subtraction/scaling\n\n\nLet\n\\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 0 & -1 \\\\ 2 & 1 \\\\ -3 & 0 \\end{bmatrix}.\n\\]\n\nThe matrices are both \\(m\\times n\\). What is \\(m\\)? What is \\(n\\)?\nCompute \\(A + B\\).\nCompute \\(2A - B\\).\n\n\n\n\n\n\n\nb. Practice with matrix multiplication\n\n\nLet\n\\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}, \\quad C = \\begin{bmatrix} 0 & -1 \\\\ 2 & 1 \\end{bmatrix}.\n\\]\n\nThe matrix \\(A\\) is \\(m\\times n\\). What is \\(m\\)? What is \\(n\\)?\nThe matrix \\(C\\) is \\(p\\times q\\). What is \\(p\\)? What is \\(q\\)?\nCompute \\(AC\\), if possible. What is the size of the resulting matrix?\nCompute \\(CA\\), if possible. What is the size of the resulting matrix?"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#exercise-3-more-practice-with-matrix-multiplication",
    "href": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#exercise-3-more-practice-with-matrix-multiplication",
    "title": "02 Vectors and matrices, part 2",
    "section": "Exercise 3: More practice with matrix multiplication",
    "text": "Exercise 3: More practice with matrix multiplication\n\n\n\nMatrices multiplied by vectors\n\n\nLet\n\\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}, \\quad \\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}.\n\\]\n\nYou can think of \\(\\mathbf{v}\\) as a matrix. If its size is \\(m\\times n\\), what is \\(m\\)? What is \\(n\\)?\nCompute \\(A\\mathbf{v}\\), if possible. What is the size of the resulting matrix?\nCompute \\(\\mathbf{v}A\\), if possible. What is the size of the resulting matrix?"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#a-quick-reminder-on-functions",
    "href": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#a-quick-reminder-on-functions",
    "title": "02 Vectors and matrices, part 2",
    "section": "A quick reminder on functions",
    "text": "A quick reminder on functions\n\nImportant to remember: A function \\(f\\) is a “machine” that accepts an input, and produces an output.\nThe set \\(D\\) of inputs is called the domain of \\(f\\), while the set \\(C\\) that contains the outputs is called the codomain.\nThe actual set of outputs is called the range of \\(f\\).\nThis is represented notationally as \\(f:D\\to C\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#exercise-4-practice-with-function-notation",
    "href": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#exercise-4-practice-with-function-notation",
    "title": "02 Vectors and matrices, part 2",
    "section": "Exercise 4: Practice with function notation",
    "text": "Exercise 4: Practice with function notation\n\n\n\nA familiar function from pre-calculus\n\n\nSuppose you’re in a pre-calculus class (or calculus, I guess) and your instructor writes \\(f(x) = x^2\\) on the board.\n\nWhat is (likely) the intended domain of \\(f\\)?\nWhat is the range of \\(f\\)?\nWhat are some possibilities for codomains of \\(f\\)?\nExpress your answers in (a) and (c) using the “arrow” notation.\n\nNow do the same four exercises, but with the expression \\(g(x) = \\sqrt{x}\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#functions-from-matrices",
    "href": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#functions-from-matrices",
    "title": "02 Vectors and matrices, part 2",
    "section": "Functions from matrices",
    "text": "Functions from matrices\n\n\n\n\nTheorem: Every matrix yields a function\n\n\nLet \\(A\\) be an \\(m\\times n\\) matrix. Then \\(A\\) defines a function \\(T:\\mathbb{R}^n \\to \\mathbb{R}^m\\) by the formula\n\\[\nT(\\mathbf{v}) = A\\mathbf{v}.\n\\]\nIn words, \\(T\\) is a function that takes an \\(n\\)-dimensional vector and outputs an \\(m\\)-dimensional vector."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#exercise-5-functions-from-matrices",
    "href": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#exercise-5-functions-from-matrices",
    "title": "02 Vectors and matrices, part 2",
    "section": "Exercise 5: Functions from matrices",
    "text": "Exercise 5: Functions from matrices\n\n\n\nNote\n\n\nLet\n\\[\nA = \\begin{bmatrix} 1 & -1 \\\\ 1 & 1 \\end{bmatrix}\n\\]\nThen \\(A\\) defines a function \\(T\\), as described on the previous slide.\n\nWhat is the domain of \\(T\\)? What is its codomain?\nCompute \\(T\\left(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\right)\\).\nCompute \\(T\\left(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\right)\\).\nCompute \\(T\\left( \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix} \\right)\\).\nConsider the line \\(y=0\\) in \\(\\mathbb{R}^2\\). What does \\(T\\) do to this line?\nConsider the line \\(x=0\\) in \\(\\mathbb{R}^2\\). What does \\(T\\) do to this line?\nConsider the unit square in the first quadrant of \\(\\mathbb{R}^2\\). This is the square that has \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\) as two of its sides. What does \\(T\\) do to this square?"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#determinants-of-2times-2-matrices",
    "href": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#determinants-of-2times-2-matrices",
    "title": "02 Vectors and matrices, part 2",
    "section": "Determinants of \\(2\\times 2\\) matrices",
    "text": "Determinants of \\(2\\times 2\\) matrices\n\n\n\n\nDeterminants of \\(2\\times 2\\) matrices\n\n\nLet\n\\[\nA = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}.\n\\]\nThe determinant of \\(A\\) is defined as the number\n\\[\n\\det(A) = ad - bc.\n\\]\n\n\n\n\n\n\n\n\nTheorem: \\(2\\times 2\\) determinants and area\n\n\nLet\n\\[\nA = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}.\n\\]\nThe area of the parallelogram with sides given by the columns of \\(A\\) is \\(|\\det(A)|\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#exercise-6-determinants-and-area",
    "href": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#exercise-6-determinants-and-area",
    "title": "02 Vectors and matrices, part 2",
    "section": "Exercise 6: Determinants and area",
    "text": "Exercise 6: Determinants and area\n\n\n\na. Computing \\(2\\times 2\\) determinants\n\n\nCompute the determinants of the following matrices.\n\n\\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n\\]\n\\[\nB = \\begin{bmatrix} 0 & -1 \\\\ 2 & 3 \\end{bmatrix}\n\\]\n\\[\nC = \\begin{bmatrix} 1 & 0 \\\\ -1 & 0 \\end{bmatrix}\n\\]\n\n\n\n\n\n\n\nb. Areas of parallelograms\n\n\nCompute the areas of the parallelograms whose sets of vertices contain the following points:\n\n\\((1,1)\\), \\((2,0)\\), \\((0,0)\\)\n\\((-1,1)\\), \\((1,2)\\), \\((0,0)\\)\n\\((1,0)\\), \\((-1,0)\\), \\((0,0)\\)\n\\((0,1)\\), \\((1,2)\\), \\((0,0)\\)"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#determinants-of-3times-3-matrices",
    "href": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#determinants-of-3times-3-matrices",
    "title": "02 Vectors and matrices, part 2",
    "section": "Determinants of \\(3\\times 3\\) matrices",
    "text": "Determinants of \\(3\\times 3\\) matrices\n\n\n\n\nDeterminants of \\(3\\times 3\\) matrices\n\n\nLet\n\\[\nA = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix}.\n\\]\nThe determinant of \\(A\\) is defined as the number\n\\[\n\\det(A) = a(ei - fh) - b(di - fg) + c(dh - eg).\n\\]\n\n\n\n\n\n\n\n\nTheorem: \\(3\\times 3\\) determinants and volume\n\n\nLet\n\\[\nA = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix}.\n\\]\nThe volume of the parallelepiped with sides given by the columns of \\(A\\) is \\(|\\det(A)|\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#exercise-7-determinants-and-volume",
    "href": "teaching/multi-calc-sp-26/slides/02-vectors-and-matrices-part-2.html#exercise-7-determinants-and-volume",
    "title": "02 Vectors and matrices, part 2",
    "section": "Exercise 7: Determinants and volume",
    "text": "Exercise 7: Determinants and volume\n\n\n\na. Computing \\(3\\times 3\\) determinants\n\n\nCompute the determinants of the following matrices.\n\n\\[\nA = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\n\\]\n\\[\nB = \\begin{bmatrix} 0 & -1 & 2 \\\\ 2 & 3 & 1 \\\\ 1 & 0 & -1 \\end{bmatrix}\n\\]\n\\[\nC = \\begin{bmatrix} 1 & 0 & 0 \\\\ -1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n\\]\n\n\n\n\n\n\n\nb. Volumes of parallelepipeds\n\n\nCompute the volumes of the parallelepipeds whose sets of vertices contain the following points:\n\n\\((1,1,1)\\), \\((2,0,0)\\), \\((1, -1, 2)\\), \\((0,0,0)\\)\n\\((0,1,0)\\), \\((1,2,1)\\), \\((2,0,1)\\), \\((0,0,0)\\)\n\\((1,0,0)\\), \\((-1,0,0)\\), \\((0,0,1)\\), \\((0,0,0)\\)"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#exercise-1-temperatures",
    "href": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#exercise-1-temperatures",
    "title": "04 Planes and linear spaces, part 2",
    "section": "Exercise 1: Temperatures",
    "text": "Exercise 1: Temperatures\n\n\n\nSuppose a \\(3\\)-dimensional coordinate system has been set up in the classroom. The temperature, \\(T\\), in degrees Fahrenheit, at a point in the classroom is a function of the coordinates \\((x,y,z)\\) of that point, \\(T = f(x,y,z)\\). Suppose that distances are measured in feet, and that\n\\[\nT = f(x,y,z) = 0.5x + 0.25y - 0.1z + 70.\n\\]\n\nWhat is the temperature at the point \\((0,0,0)\\)?\nWhat is the temperature at the point \\((2,4,10)\\)?\nFor every foot moved in the positive \\(x\\)-direction, how much does the temperature change?\nFor every foot moved in the positive \\(y\\)-direction, how much does the temperature change?\nFor every foot moved in the positive \\(z\\)-direction, how much does the temperature change?\nFor every foot moved in the negative \\(z\\)-direction, how much does the temperature change?"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#exercise-2-production-costs",
    "href": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#exercise-2-production-costs",
    "title": "04 Planes and linear spaces, part 2",
    "section": "Exercise 2: Production costs",
    "text": "Exercise 2: Production costs\n\n\n\nA factory produces custom furniture. The total cost to produce one item of furniture, \\(C\\), in dollars, depends on four variables:\n\n\\(x\\) = number of workers assigned\n\\(y\\) = hours of machine time used\n\\(z\\) = square feet of wood used\n\\(w\\) = gallons of finish/stain applied\n\nThe cost function is: \\[\nC = f(x,y,z,w) = 25x + 15y + 3z + 12w + 200.\n\\]\n\nWhat is the base cost (when \\(x=y=z=w=0\\))?\nWhat is the cost when \\(x=4\\) workers, \\(y=8\\) hours of machine time, \\(z=50\\) square feet of wood, and \\(w=2\\) gallons of finish are used?\nHow much does the cost increase for each additional worker?\nHow much does the cost increase for each additional square foot of wood?\nHow much does the cost increase for each additional gallon of finish?"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#what-do-these-exercises-have-in-common",
    "href": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#what-do-these-exercises-have-in-common",
    "title": "04 Planes and linear spaces, part 2",
    "section": "What do these exercises have in common?",
    "text": "What do these exercises have in common?\n\nThe function in Exercise 1 is of the form \\[\nw = m(x-x_0) + n(y-y_0) + p(z-z_0) + w_0.\n\\]\nThe function in Exercise 2 is of the form \\[\nu = m(x-x_0) + n(y-y_0) + p(z-z_0) + q(w-w_0) + u_0.\n\\]\nDo you see the pattern?!"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#point-slope-equations-of-hyperplanes",
    "href": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#point-slope-equations-of-hyperplanes",
    "title": "04 Planes and linear spaces, part 2",
    "section": "Point-slope equations of “hyperplanes”",
    "text": "Point-slope equations of “hyperplanes”\n\nIn two dimensions, we use the coordinates \\((x,y)\\).\nIn three dimensions, we use the coordinates \\((x,y,z)\\).\nIn four dimensions, we use the coordinates \\((x,y,z,w)\\).\nIn five dimensions, we use the coordinates \\((x,y,z,w, u)\\).\nIn \\(n\\) dimensions, we use the coordinates \\((x_1, x_2, \\ldots, x_n)\\).\n\n\n\n\n\nPoint-slope equation for a hyperplane in \\(\\mathbb{R}^4\\)\n\n\nA point-slope equation for a hyperplane through a point \\((x_0,y_0, z_0, w_0)\\) with\n\nslope \\(m\\) in the positive \\(x\\)-direction,\nslope \\(n\\) in the positive \\(y\\)-direction, and\nslope \\(p\\) in the positive \\(z\\)-direction\n\nis \\[\nw = m(x-x_0) + n(y-y_0) + p(z-z_0) + w_0.\n\\]\n\n\n\n\n\nUp one more dimension:\n\n\n\n\n\nPoint-slope equation for a hyperplane in \\(\\mathbb{R}^5\\)\n\n\nA point-slope equation for a hyperplane through a point \\((x_0,y_0, z_0, w_0, u_0)\\) with\n\nslope \\(m\\) in the positive \\(x\\)-direction,\nslope \\(n\\) in the positive \\(y\\)-direction,\nslope \\(p\\) in the positive \\(z\\)-direction, and\nslope \\(q\\) in the positive \\(w\\)-direction\n\nis \\[\nu = m(x-x_0) + n(y-y_0) + p(z-z_0) + q(w-w_0) + u_0.\n\\]\n\n\n\n\n\nSo, the graphs of the functions in Exercise 1 and 2 were hyperplanes in \\(\\mathbb{R}^4\\) and \\(\\mathbb{R}^5\\), respectively."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#the-full-story",
    "href": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#the-full-story",
    "title": "04 Planes and linear spaces, part 2",
    "section": "The full story",
    "text": "The full story\n\n\n\n\nPoint-slope equation for a hyperplane in \\(\\mathbb{R}^n\\)\n\n\nA point-slope equation for a hyperplane in \\(\\mathbb{R}^n\\) is \\[\nx_n = m_1(x_1-p_1) + m_2(x_2-p_2) + \\cdots + m_{n-1}(x_{n-1}-p_{n-1}) + p_{n},\n\\]\nwhere \\((p_1, p_2, \\ldots, p_{n})\\) is a point in \\(\\mathbb{R}^n\\) and \\(m_1, m_2, \\ldots, m_{n-1}\\) are the slopes in the positive \\(x_1\\), \\(x_2\\), \\(\\ldots\\), \\(x_{n-1}\\) directions.\n\n\n\n\n\nYou can’t see the graphs of these hyperplanes when \\(n&gt;3\\), but that doesn’t mean these are imaginary objects with no real-world applications.\nIf you doubt this, just look back at Exercises 1 and 2."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#vector-standard-and-affine-equations",
    "href": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#vector-standard-and-affine-equations",
    "title": "04 Planes and linear spaces, part 2",
    "section": "Vector, standard and affine equations",
    "text": "Vector, standard and affine equations\n\n\n\n\nVector equation for a hyperplane in \\(\\mathbb{R}^4\\)\n\n\nA vector equation for a hyperplane in \\(\\mathbb{R}^4\\) is \\[\n\\langle \\mathbf{n}, \\mathbf{r}-\\mathbf{r}_0 \\rangle = 0,\n\\]\nwhere \\(\\mathbf{n}\\) is a normal vector to the hyperplane, \\(\\mathbf{r}_0\\) is the position vector of a point on the hyperplane, and \\(\\mathbf{r}\\) is a variable position vector.\n\n\n\n\n\n\n\n\nStandard equation for a hyperplane in \\(\\mathbb{R}^4\\)\n\n\nA standard equation for a hyperplane in \\(\\mathbb{R}^4\\) is \\[\na(x-x_0) + b(y-y_0) + c(z-z_0) + d(w-w_0) = 0,\n\\]\nwhere \\((x_0,y_0,z_0,w_0)\\) is a point on the hyperplane and \\(a,b,c,d\\) are the components of a normal vector to the hyperplane.\n\n\n\n\n\n\n\n\nAffine equation for a hyperplane in \\(\\mathbb{R}^4\\)\n\n\nAn affine equation for a hyperplane in \\(\\mathbb{R}^4\\) is \\[\nax + by + cz + dw = e,\n\\]\nwhere:\n\n\\(a\\), \\(b\\), \\(c\\), \\(d\\), and \\(e\\) are constants,\nnot all of \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are \\(0\\), and\nthe numbers \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are the components of a normal vector to the hyperplane.\n\n\n\n\n\n\nCan you guess what these equations would look like in \\(\\mathbb{R}^5\\)? In \\(\\mathbb{R}^n\\)?"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#exercise-3-practice-with-equations-of-hyperplanes",
    "href": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#exercise-3-practice-with-equations-of-hyperplanes",
    "title": "04 Planes and linear spaces, part 2",
    "section": "Exercise 3: Practice with equations of hyperplanes",
    "text": "Exercise 3: Practice with equations of hyperplanes\n\n\n\na. Point-slope equation to standard equation\n\n\nConsider the hyperplane in \\(\\mathbb{R}^4\\) with point-slope equation \\[\nw = 2(x-1) - (y-2) + 3(z-3) + 4.\n\\]\nIdentify a point on the hyperplane. Write the equation in standard form and identify a normal vector to the hyperplane.\n\n\n\n\n\n\nb. Affine equation to standard equation\n\n\nConsider the hyperplane in \\(\\mathbb{R}^4\\) with affine equation \\[\n2x - y + 3z - 3w = 4.\n\\]\nIdentify a normal vector to the hyperplane. Write the equation in standard form and identify a point on the hyperplane."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#exercise-4-identifying-points-on-hyperplanes",
    "href": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#exercise-4-identifying-points-on-hyperplanes",
    "title": "04 Planes and linear spaces, part 2",
    "section": "Exercise 4: Identifying points on hyperplanes",
    "text": "Exercise 4: Identifying points on hyperplanes\n\n\n\nConsider the hyperplane in \\(\\mathbb{R}^5\\) with affine equation\n\\[\n3x - 2y + z + 4w - 5u = 7.\n\\]\n\nIdentify a normal vector to the hyperplane.\nIdentify a point on the hyperplane.\nWhere does the hyperplane intersect the \\(x\\)-axis?\nWhere does the hyperplane intersect the \\(y\\)-axis?"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#exercise-5-level-sets",
    "href": "teaching/multi-calc-sp-26/slides/04-planes-and-linear-spaces-part-2.html#exercise-5-level-sets",
    "title": "04 Planes and linear spaces, part 2",
    "section": "Exercise 5: Level sets",
    "text": "Exercise 5: Level sets\n\n\n\na. Level sets of linear functions of two variables\n\n\nConsider the linear function with point-slope equation \\[\nz = f(x,y) = 2x - 2y + 3.\n\\]\n\nIf we intersect the graph of \\(f\\) with the plane \\(z = 0\\), what sort of shape do we get?\nIf we intersect it with the plane \\(z = 1\\), what sort of shape do we get?\nWhat about the planes \\(z = -1\\) and \\(z = 2\\)?\n\nThese intersections are called level sets of the function.\n\n\n\n\n\n\nb. Level sets of linear functions of three variables\n\n\nConsider the linear function with point-slope equation\n\\[\nw = f(x,y,z) = 2x - y + 3z + 4.\n\\]\n\nIf we intersect the graph of \\(f\\) with the hyperplane \\(w = 0\\), what sort of shape do we get?\nIf we intersect it with the hyperplane \\(w = 1\\), what sort of shape do we get?\n\nThese intersections are called level sets of the function."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/08ex-the-definition-of-the-derivative.html",
    "href": "teaching/multi-calc-sp-26/exercises/08ex-the-definition-of-the-derivative.html",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall the definition of the derivative of a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\) at a point \\(\\mathbf{v}_0\\) given in class:\n\\[\nf'(\\mathbf{v}_0) \\mathbf{h} = \\lim_{\\lambda \\to 0} \\frac{f(\\mathbf{v}_0 + \\lambda\\mathbf{h}) - f(\\mathbf{v}_0)}{\\lambda},\n\\]\nfor all \\(\\mathbf{h}\\) in \\(\\mathbb{R}^n\\).\n\n\n\n\n\n\nUsing the above definition, compute the derivatives of the following functions.\n\n\\(f: \\mathbb{R}^2 \\to \\mathbb{R}\\) given by \\(f(x,y) = x^2 - y^2\\). Compute \\(f'(x,y)\\).\n\\(g: \\mathbb{R}^2 \\to \\mathbb{R}\\) given by \\(g(x,y) = 4-x^2-4y^2\\). Compute \\(g'(x,y)\\).\n\\(q: \\mathbb{R} \\to \\mathbb{R}^2\\) given by \\(q(t) = (t + 2t^2, -t^3)\\). Compute \\(q'(t)\\).\n\\(r: \\mathbb{R} \\to \\mathbb{R}^3\\) given by \\(r(t) = (e^t, \\sin{t}, \\cos{t})\\). Compute \\(r'(t)\\).\n\\(u:\\mathbb{R}^3 \\to \\mathbb{R}\\) given by \\(u(x,y,z) = x^2 + y^2 + z^2\\). Compute \\(u'(x,y,z)\\).\n\\(p: \\mathbb{R}^2 \\to \\mathbb{R}^3\\) given by \\(p(x,y) = (x^2, 3y, y^2)\\). Compute \\(p'(x,y)\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(f'(x,y) = \\begin{bmatrix} 2x & -2y \\end{bmatrix}\\).\n\\(g'(x,y) = \\begin{bmatrix} -2x & -8y \\end{bmatrix}\\).\n\\(q'(t) = \\begin{bmatrix} 1 + 4t \\\\ -3t^2 \\end{bmatrix}\\).\n\\(r'(t) = \\begin{bmatrix} e^t \\\\ \\cos{t} \\\\ -\\sin{t} \\end{bmatrix}\\).\n\\(u'(x,y,z) = \\begin{bmatrix} 2x & 2y & 2z \\end{bmatrix}\\).\n\\(p'(x,y) = \\begin{bmatrix} 2x & 0 \\\\ 0 & 3 \\\\ 0 & 2y \\end{bmatrix}\\).\n\n\n\n\n\n\n\nRecall that the tangent space to a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\) at a point \\(\\mathbf{v}_0\\) is the range of the tangent approximation function:\n\\[\nL_{\\mathbf{v}_0}(\\mathbf{v}) = f'(\\mathbf{v}_0)(\\mathbf{v} - \\mathbf{v}_0) + f(\\mathbf{v}_0).\n\\]\n\n\n\n\n\n\nThis exercise refers to the functions defined in Exercise 1.\n\nFor \\(f(x,y) = x^2 - y^2\\), compute the tangent approximation at the point \\((x_0,y_0)=(1,0)\\). Compute the tangent plane at this point as well, and check your answer using Desmos.\nFor \\(g(x,y) = 4-x^2-4y^2\\), compute the tangent approximation at the point \\((x_0,y_0)=(1,1)\\). Compute the tangent plane at this point as well, and check your answer using Desmos.\nFor \\(q(t) = (t + 2t^2, -t^3)\\), compute the tangent approximation at the point \\(t_0=1\\). Compute the tangent line at this point as well, and check your answer using Desmos.\nFor \\(r(t) = (e^t, \\sin{t}, \\cos{t})\\), compute the tangent approximation at the point \\(t_0=0\\). Compute the tangent line at this point as well, and check your answer using Desmos.\nFor \\(u(x,y,z) = x^2 + y^2 + z^2\\), compute the tangent approximation at the point \\((x_0,y_0,z_0)=(1,1,1)\\).\nFor \\(p(x,y) = (x^2, 3y, y^2)\\), compute the tangent approximation at the point \\((x_0,y_0)=(1,1)\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe tangent approximation is \\[\nL(x,y) = \\begin{bmatrix} 2 & 0 \\end{bmatrix} \\begin{bmatrix} x-1 \\\\ y \\end{bmatrix} + 1 = 2x - 1.\n\\] The tangent plane is here.\nThe tangent approximation is \\[\nL(x,y) = \\begin{bmatrix} -2 & -8 \\end{bmatrix} \\begin{bmatrix} x-1 \\\\ y-1 \\end{bmatrix} - 1 = -2x - 8y + 9.\n\\] The tangent plane is here.\nThe tangent approximation is \\[\nL(t) = \\begin{bmatrix} 5 \\\\ -3 \\end{bmatrix} (t-1) + \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 5t - 2 \\\\ -3t + 2 \\end{bmatrix}.\n\\] The tangent line is here.\nThe tangent approximation is \\[\nL(t) = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} t + \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} t + 1 \\\\ t \\\\ 1 \\end{bmatrix}.\n\\] The tangent line is here.\nThe tangent approximation is \\[\nL(x,y,z) = \\begin{bmatrix} 2 & 2 & 2 \\end{bmatrix} \\begin{bmatrix} x-1 \\\\ y-1 \\\\ z-1 \\end{bmatrix} + 3 = 2x + 2y + 2z - 3.\n\\]\nThe tangent approximation is \\[\nL(x,y) = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} x-1 \\\\ y-1 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 3 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2x - 1 \\\\ 3y \\\\ 2y - 1 \\end{bmatrix}.\n\\]\n\n\n\n\n\n\n\nFor this exercise, you’ll need to remember the definition of normal vectors to planes, and unit vectors. Go back and look them up!\n\n\n\n\n\n\n\nIn Exercise 2(a), you computed the tangent plane to the function \\(f\\) at the specified point. Compute an upward-pointing unit normal vector. (“Upward-pointing” means that the \\(z\\)-component of the normal vector should be positive.) Check your answer using Desmos.\nIn Exercise 2(b), you computed the tangent plane to the function \\(g\\) at the specified point. Compute an upward-pointing unit normal vector. Check your answer using Desmos.\nIn Exercise 2(c), you computed the tangent line to the function \\(q\\) at the specified points. Compute a positively-oriented unit tangent vector at the point. (“Tangent vector” means it lies in the tangent line, and “positively-oriented” means that the vector points in the direction that the curve is traced out as \\(t\\) increases.) Check your answer using Desmos.\nIn Exercise 2(d), you computed the tangent line to the function \\(r\\) at the specified points. Compute a positively-oriented unit tangent vector at the point. Check your answer using Desmos.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe tangent plane for \\(f\\) had equation \\(z = 2x-1\\). Since this is in point-slope form, as we learned in class, an upward-pointing normal vector is \\[\n\\begin{bmatrix} -2 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n\\] To make this a unit vector, we divide by its length: \\[\n\\mathbf{n} = \\frac{1}{\\sqrt{5}} \\begin{bmatrix} -2 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -\\frac{2}{\\sqrt{5}} \\\\ 0 \\\\ \\frac{1}{\\sqrt{5}} \\end{bmatrix}.\n\\] The graph is here.\nThe tangent plane for \\(g\\) had equation \\(z = -2x - 8y + 9\\). An upward-pointing normal vector is \\[\n\\begin{bmatrix} 2 \\\\ 8 \\\\ 1 \\end{bmatrix}.\n\\] To make this a unit vector, we divide by its length: \\[\n\\mathbf{n} = \\frac{1}{\\sqrt{69}} \\begin{bmatrix} 2 \\\\ 8 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{\\sqrt{69}} \\\\ \\frac{8}{\\sqrt{69}} \\\\ \\frac{1}{\\sqrt{69}} \\end{bmatrix}.\n\\] The graph is here.\nThe tangent line for \\(q\\) is the image of the function \\[\nL(t) = \\begin{bmatrix} 5 \\\\ -3 \\end{bmatrix} (t-1) + \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix}.\n\\] Thus, for all \\(t\\), the vector \\[\nL(t) - \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ -3 \\end{bmatrix} (t-1)\n\\] must lie in the tangent line. (Convince yourself of this by drawing a picture!) We may as well pick \\(t=2\\) to get a nonzero vector in the tangent line: \\[\n\\begin{bmatrix} 5 \\\\ -3 \\end{bmatrix}.\n\\] This vector points in the direction that the curve is traced out as \\(t\\) increases, so it is a positively-oriented tangent vector. To make it a unit vector, we divide by its length: \\[\n\\mathbf{t} = \\frac{1}{\\sqrt{34}} \\begin{bmatrix} 5 \\\\ -3 \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{\\sqrt{34}} \\\\ -\\frac{3}{\\sqrt{34}} \\end{bmatrix}.\n\\] The graph is here.\nThe tangent line for \\(r\\) is the image of the function \\[\nL(t) = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} t + \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n\\] Thus, for all \\(t\\), the vector \\[\nL(t) - \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} t\n\\] must lie in the tangent line. We may as well pick \\(t=1\\) to get a nonzero vector in the tangent line: \\[\n\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}.\n\\] This vector points in the direction that the curve is traced out as \\(t\\) increases, so it is a positively-oriented tangent vector. To make it a unit vector, we divide by its length: \\[\n\\mathbf{t} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{bmatrix}.\n\\] The graph is here.\n\n\n\n\n\n\n\nRecall from the class that the derivative of a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\) at a point \\(\\mathbf{v}_0\\) can be used to approximate changes in the value of \\(f\\) near \\(\\mathbf{v}_0\\). Specifically, for \\(\\mathbf{h}\\) small, we have \\[\nf(\\mathbf{v}_0 + \\mathbf{h}) - f(\\mathbf{v}_0) \\approx f'(\\mathbf{v}_0) \\mathbf{h}.\n\\]\n\n\n\n\n\n\nFor each of the six functions in Exercise 1, use the derivative to approximate the following changes in function values. Compare your approximations to the true values.\n\n\\(f: \\mathbb{R}^2 \\to \\mathbb{R}\\) given by \\(f(x,y) = x^2 - y^2\\). Compute \\(f'(x,y)\\).\n\\(g: \\mathbb{R}^2 \\to \\mathbb{R}\\) given by \\(g(x,y) = 4-x^2-4y^2\\). Compute \\(g'(x,y)\\).\n\\(q: \\mathbb{R} \\to \\mathbb{R}^2\\) given by \\(q(t) = (t + 2t^2, -t^3)\\). Compute \\(q'(t)\\).\n\\(r: \\mathbb{R} \\to \\mathbb{R}^3\\) given by \\(r(t) = (e^t, \\sin{t}, \\cos{t})\\). Compute \\(r'(t)\\).\n\\(u:\\mathbb{R}^3 \\to \\mathbb{R}\\) given by \\(u(x,y,z) = x^2 + y^2 + z^2\\). Compute \\(u'(x,y,z)\\).\n\\(p: \\mathbb{R}^2 \\to \\mathbb{R}^3\\) given by \\(p(x,y) = (x^2, 3y, y^2)\\). Compute \\(p'(x,y)\\).\n\n\n\n\\(f(1.2,0.98) - f(1.1,1.0)\\).\n\\(g(1.17,0.98) - g(1.15,1.01)\\).\n\\(q(1.24) - q(1.2)\\).\n\\(r(5.35) - r(5.26)\\).\n\\(u(4.2,0.32,18.5) - u(4.13,0.42,18.3)\\).\n\\(p(-21.2,0.34) - p(-21.8,0.33)\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nTrue value: \\(0.2696\\). Approximation: \\(0.26\\).\nTrue value: \\(0.1924\\). Approximation: \\(0.1964\\).\nTrue value: \\(\\begin{bmatrix} 0.2352 \\\\ -0.178624\\end{bmatrix}\\). Approximation: \\(\\begin{bmatrix} 0.232 \\\\ -0.1728 \\end{bmatrix}\\).\nTrue value: \\(\\begin{bmatrix}18.1268 \\\\ 0.0502506 \\\\ 0.0746285\\end{bmatrix}\\). Approximation: \\(\\begin{bmatrix} 17.3233 \\\\ 0.0468584 \\\\ 0.0768394 \\end{bmatrix}\\).\nTrue value: \\(7.8691\\). Approximation: \\(7.8142\\).\nTrue value: \\(\\begin{bmatrix}-25.8 \\\\ 0.03 \\\\ 0.0067\\end{bmatrix}\\). Approximation: \\(\\begin{bmatrix}-26.16 \\\\ 0.03 \\\\ 0.0066\\end{bmatrix}\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/08ex-the-definition-of-the-derivative.html#exercise-1-computing-derivatives-from-the-definition",
    "href": "teaching/multi-calc-sp-26/exercises/08ex-the-definition-of-the-derivative.html#exercise-1-computing-derivatives-from-the-definition",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall the definition of the derivative of a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\) at a point \\(\\mathbf{v}_0\\) given in class:\n\\[\nf'(\\mathbf{v}_0) \\mathbf{h} = \\lim_{\\lambda \\to 0} \\frac{f(\\mathbf{v}_0 + \\lambda\\mathbf{h}) - f(\\mathbf{v}_0)}{\\lambda},\n\\]\nfor all \\(\\mathbf{h}\\) in \\(\\mathbb{R}^n\\).\n\n\n\n\n\n\nUsing the above definition, compute the derivatives of the following functions.\n\n\\(f: \\mathbb{R}^2 \\to \\mathbb{R}\\) given by \\(f(x,y) = x^2 - y^2\\). Compute \\(f'(x,y)\\).\n\\(g: \\mathbb{R}^2 \\to \\mathbb{R}\\) given by \\(g(x,y) = 4-x^2-4y^2\\). Compute \\(g'(x,y)\\).\n\\(q: \\mathbb{R} \\to \\mathbb{R}^2\\) given by \\(q(t) = (t + 2t^2, -t^3)\\). Compute \\(q'(t)\\).\n\\(r: \\mathbb{R} \\to \\mathbb{R}^3\\) given by \\(r(t) = (e^t, \\sin{t}, \\cos{t})\\). Compute \\(r'(t)\\).\n\\(u:\\mathbb{R}^3 \\to \\mathbb{R}\\) given by \\(u(x,y,z) = x^2 + y^2 + z^2\\). Compute \\(u'(x,y,z)\\).\n\\(p: \\mathbb{R}^2 \\to \\mathbb{R}^3\\) given by \\(p(x,y) = (x^2, 3y, y^2)\\). Compute \\(p'(x,y)\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(f'(x,y) = \\begin{bmatrix} 2x & -2y \\end{bmatrix}\\).\n\\(g'(x,y) = \\begin{bmatrix} -2x & -8y \\end{bmatrix}\\).\n\\(q'(t) = \\begin{bmatrix} 1 + 4t \\\\ -3t^2 \\end{bmatrix}\\).\n\\(r'(t) = \\begin{bmatrix} e^t \\\\ \\cos{t} \\\\ -\\sin{t} \\end{bmatrix}\\).\n\\(u'(x,y,z) = \\begin{bmatrix} 2x & 2y & 2z \\end{bmatrix}\\).\n\\(p'(x,y) = \\begin{bmatrix} 2x & 0 \\\\ 0 & 3 \\\\ 0 & 2y \\end{bmatrix}\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/08ex-the-definition-of-the-derivative.html#exercise-2-computing-tangent-approximations-and-spaces",
    "href": "teaching/multi-calc-sp-26/exercises/08ex-the-definition-of-the-derivative.html#exercise-2-computing-tangent-approximations-and-spaces",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall that the tangent space to a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\) at a point \\(\\mathbf{v}_0\\) is the range of the tangent approximation function:\n\\[\nL_{\\mathbf{v}_0}(\\mathbf{v}) = f'(\\mathbf{v}_0)(\\mathbf{v} - \\mathbf{v}_0) + f(\\mathbf{v}_0).\n\\]\n\n\n\n\n\n\nThis exercise refers to the functions defined in Exercise 1.\n\nFor \\(f(x,y) = x^2 - y^2\\), compute the tangent approximation at the point \\((x_0,y_0)=(1,0)\\). Compute the tangent plane at this point as well, and check your answer using Desmos.\nFor \\(g(x,y) = 4-x^2-4y^2\\), compute the tangent approximation at the point \\((x_0,y_0)=(1,1)\\). Compute the tangent plane at this point as well, and check your answer using Desmos.\nFor \\(q(t) = (t + 2t^2, -t^3)\\), compute the tangent approximation at the point \\(t_0=1\\). Compute the tangent line at this point as well, and check your answer using Desmos.\nFor \\(r(t) = (e^t, \\sin{t}, \\cos{t})\\), compute the tangent approximation at the point \\(t_0=0\\). Compute the tangent line at this point as well, and check your answer using Desmos.\nFor \\(u(x,y,z) = x^2 + y^2 + z^2\\), compute the tangent approximation at the point \\((x_0,y_0,z_0)=(1,1,1)\\).\nFor \\(p(x,y) = (x^2, 3y, y^2)\\), compute the tangent approximation at the point \\((x_0,y_0)=(1,1)\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe tangent approximation is \\[\nL(x,y) = \\begin{bmatrix} 2 & 0 \\end{bmatrix} \\begin{bmatrix} x-1 \\\\ y \\end{bmatrix} + 1 = 2x - 1.\n\\] The tangent plane is here.\nThe tangent approximation is \\[\nL(x,y) = \\begin{bmatrix} -2 & -8 \\end{bmatrix} \\begin{bmatrix} x-1 \\\\ y-1 \\end{bmatrix} - 1 = -2x - 8y + 9.\n\\] The tangent plane is here.\nThe tangent approximation is \\[\nL(t) = \\begin{bmatrix} 5 \\\\ -3 \\end{bmatrix} (t-1) + \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 5t - 2 \\\\ -3t + 2 \\end{bmatrix}.\n\\] The tangent line is here.\nThe tangent approximation is \\[\nL(t) = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} t + \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} t + 1 \\\\ t \\\\ 1 \\end{bmatrix}.\n\\] The tangent line is here.\nThe tangent approximation is \\[\nL(x,y,z) = \\begin{bmatrix} 2 & 2 & 2 \\end{bmatrix} \\begin{bmatrix} x-1 \\\\ y-1 \\\\ z-1 \\end{bmatrix} + 3 = 2x + 2y + 2z - 3.\n\\]\nThe tangent approximation is \\[\nL(x,y) = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} x-1 \\\\ y-1 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 3 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2x - 1 \\\\ 3y \\\\ 2y - 1 \\end{bmatrix}.\n\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/08ex-the-definition-of-the-derivative.html#exercise-3-normal-and-unit-vectors",
    "href": "teaching/multi-calc-sp-26/exercises/08ex-the-definition-of-the-derivative.html#exercise-3-normal-and-unit-vectors",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "For this exercise, you’ll need to remember the definition of normal vectors to planes, and unit vectors. Go back and look them up!\n\n\n\n\n\n\n\nIn Exercise 2(a), you computed the tangent plane to the function \\(f\\) at the specified point. Compute an upward-pointing unit normal vector. (“Upward-pointing” means that the \\(z\\)-component of the normal vector should be positive.) Check your answer using Desmos.\nIn Exercise 2(b), you computed the tangent plane to the function \\(g\\) at the specified point. Compute an upward-pointing unit normal vector. Check your answer using Desmos.\nIn Exercise 2(c), you computed the tangent line to the function \\(q\\) at the specified points. Compute a positively-oriented unit tangent vector at the point. (“Tangent vector” means it lies in the tangent line, and “positively-oriented” means that the vector points in the direction that the curve is traced out as \\(t\\) increases.) Check your answer using Desmos.\nIn Exercise 2(d), you computed the tangent line to the function \\(r\\) at the specified points. Compute a positively-oriented unit tangent vector at the point. Check your answer using Desmos.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe tangent plane for \\(f\\) had equation \\(z = 2x-1\\). Since this is in point-slope form, as we learned in class, an upward-pointing normal vector is \\[\n\\begin{bmatrix} -2 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n\\] To make this a unit vector, we divide by its length: \\[\n\\mathbf{n} = \\frac{1}{\\sqrt{5}} \\begin{bmatrix} -2 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -\\frac{2}{\\sqrt{5}} \\\\ 0 \\\\ \\frac{1}{\\sqrt{5}} \\end{bmatrix}.\n\\] The graph is here.\nThe tangent plane for \\(g\\) had equation \\(z = -2x - 8y + 9\\). An upward-pointing normal vector is \\[\n\\begin{bmatrix} 2 \\\\ 8 \\\\ 1 \\end{bmatrix}.\n\\] To make this a unit vector, we divide by its length: \\[\n\\mathbf{n} = \\frac{1}{\\sqrt{69}} \\begin{bmatrix} 2 \\\\ 8 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{\\sqrt{69}} \\\\ \\frac{8}{\\sqrt{69}} \\\\ \\frac{1}{\\sqrt{69}} \\end{bmatrix}.\n\\] The graph is here.\nThe tangent line for \\(q\\) is the image of the function \\[\nL(t) = \\begin{bmatrix} 5 \\\\ -3 \\end{bmatrix} (t-1) + \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix}.\n\\] Thus, for all \\(t\\), the vector \\[\nL(t) - \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ -3 \\end{bmatrix} (t-1)\n\\] must lie in the tangent line. (Convince yourself of this by drawing a picture!) We may as well pick \\(t=2\\) to get a nonzero vector in the tangent line: \\[\n\\begin{bmatrix} 5 \\\\ -3 \\end{bmatrix}.\n\\] This vector points in the direction that the curve is traced out as \\(t\\) increases, so it is a positively-oriented tangent vector. To make it a unit vector, we divide by its length: \\[\n\\mathbf{t} = \\frac{1}{\\sqrt{34}} \\begin{bmatrix} 5 \\\\ -3 \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{\\sqrt{34}} \\\\ -\\frac{3}{\\sqrt{34}} \\end{bmatrix}.\n\\] The graph is here.\nThe tangent line for \\(r\\) is the image of the function \\[\nL(t) = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} t + \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n\\] Thus, for all \\(t\\), the vector \\[\nL(t) - \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} t\n\\] must lie in the tangent line. We may as well pick \\(t=1\\) to get a nonzero vector in the tangent line: \\[\n\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}.\n\\] This vector points in the direction that the curve is traced out as \\(t\\) increases, so it is a positively-oriented tangent vector. To make it a unit vector, we divide by its length: \\[\n\\mathbf{t} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{bmatrix}.\n\\] The graph is here."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/08ex-the-definition-of-the-derivative.html#exercise-4-using-derivatives-to-approximate-changes",
    "href": "teaching/multi-calc-sp-26/exercises/08ex-the-definition-of-the-derivative.html#exercise-4-using-derivatives-to-approximate-changes",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall from the class that the derivative of a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\) at a point \\(\\mathbf{v}_0\\) can be used to approximate changes in the value of \\(f\\) near \\(\\mathbf{v}_0\\). Specifically, for \\(\\mathbf{h}\\) small, we have \\[\nf(\\mathbf{v}_0 + \\mathbf{h}) - f(\\mathbf{v}_0) \\approx f'(\\mathbf{v}_0) \\mathbf{h}.\n\\]\n\n\n\n\n\n\nFor each of the six functions in Exercise 1, use the derivative to approximate the following changes in function values. Compare your approximations to the true values.\n\n\\(f: \\mathbb{R}^2 \\to \\mathbb{R}\\) given by \\(f(x,y) = x^2 - y^2\\). Compute \\(f'(x,y)\\).\n\\(g: \\mathbb{R}^2 \\to \\mathbb{R}\\) given by \\(g(x,y) = 4-x^2-4y^2\\). Compute \\(g'(x,y)\\).\n\\(q: \\mathbb{R} \\to \\mathbb{R}^2\\) given by \\(q(t) = (t + 2t^2, -t^3)\\). Compute \\(q'(t)\\).\n\\(r: \\mathbb{R} \\to \\mathbb{R}^3\\) given by \\(r(t) = (e^t, \\sin{t}, \\cos{t})\\). Compute \\(r'(t)\\).\n\\(u:\\mathbb{R}^3 \\to \\mathbb{R}\\) given by \\(u(x,y,z) = x^2 + y^2 + z^2\\). Compute \\(u'(x,y,z)\\).\n\\(p: \\mathbb{R}^2 \\to \\mathbb{R}^3\\) given by \\(p(x,y) = (x^2, 3y, y^2)\\). Compute \\(p'(x,y)\\).\n\n\n\n\\(f(1.2,0.98) - f(1.1,1.0)\\).\n\\(g(1.17,0.98) - g(1.15,1.01)\\).\n\\(q(1.24) - q(1.2)\\).\n\\(r(5.35) - r(5.26)\\).\n\\(u(4.2,0.32,18.5) - u(4.13,0.42,18.3)\\).\n\\(p(-21.2,0.34) - p(-21.8,0.33)\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nTrue value: \\(0.2696\\). Approximation: \\(0.26\\).\nTrue value: \\(0.1924\\). Approximation: \\(0.1964\\).\nTrue value: \\(\\begin{bmatrix} 0.2352 \\\\ -0.178624\\end{bmatrix}\\). Approximation: \\(\\begin{bmatrix} 0.232 \\\\ -0.1728 \\end{bmatrix}\\).\nTrue value: \\(\\begin{bmatrix}18.1268 \\\\ 0.0502506 \\\\ 0.0746285\\end{bmatrix}\\). Approximation: \\(\\begin{bmatrix} 17.3233 \\\\ 0.0468584 \\\\ 0.0768394 \\end{bmatrix}\\).\nTrue value: \\(7.8691\\). Approximation: \\(7.8142\\).\nTrue value: \\(\\begin{bmatrix}-25.8 \\\\ 0.03 \\\\ 0.0067\\end{bmatrix}\\). Approximation: \\(\\begin{bmatrix}-26.16 \\\\ 0.03 \\\\ 0.0066\\end{bmatrix}\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html",
    "href": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "A vector in \\(\\mathbb{R}^n\\) is an ordered list of \\(n\\) real numbers, which we typically write in boldface, such as \\(\\mathbf{v}\\). For example, when \\(n=2\\), we might have the \\(2\\)-dimensional vector\n\\[\n\\mathbf{v} = \\begin{bmatrix} 3 \\\\ -2 \\end{bmatrix},\n\\]\nwhich we visualize as an arrow with its tail at the origin and its head at the point \\((3, -2)\\). Technically, though, a vector can be moved and still be the same vector, just as long as it keeps its direction and length intact. So, while it is often convenient to draw vectors with their tails at the origin, they could have their tails anywhere and still be the same vector.\nSimilarly, when \\(n=3\\), a \\(3\\)-dimensional vector might look like\n\\[\n\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 4 \\\\ -3 \\end{bmatrix}.\n\\]\nVectors can be added, subtracted, and scaled (multiplied by real numbers) componentwise. For example, if\n\\[\n\\mathbf{u} = \\begin{bmatrix} 2 \\\\ 5 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{v} = \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix},\n\\]\nthen\n\\[\n\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} 2 + 3 \\\\ 5 + (-1) \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 4 \\end{bmatrix},\n\\]\n\\[\n\\mathbf{u} - \\mathbf{v} = \\begin{bmatrix} 2 - 3 \\\\ 5 - (-1) \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 6 \\end{bmatrix},\n\\]\nand for a scalar \\(c = 3\\),\n\\[\n3\\mathbf{u} = \\begin{bmatrix} 3 \\cdot 2 \\\\ 3 \\cdot 5 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 15 \\end{bmatrix}.\n\\]\n\n\n\n\n\n\nLet \\(\\mathbf{u} = \\begin{bmatrix} 4 \\\\ -1 \\end{bmatrix}\\), \\(\\mathbf{v} = \\begin{bmatrix} -2 \\\\ 3 \\end{bmatrix}\\), \\(\\mathbf{a} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix}\\), and \\(\\mathbf{b} = \\begin{bmatrix} 4 \\\\ 0 \\\\ -2 \\end{bmatrix}\\).\n\nCompute \\(\\mathbf{u} + \\mathbf{v}\\).\nCompute \\(\\mathbf{u} - \\mathbf{v}\\).\nCompute \\(2\\mathbf{u} + 3\\mathbf{v}\\).\nCompute \\(-\\mathbf{u}\\).\nCompute \\(\\mathbf{a} + \\mathbf{b}\\).\nCompute \\(\\mathbf{a} - 2\\mathbf{b}\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} 4 + (-2) \\\\ -1 + 3 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\\)\n\\(\\mathbf{u} - \\mathbf{v} = \\begin{bmatrix} 4 - (-2) \\\\ -1 - 3 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ -4 \\end{bmatrix}\\)\n\\(2\\mathbf{u} + 3\\mathbf{v} = 2\\begin{bmatrix} 4 \\\\ -1 \\end{bmatrix} + 3\\begin{bmatrix} -2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ -2 \\end{bmatrix} + \\begin{bmatrix} -6 \\\\ 9 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 7 \\end{bmatrix}\\)\n\\(-\\mathbf{u} = \\begin{bmatrix} -4 \\\\ 1 \\end{bmatrix}\\)\n\\(\\mathbf{a} + \\mathbf{b} = \\begin{bmatrix} 2 + 4 \\\\ -1 + 0 \\\\ 3 + (-2) \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ -1 \\\\ 1 \\end{bmatrix}\\)\n\\(\\mathbf{a} - 2\\mathbf{b} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix} - 2\\begin{bmatrix} 4 \\\\ 0 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 8 \\\\ 0 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix} -6 \\\\ -1 \\\\ 7 \\end{bmatrix}\\)\n\n\n\n\n\n\n\nThe dot product (also called the inner product or scalar product) of two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(\\mathbb{R}^n\\) is a real number obtained by multiplying corresponding components and summing the results. For example, if\n\\[\n\\mathbf{u} = \\begin{bmatrix} 2 \\\\ 3 \\\\ -1 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{v} = \\begin{bmatrix} 4 \\\\ 0 \\\\ 5 \\end{bmatrix},\n\\]\nthen the dot product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = (2)(4) + (3)(0) + (-1)(5) = 8 + 0 - 5 = 3.\n\\]\nWe also write this using angle bracket notation as\n\\[\n\\langle \\mathbf{u}, \\mathbf{v} \\rangle = 3.\n\\]\nThe length (or magnitude, or norm) of a vector \\(\\mathbf{v}\\) is defined as\n\\[\n\\|\\mathbf{v}\\| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}} = \\sqrt{\\langle \\mathbf{v}, \\mathbf{v} \\rangle}.\n\\]\nFor example, if \\(\\mathbf{u} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}\\), then\n\\[\n\\|\\mathbf{u}\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5.\n\\]\nThe dot product is closely related to the angle between two vectors. Specifically, if \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are two nonzero vectors in \\(\\mathbb{R}^n\\) and \\(\\theta\\) is the angle between them (with \\(0 \\leq \\theta \\leq \\pi\\)), then\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = \\|\\mathbf{u}\\| \\|\\mathbf{v}\\| \\cos{\\theta}.\n\\]\nRearranging, we get\n\\[\n\\cos{\\theta} = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}.\n\\]\nTwo vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are said to be orthogonal (or perpendicular) if their dot product is zero, i.e., \\(\\mathbf{u} \\cdot \\mathbf{v} = 0\\). This corresponds to the angle between them being \\(\\theta = \\frac{\\pi}{2}\\).\n\n\n\n\n\n\n\nLet \\(\\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix}\\). Compute \\(\\mathbf{u} \\cdot \\mathbf{v}\\).\nCompute \\(\\langle \\mathbf{u}, \\mathbf{u} \\rangle\\) for \\(\\mathbf{u} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix}\\).\nFind the length of the vector \\(\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix}\\).\nLet \\(\\mathbf{a} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\). Find the angle \\(\\theta\\) between \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\).\nDetermine whether the vectors \\(\\mathbf{p} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\) and \\(\\mathbf{q} = \\begin{bmatrix} 6 \\\\ -4 \\end{bmatrix}\\) are orthogonal.\nFind a vector in \\(\\mathbb{R}^2\\) that is orthogonal to \\(\\mathbf{w} = \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix}\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\mathbf{u} \\cdot \\mathbf{v} = (1)(3) + (2)(-4) = 3 - 8 = -5\\)\n\\(\\langle \\mathbf{u}, \\mathbf{u} \\rangle = 2^2 + (-1)^2 + 3^2 = 4 + 1 + 9 = 14\\)\n\\(\\|\\mathbf{v}\\| = \\sqrt{1^2 + 2^2 + 2^2} = \\sqrt{1 + 4 + 4} = \\sqrt{9} = 3\\)\nFirst, we compute the dot product and the magnitudes: \\[\n\\mathbf{a} \\cdot \\mathbf{b} = (1)(1) + (1)(0) = 1,\n\\] \\[\n\\|\\mathbf{a}\\| = \\sqrt{1^2 + 1^2} = \\sqrt{2},\n\\] \\[\n\\|\\mathbf{b}\\| = \\sqrt{1^2 + 0^2} = 1.\n\\] Therefore, \\[\n\\cos{\\theta} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|} = \\frac{1}{\\sqrt{2} \\cdot 1} = \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2}.\n\\] Thus, \\(\\theta = \\arccos\\left(\\frac{\\sqrt{2}}{2}\\right) = \\frac{\\pi}{4}\\).\nWe compute \\(\\mathbf{p} \\cdot \\mathbf{q} = (2)(6) + (3)(-4) = 12 - 12 = 0\\). Since the dot product is zero, the vectors are orthogonal.\nThere are infinitely many correct answers. One simple choice is \\(\\begin{bmatrix} -2 \\\\ 5 \\end{bmatrix}\\), since \\[\n\\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix} \\cdot \\begin{bmatrix} -2 \\\\ 5 \\end{bmatrix} = (5)(-2) + (2)(5) = -10 + 10 = 0.\n\\] Another choice is \\(\\begin{bmatrix} 2 \\\\ -5 \\end{bmatrix}\\).\n\n\n\n\n\n\n\nA matrix is a rectangular array of numbers arranged in rows and columns. For example, the matrix\n\\[\nA = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\n\\]\nhas 2 rows and 3 columns, so we say \\(A\\) is a \\(2 \\times 3\\) matrix. The entry in row \\(i\\) and column \\(j\\) is often denoted \\(a_{ij}\\).\nMatrices can be added and subtracted (if they have the same number of rows and columns) by adding or subtracting corresponding entries. For example,\n\\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} + \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 6 & 8 \\\\ 10 & 12 \\end{bmatrix}.\n\\]\nMatrices can also be multiplied by scalars by multiplying each entry:\n\\[\n3\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 3 & 6 \\\\ 9 & 12 \\end{bmatrix}.\n\\]\nMatrix multiplication is more intricate. If \\(A\\) is an \\(m \\times n\\) matrix and \\(B\\) is an \\(n \\times p\\) matrix, then their product \\(AB\\) is an \\(m \\times p\\) matrix. The entry in row \\(i\\) and column \\(j\\) of \\(AB\\) is obtained by taking the dot product of the \\(i\\)-th row of \\(A\\) with the \\(j\\)-th column of \\(B\\). For example,\n\\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} (1)(5) + (2)(7) & (1)(6) + (2)(8) \\\\ (3)(5) + (4)(7) & (3)(6) + (4)(8) \\end{bmatrix} = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}.\n\\]\nNote: Matrix multiplication is not commutative in general, meaning that \\(AB \\neq BA\\) in most cases.\n\n\n\n\n\n\n\nLet \\(A = \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix}\\) and \\(B = \\begin{bmatrix} 4 & 1 \\\\ -2 & 5 \\end{bmatrix}\\). Compute \\(A + B\\).\nCompute \\(2A - B\\).\nCompute the matrix product \\(AB\\).\nCompute the matrix product \\(BA\\).\nDoes \\(AB = BA\\) for these matrices? What does this tell you about matrix multiplication?\nLet \\(C = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\). Compute \\(AC\\) and \\(CA\\). What do you notice about the matrix \\(C\\)?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(A + B = \\begin{bmatrix} 2 + 4 & -1 + 1 \\\\ 0 + (-2) & 3 + 5 \\end{bmatrix} = \\begin{bmatrix} 6 & 0 \\\\ -2 & 8 \\end{bmatrix}\\)\n\\(2A - B = 2\\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix} - \\begin{bmatrix} 4 & 1 \\\\ -2 & 5 \\end{bmatrix} = \\begin{bmatrix} 4 & -2 \\\\ 0 & 6 \\end{bmatrix} - \\begin{bmatrix} 4 & 1 \\\\ -2 & 5 \\end{bmatrix} = \\begin{bmatrix} 0 & -3 \\\\ 2 & 1 \\end{bmatrix}\\)\n\\(AB = \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix} \\begin{bmatrix} 4 & 1 \\\\ -2 & 5 \\end{bmatrix} = \\begin{bmatrix} (2)(4) + (-1)(-2) & (2)(1) + (-1)(5) \\\\ (0)(4) + (3)(-2) & (0)(1) + (3)(5) \\end{bmatrix} = \\begin{bmatrix} 10 & -3 \\\\ -6 & 15 \\end{bmatrix}\\)\n\\(BA = \\begin{bmatrix} 4 & 1 \\\\ -2 & 5 \\end{bmatrix} \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix} = \\begin{bmatrix} (4)(2) + (1)(0) & (4)(-1) + (1)(3) \\\\ (-2)(2) + (5)(0) & (-2)(-1) + (5)(3) \\end{bmatrix} = \\begin{bmatrix} 8 & -1 \\\\ -4 & 17 \\end{bmatrix}\\)\nNo, \\(AB \\neq BA\\). This demonstrates that matrix multiplication is not commutative—the order of multiplication matters.\n\\(AC = \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix} = A\\)\n\\(CA = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix} = \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix} = A\\)\nThe matrix \\(C\\) is the identity matrix, which has the property that \\(CA = AC = A\\) for any matrix \\(A\\) (of compatible shape). It acts as the “multiplicative identity” for matrices, analogous to the number 1 for real numbers.\n\n\n\n\n\n\n\nFor a \\(2 \\times 2\\) matrix\n\\[\nA = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix},\n\\]\nthe determinant of \\(A\\) is defined as\n\\[\n\\det(A) = ad - bc.\n\\]\nThe determinant has many important interpretations and applications in mathematics. One of the most geometric interpretations is that \\(|\\det(A)|\\) (the absolute value of the determinant) represents the area of the parallelogram formed by the column vectors of \\(A\\).\nSpecifically, if we think of the matrix \\(A\\) above as having column vectors\n\\[\n\\mathbf{v}_1 = \\begin{bmatrix} a \\\\ c \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{v}_2 = \\begin{bmatrix} b \\\\ d \\end{bmatrix},\n\\]\nthen the parallelogram with sides \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) (with one vertex at the origin) has area \\(|\\det(A)|\\).\nFor example, the matrix\n\\[\nA = \\begin{bmatrix} 3 & 1 \\\\ 0 & 2 \\end{bmatrix}\n\\]\nhas determinant \\(\\det(A) = (3)(2) - (1)(0) = 6\\), so the parallelogram formed by the vectors \\(\\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}\\) and \\(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) has area 6.\nNote: If \\(\\det(A) = 0\\), the matrix is called singular, and geometrically this means the two column vectors are parallel (or one is the zero vector), so they don’t span a two-dimensional region.\n\n\n\n\n\n\n\nCompute the determinant of \\(A = \\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}\\).\nCompute the determinant of \\(B = \\begin{bmatrix} 5 & -2 \\\\ 3 & 1 \\end{bmatrix}\\).\nFind the area of the parallelogram formed by the vectors \\(\\mathbf{v}_1 = \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathbf{v}_2 = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\).\nDetermine whether the matrix \\(C = \\begin{bmatrix} 6 & 2 \\\\ 9 & 3 \\end{bmatrix}\\) is singular.\nFind a value of \\(k\\) such that the matrix \\(D = \\begin{bmatrix} k & 4 \\\\ 3 & 6 \\end{bmatrix}\\) is singular.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\det(A) = (2)(4) - (3)(1) = 8 - 3 = 5\\)\n\\(\\det(B) = (5)(1) - (-2)(3) = 5 + 6 = 11\\)\nThe area is the absolute value of the determinant of the matrix with columns \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\): \\[\n\\left|\\det\\begin{bmatrix} 4 & 2 \\\\ 1 & 3 \\end{bmatrix}\\right| = |(4)(3) - (2)(1)| = |12 - 2| = 10.\n\\]\n\\(\\det(C) = (6)(3) - (2)(9) = 18 - 18 = 0\\). Since the determinant is zero, the matrix is singular. (Note: The second column is exactly \\(\\frac{1}{3}\\) times the first column, so the column vectors are parallel.)\nFor \\(D\\) to be singular, we need \\(\\det(D) = 0\\): \\[\n(k)(6) - (4)(3) = 0\n\\] \\[\n6k - 12 = 0\n\\] \\[\nk = 2.\n\\]\n\n\n\n\n\n\n\nRecall from single-variable calculus that the derivative of a function \\(f\\) at a point \\(x = a\\) is defined as the limit\n\\[\nf'(a) = \\lim_{h \\to 0} \\frac{f(a + h) - f(a)}{h},\n\\]\nprovided this limit exists. This ratio \\(\\frac{f(a + h) - f(a)}{h}\\) is called a difference quotient, and it represents the slope of the secant line connecting the points \\((a, f(a))\\) and \\((a + h, f(a + h))\\) on the graph of \\(f\\). As \\(h\\) approaches zero, this secant line approaches the tangent line to the graph of \\(f\\) at the point \\((a, f(a))\\).\nThe tangent line at \\(x = a\\) has equation\n\\[\ny = f(a) + f'(a)(x - a).\n\\]\nThis is called the linear approximation (or tangent line approximation) to \\(f\\) at \\(x = a\\). For values of \\(x\\) close to \\(a\\), we have\n\\[\nf(x) \\approx f(a) + f'(a)(x - a).\n\\]\nThis approximation says that, near the point \\(x = a\\), the function \\(f\\) is approximately equal to a linear function with slope \\(f'(a)\\) passing through the point \\((a, f(a))\\). In fact, the tangent line is the best linear approximation to \\(f\\) near \\(x = a\\).\n\n\n\n\n\n\n\nLet \\(f(x) = x^2\\). Write out the difference quotient \\(\\frac{f(3 + h) - f(3)}{h}\\) explicitly (without taking the limit).\nSimplify your expression from part 1 as much as possible.\nNow compute \\(f'(3)\\) by taking the limit as \\(h \\to 0\\) of your simplified expression.\nUse the definition of the derivative as a limit to compute \\(f'(2)\\) for \\(f(x) = \\frac{1}{x}\\).\nFind the equation of the tangent line to \\(f(x) = x^3\\) at the point \\(x = 1\\).\nUse the linear approximation to estimate \\(f(1.1)\\) for \\(f(x) = x^3\\) at \\(x = 1\\).\nCompare your estimate from part 6 to the actual value \\(f(1.1) = (1.1)^3 = 1.331\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nWe have \\[\n\\frac{f(3 + h) - f(3)}{h} = \\frac{(3 + h)^2 - 3^2}{h} = \\frac{(3 + h)^2 - 9}{h}.\n\\]\nExpanding the numerator: \\[\n\\frac{(3 + h)^2 - 9}{h} = \\frac{9 + 6h + h^2 - 9}{h} = \\frac{6h + h^2}{h} = \\frac{h(6 + h)}{h} = 6 + h.\n\\]\nTaking the limit: \\[\nf'(3) = \\lim_{h \\to 0} (6 + h) = 6.\n\\]\nWe compute the difference quotient: \\[\n\\frac{f(2 + h) - f(2)}{h} = \\frac{\\frac{1}{2 + h} - \\frac{1}{2}}{h} = \\frac{\\frac{2 - (2 + h)}{2(2 + h)}}{h} = \\frac{-h}{2h(2 + h)} = \\frac{-1}{2(2 + h)}.\n\\] Taking the limit: \\[\nf'(2) = \\lim_{h \\to 0} \\frac{-1}{2(2 + h)} = \\frac{-1}{2(2)} = -\\frac{1}{4}.\n\\]\nFirst, we need \\(f(1) = 1^3 = 1\\) and \\(f'(1) = 3(1)^2 = 3\\). The tangent line has equation \\[\ny = f(1) + f'(1)(x - 1) = 1 + 3(x - 1) = 1 + 3x - 3 = 3x - 2.\n\\]\nUsing the linear approximation at \\(x = 1\\): \\[\nf(1.1) \\approx f(1) + f'(1)(1.1 - 1) = 1 + 3(0.1) = 1 + 0.3 = 1.3.\n\\]\nThe actual value is \\(f(1.1) = 1.331\\), so our linear approximation gave \\(1.3\\), which is quite close. The error is \\(|1.331 - 1.3| = 0.031\\).\n\n\n\n\n\n\n\nRecall from single-variable calculus that the definite integral of a function \\(f\\) over an interval \\([a, b]\\) is defined as\n\\[\n\\int_a^b f(x)\\, dx,\n\\]\nand it represents the signed area between the graph of \\(f\\) and the \\(x\\)-axis over the interval \\([a, b]\\). Here, “signed” means that:\n\nAreas above the \\(x\\)-axis (where \\(f(x) &gt; 0\\)) contribute positively to the integral.\nAreas below the \\(x\\)-axis (where \\(f(x) &lt; 0\\)) contribute negatively to the integral.\n\nThe Fundamental Theorem of Calculus states that if \\(F\\) is an antiderivative of \\(f\\) (meaning \\(F'(x) = f(x)\\)), then\n\\[\n\\int_a^b f(x)\\, dx = F(b) - F(a).\n\\]\nThis theorem connects the two main concepts of calculus: derivatives and integrals.\nFor example, if \\(f(x) = x\\) and we integrate from \\(x = 0\\) to \\(x = 2\\), we get\n\\[\n\\int_0^2 x\\, dx = \\left[\\frac{x^2}{2}\\right]_0^2 = \\frac{4}{2} - \\frac{0}{2} = 2.\n\\]\nGeometrically, this represents the area of a triangle with base 2 and height 2, which is \\(\\frac{1}{2}(2)(2) = 2\\).\n\n\n\n\n\n\n\nCompute \\(\\displaystyle\\int_1^3 2x\\, dx\\) using the Fundamental Theorem of Calculus.\nVerify your answer from part 1 by interpreting the integral as the area of a trapezoid.\nCompute \\(\\displaystyle\\int_0^\\pi \\sin{x}\\, dx\\).\nWhat is the geometric interpretation of your answer from part 3?\nCompute \\(\\displaystyle\\int_{-1}^1 x^3\\, dx\\).\nExplain why the answer to part 5 is zero using the concept of signed area.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nAn antiderivative of \\(f(x) = 2x\\) is \\(F(x) = x^2\\). Therefore, \\[\n\\int_1^3 2x\\, dx = F(3) - F(1) = 3^2 - 1^2 = 9 - 1 = 8.\n\\]\nThe region under the curve \\(y = 2x\\) from \\(x = 1\\) to \\(x = 3\\) is a trapezoid with parallel sides of heights \\(f(1) = 2\\) and \\(f(3) = 6\\), and base width \\(3 - 1 = 2\\). The area is \\[\n\\frac{1}{2}(2 + 6)(2) = \\frac{1}{2}(8)(2) = 8.\n\\] This matches our answer from part 1.\nAn antiderivative of \\(\\sin{x}\\) is \\(-\\cos{x}\\). Therefore, \\[\n\\int_0^\\pi \\sin{x}\\, dx = [-\\cos{x}]_0^\\pi = -\\cos{\\pi} - (-\\cos{0}) = -(-1) - (-1) = 1 + 1 = 2.\n\\]\nThis represents the total area between the sine curve and the \\(x\\)-axis from \\(x = 0\\) to \\(x = \\pi\\). Since \\(\\sin{x} &gt; 0\\) for all \\(x \\in (0, \\pi)\\), the entire region is above the \\(x\\)-axis, so the signed area equals the actual geometric area.\nAn antiderivative of \\(x^3\\) is \\(\\frac{x^4}{4}\\). Therefore, \\[\n\\int_{-1}^1 x^3\\, dx = \\left[\\frac{x^4}{4}\\right]_{-1}^1 = \\frac{1^4}{4} - \\frac{(-1)^4}{4} = \\frac{1}{4} - \\frac{1}{4} = 0.\n\\]\nThe function \\(f(x) = x^3\\) is an odd function, meaning \\(f(-x) = -f(x)\\). This means the graph is symmetric about the origin. For \\(x \\in (0, 1)\\), we have \\(f(x) &gt; 0\\), contributing positive area. For \\(x \\in (-1, 0)\\), we have \\(f(x) &lt; 0\\), contributing negative area. Because of the symmetry, these two areas are equal in magnitude but opposite in sign, so they cancel out, giving a total signed area of zero.\n\n\n\n\n\n\n\nIn this problem, we will work with rectangular boxes in \\(\\mathbb{R}^3\\). We assume that all boxes have eight vertices (i.e., corners), and that their edges are aligned with the coordinate axes. In other words, every edge of a box is parallel to either the \\(x\\)-, \\(y\\)-, or \\(z\\)-axis.\n\n\n\n\n\n\n\nA rectangular box has two opposite corners at \\(P = (0, 0, 0)\\) and \\(Q = (3, 2, 4)\\). Find the coordinates of all eight vertices of the box.\nA rectangular box has three vertices at \\(A = (1, 1, 1)\\), \\(B = (5, 1, 1)\\), and \\(C = (1, 3, 4)\\). Find the coordinates of all eight vertices of the box.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe four vertices of the bottom of the box are at \\[\n(0,0,0), \\quad (3,0,0), \\quad (0,2,0), \\quad (3,2,0).\n\\] The four vertices of the top of the box are at \\[\n(0,0,4), \\quad (3,0,4), \\quad (0,2,4), \\quad (3,2,4).\n\\]\nThe four vertices of the bottom of the box are at \\[\n(1, 1, 1), \\quad (5, 1, 1), \\quad (5, 3, 1), \\quad (1, 3, 1).\n\\] The four vertices of the top of the box are at \\[\n(1, 1, 4), \\quad (5, 1, 4), \\quad (5, 3, 4), \\quad (1, 3, 4).\n\\]\n\n\n\n\n\n\n\nRecall that in \\(\\mathbb{R}^3\\), there are three coordinate planes:\n\nThe \\(xy\\)-plane, where \\(z = 0\\).\nThe \\(xz\\)-plane, where \\(y = 0\\).\nThe \\(yz\\)-plane, where \\(x = 0\\).\n\nOther planes that are parallel with one of these coordinate planes can be described by equations of the form:\n\n\\(x = k\\) for planes parallel to the \\(yz\\)-plane,\n\\(y = k\\) for planes parallel to the \\(xz\\)-plane,\n\\(z = k\\) for planes parallel to the \\(xy\\)-plane,\n\nwhere \\(k\\) is a constant.\nSimilarly, the three coordinate axes are:\n\nThe \\(x\\)-axis, where \\(y = 0\\) and \\(z = 0\\).\nThe \\(y\\)-axis, where \\(x = 0\\) and \\(z = 0\\).\nThe \\(z\\)-axis, where \\(x = 0\\) and \\(y = 0\\).\n\nOther lines that are parallel with one of these coordinate axes can be described by pairs of equations of the form:\n\n\\(y= k_1\\) and \\(z = k_2\\) for lines parallel to the \\(x\\)-axis,\n\\(x = k_1\\) and \\(z = k_2\\) for lines parallel to the \\(y\\)-axis,\n\\(x = k_1\\) and \\(y = k_2\\) for lines parallel to the \\(z\\)-axis,\n\nwhere \\(k_1\\) and \\(k_2\\) are constants.\n\n\n\n\n\n\n\nYou are standing at a point in the \\(yz\\)-plane and you walk \\(3\\) units in the negative \\(x\\)-direction to arrive at the point \\((-3, 1, 2)\\). What point were you at to begin with?\nYou start at a point on the \\(xy\\)-plane and walk \\(4\\) units in the positive \\(z\\)-direction to arrive at \\((2, -1, 4)\\). Find your starting point.\nYou begin at a point on the \\(x\\)-axis and walk \\(2\\) units in the positive \\(y\\)-direction and \\(3\\) units in the negative \\(z\\)-direction to arrive at \\((5, 2, -3)\\). Where did you start?\nYou start at the point \\((1, 4, 2)\\) and walk \\(5\\) units in the negative \\(y\\)-direction. At what point do you arrive?\nStarting from the origin, you walk \\(3\\) units in the positive \\(x\\)-direction, then \\(2\\) units in the negative \\(y\\)-direction, and finally \\(4\\) units in the positive \\(z\\)-direction. What is your final position?\nYou begin at a point on the plane \\(x=2\\) and walk \\(4\\) units in the negative \\(x\\)-direction, \\(2\\) units in the positive \\(y\\)-direction, and \\(3\\) units in the negative \\(z\\)-direction to arrive at \\((-2, 4, -2)\\). What was your starting point?\nYou begin at a point on the line defined by \\(y=3\\) and \\(z=-1\\). You walk \\(1\\) unit in the negative \\(y\\)-direction, then \\(4\\) units in the negative \\(x\\)-direction, and finally \\(6\\) units in the positive \\(z\\)-direction to arrive at \\((-1, 2, 5)\\). Where did you start?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\((0, 1, 2)\\)\n\\((2, -1, 0)\\)\n\\((5, 0, 0)\\)\n\\((1, -1, 2)\\)\n\\((3, -2, 4)\\)\n\\((2, 2, 1)\\)\n\\((3, 3, -1)\\)\n\n\n\n\n\n\n\nRecall from class that the standard basis vectors in \\(\\mathbb{R}^2\\) are\n\\[\n\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}.\n\\]\nSimilarly, in \\(\\mathbb{R}^3\\), the standard basis vectors are\n\\[\n\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n\\]\nWe saw that any vector in \\(\\mathbb{R}^n\\) can be written as a linear combination of the standard basis vectors. For example, in \\(\\mathbb{R}^3\\),\n\\[\n\\mathbf{v} = \\begin{bmatrix} 2 \\\\ -3 \\\\ 5 \\end{bmatrix} = 2\\mathbf{e}_1 - 3\\mathbf{e}_2 + 5\\mathbf{e}_3.\n\\]\nOne important property of the standard basis vectors is that they are orthonormal, meaning:\n\nThey are all unit vectors: \\(\\|\\mathbf{e}_i\\| = 1\\) for all \\(i\\).\nThey are mutually orthogonal: \\(\\mathbf{e}_i \\cdot \\mathbf{e}_j = 0\\) whenever \\(i \\neq j\\).\n\n\n\n\n\n\n\n\nWrite the vector \\(\\mathbf{v} = \\begin{bmatrix} 4 \\\\ -2 \\\\ 7 \\end{bmatrix}\\) as a linear combination of the standard basis vectors \\(\\mathbf{e}_1\\), \\(\\mathbf{e}_2\\), and \\(\\mathbf{e}_3\\).\nLet \\(\\mathbf{w} = 3\\mathbf{e}_1 - 5\\mathbf{e}_2 + 2\\mathbf{e}_3\\). Write \\(\\mathbf{w}\\) in column vector form.\nVerify that the standard basis vectors in \\(\\mathbb{R}^3\\) are orthonormal by computing all pairwise dot products and all norms.\nLet \\(\\mathbf{u} = \\begin{bmatrix} -1 \\\\ 6 \\\\ 2 \\end{bmatrix}\\). Compute \\(\\mathbf{u} \\cdot \\mathbf{e}_1\\), \\(\\mathbf{u} \\cdot \\mathbf{e}_2\\), and \\(\\mathbf{u} \\cdot \\mathbf{e}_3\\). What do you notice?\nExpress the zero vector \\(\\mathbf{0}\\) in \\(\\mathbb{R}^3\\) as a linear combination of the standard basis vectors.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\mathbf{v} = 4\\mathbf{e}_1 - 2\\mathbf{e}_2 + 7\\mathbf{e}_3\\)\n\\(\\mathbf{w} = \\begin{bmatrix} 3 \\\\ -5 \\\\ 2 \\end{bmatrix}\\)\nComputing the norms: \\[\n\\|\\mathbf{e}_1\\| = \\sqrt{1^2 + 0^2 + 0^2} = 1, \\quad \\|\\mathbf{e}_2\\| = \\sqrt{0^2 + 1^2 + 0^2} = 1, \\quad \\|\\mathbf{e}_3\\| = \\sqrt{0^2 + 0^2 + 1^2} = 1.\n\\] Computing the pairwise dot products: \\[\n\\mathbf{e}_1 \\cdot \\mathbf{e}_2 = (1)(0) + (0)(1) + (0)(0) = 0,\n\\] \\[\n\\mathbf{e}_1 \\cdot \\mathbf{e}_3 = (1)(0) + (0)(0) + (0)(1) = 0,\n\\] \\[\n\\mathbf{e}_2 \\cdot \\mathbf{e}_3 = (0)(0) + (1)(0) + (0)(1) = 0.\n\\] Since all the norms are 1 and all pairwise dot products are 0, the vectors are orthonormal.\n\\(\\mathbf{u} \\cdot \\mathbf{e}_1 = (-1)(1) + (6)(0) + (2)(0) = -1\\)\n\\(\\mathbf{u} \\cdot \\mathbf{e}_2 = (-1)(0) + (6)(1) + (2)(0) = 6\\)\n\\(\\mathbf{u} \\cdot \\mathbf{e}_3 = (-1)(0) + (6)(0) + (2)(1) = 2\\)\nThe dot products give exactly the components of \\(\\mathbf{u}\\)! This shows that \\(\\mathbf{u} = (\\mathbf{u} \\cdot \\mathbf{e}_1)\\mathbf{e}_1 + (\\mathbf{u} \\cdot \\mathbf{e}_2)\\mathbf{e}_2 + (\\mathbf{u} \\cdot \\mathbf{e}_3)\\mathbf{e}_3\\).\n\\(\\mathbf{0} = 0\\mathbf{e}_1 + 0\\mathbf{e}_2 + 0\\mathbf{e}_3\\)\n\n\n\n\n\n\n\nRecall that a unit vector is a vector with length (norm) equal to 1. Given any nonzero vector \\(\\mathbf{v}\\), we can always construct a unit vector in the same direction, often denoted \\(\\mathbf{\\hat{v}}\\), by dividing \\(\\mathbf{v}\\) by its length:\n\\[\n\\mathbf{\\hat{v}} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}.\n\\]\nThis process is called normalization of \\(\\mathbf{v}\\).\n\n\n\n\n\n\n\nFind the length of the vector \\(\\mathbf{v} = \\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix}\\).\nFind a unit vector pointing in the same direction as \\(\\mathbf{v} = \\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix}\\).\nFind a vector with length \\(10\\) pointing in the same direction as \\(\\mathbf{v} = \\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix}\\).\nFind a vector with length \\(2\\) pointing in the same direction as \\(\\mathbf{u} = \\mathbf{e}_1 - \\mathbf{e}_2 + 2\\mathbf{e}_3\\).\nLet \\(\\mathbf{a} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\). Find a unit vector in the direction of \\(\\mathbf{a}\\).\nFind a vector of length \\(5\\) pointing in the opposite direction of \\(\\mathbf{b} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 2 \\end{bmatrix}\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\|\\mathbf{v}\\| = \\sqrt{3^2 + (-4)^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\\)\n\\(\\mathbf{\\hat{v}} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|} = \\frac{1}{5}\\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix} 3/5 \\\\ -4/5 \\end{bmatrix}\\)\nThe vector is \\(10 \\cdot \\mathbf{\\hat{v}} = 10 \\cdot \\frac{1}{5}\\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix} = 2\\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ -8 \\end{bmatrix}\\)\nFirst, we write \\(\\mathbf{u}\\) in column form: \\(\\mathbf{u} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}\\). Then we compute its length: \\[\n\\|\\mathbf{u}\\| = \\sqrt{1^2 + (-1)^2 + 2^2} = \\sqrt{1 + 1 + 4} = \\sqrt{6}.\n\\] The desired vector is \\[\n2 \\cdot \\frac{\\mathbf{u}}{\\|\\mathbf{u}\\|} = \\frac{2}{\\sqrt{6}}\\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 2/\\sqrt{6} \\\\ -2/\\sqrt{6} \\\\ 4/\\sqrt{6} \\end{bmatrix}.\n\\]\n\\(\\|\\mathbf{a}\\| = \\sqrt{1^2 + 1^2 + 1^2} = \\sqrt{3}\\). The unit vector is \\[\n\\mathbf{\\hat{a}} = \\frac{1}{\\sqrt{3}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1/\\sqrt{3} \\\\ 1/\\sqrt{3} \\\\ 1/\\sqrt{3} \\end{bmatrix} = \\begin{bmatrix} \\sqrt{3}/3 \\\\ \\sqrt{3}/3 \\\\ \\sqrt{3}/3 \\end{bmatrix}.\n\\]\nFirst, \\(\\|\\mathbf{b}\\| = \\sqrt{2^2 + (-1)^2 + 2^2} = \\sqrt{4 + 1 + 4} = \\sqrt{9} = 3\\). To point in the opposite direction, we need a negative scalar: \\[\n-5 \\cdot \\frac{\\mathbf{b}}{\\|\\mathbf{b}\\|} = -\\frac{5}{3}\\begin{bmatrix} 2 \\\\ -1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} -10/3 \\\\ 5/3 \\\\ -10/3 \\end{bmatrix}.\n\\]\n\n\n\n\n\n\n\nTwo nonzero vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are said to be parallel if one is a scalar multiple of the other. That is, \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are parallel if there exists a scalar \\(c\\) such that \\(\\mathbf{u} = c\\mathbf{v}\\) (or \\(\\mathbf{v} = c\\mathbf{u}\\), it doesn’t matter which way you write it).\nGeometrically, parallel vectors point in the same direction (if \\(c &gt; 0\\)) or in opposite directions (if \\(c &lt; 0\\)).\n\n\n\n\n\n\n\nDetermine whether the vectors \\(\\mathbf{u} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}\\) are parallel.\nFind a vector parallel to \\(\\mathbf{w} = \\begin{bmatrix} 1 \\\\ -3 \\\\ 2 \\end{bmatrix}\\) with length \\(7\\).\nFind the values of \\(a\\) that make \\(\\mathbf{v} = 5a\\mathbf{e}_1 - 3\\mathbf{e}_2\\) parallel to \\(\\mathbf{w} = a^2\\mathbf{e}_1 + 6\\mathbf{e}_2\\).\nLet \\(\\mathbf{p} = \\begin{bmatrix} 4 \\\\ -2 \\\\ 6 \\end{bmatrix}\\) and \\(\\mathbf{q} = \\begin{bmatrix} -6 \\\\ 3 \\\\ k \\end{bmatrix}\\). Find the value of \\(k\\) such that \\(\\mathbf{p}\\) and \\(\\mathbf{q}\\) are parallel.\nExplain why the zero vector \\(\\mathbf{0}\\) is considered parallel to every vector.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nWe check if \\(\\mathbf{v} = c\\mathbf{u}\\) for some scalar \\(c\\): \\[\n\\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix} = c\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 2c \\\\ 4c \\end{bmatrix}.\n\\] From the first component, \\(3 = 2c\\), so \\(c = 3/2\\). From the second component, \\(6 = 4c\\), so \\(c = 6/4 = 3/2\\). Since both components give the same value of \\(c\\), the vectors are parallel.\nFirst, we need the length of \\(\\mathbf{w}\\): \\(\\|\\mathbf{w}\\| = \\sqrt{1^2 + (-3)^2 + 2^2} = \\sqrt{1 + 9 + 4} = \\sqrt{14}\\). A parallel vector with length \\(7\\) is \\[\n7 \\cdot \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|} = \\frac{7}{\\sqrt{14}}\\begin{bmatrix} 1 \\\\ -3 \\\\ 2 \\end{bmatrix}.\n\\]\nWriting in column form: \\(\\mathbf{v} = \\begin{bmatrix} 5a \\\\ -3 \\end{bmatrix}\\) and \\(\\mathbf{w} = \\begin{bmatrix} a^2 \\\\ 6 \\end{bmatrix}\\). For these to be parallel, we need \\(\\mathbf{v} = c\\mathbf{w}\\) for some scalar \\(c\\): \\[\n\\begin{bmatrix} 5a \\\\ -3 \\end{bmatrix} = c\\begin{bmatrix} a^2 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} ca^2 \\\\ 6c \\end{bmatrix}.\n\\] From the first component: \\(5a = ca^2\\), so \\(c = 5a/a^2 = 5/a\\) (assuming \\(a \\neq 0\\)).\nFrom the second component: \\(-3 = 6c\\), so \\(c = -1/2\\).\nSetting these equal: \\(5/a = -1/2\\), which gives \\(a = -10\\).\nWe should also check the case \\(a = 0\\): if \\(a = 0\\), then \\(\\mathbf{v} = \\begin{bmatrix} 0 \\\\ -3 \\end{bmatrix}\\) and \\(\\mathbf{w} = \\begin{bmatrix} 0 \\\\ 6 \\end{bmatrix}\\), which are parallel (one is \\(-1/2\\) times the other). So \\(a = 0\\) and \\(a = -10\\) both work.\nFor \\(\\mathbf{p}\\) and \\(\\mathbf{q}\\) to be parallel, we need \\(\\mathbf{q} = c\\mathbf{p}\\) for some scalar \\(c\\): \\[\n\\begin{bmatrix} -6 \\\\ 3 \\\\ k \\end{bmatrix} = c\\begin{bmatrix} 4 \\\\ -2 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 4c \\\\ -2c \\\\ 6c \\end{bmatrix}.\n\\] From the first component: \\(-6 = 4c\\), so \\(c = -3/2\\).\nFrom the second component: \\(3 = -2c\\), so \\(c = -3/2\\). (Consistent!)\nFrom the third component: \\(k = 6c = 6(-3/2) = -9\\).\nThe zero vector can be written as \\(\\mathbf{0} = 0 \\cdot \\mathbf{v}\\) for any vector \\(\\mathbf{v}\\). Since \\(\\mathbf{0}\\) is always a scalar multiple of any vector (with scalar \\(c = 0\\)), it is parallel to every vector by definition.\n\n\n\n\n\n\n\nGiven two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) (with \\(\\mathbf{v} \\neq \\mathbf{0}\\)), we can decompose \\(\\mathbf{u}\\) into two parts:\n\nA component parallel to \\(\\mathbf{v}\\), called the projection of \\(\\mathbf{u}\\) onto \\(\\mathbf{v}\\)\nA component perpendicular (orthogonal) to \\(\\mathbf{v}\\)\n\nThe projection of \\(\\mathbf{u}\\) onto \\(\\mathbf{v}\\) is denoted \\(\\text{proj}_{\\mathbf{v}}\\mathbf{u}\\) and is defined as\n\\[\n\\text{proj}_{\\mathbf{v}}\\mathbf{u} = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\mathbf{v} \\cdot \\mathbf{v}} \\mathbf{v} = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{v}\\|^2} \\mathbf{v}.\n\\]\nThe component of \\(\\mathbf{u}\\) perpendicular to \\(\\mathbf{v}\\) is the vector\n\\[\n\\mathbf{u}_{\\perp} = \\mathbf{u} - \\text{proj}_{\\mathbf{v}}\\mathbf{u}.\n\\]\nThis gives us the decomposition: \\(\\mathbf{u} = \\text{proj}_{\\mathbf{v}}\\mathbf{u} + \\mathbf{u}_{\\perp}\\).\n\n\n\n\n\n\n\nLet \\(\\mathbf{u} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\). Compute the projection of \\(\\mathbf{u}\\) onto \\(\\mathbf{v}\\).\nFor the same vectors as in part a, find the component of \\(\\mathbf{u}\\) perpendicular to \\(\\mathbf{v}\\).\nVerify that \\(\\mathbf{u} = \\text{proj}_{\\mathbf{v}}\\mathbf{u} + \\mathbf{u}_{\\perp}\\) for the vectors in parts a and b.\nLet \\(\\mathbf{u} = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\). Compute \\(\\text{proj}_{\\mathbf{v}}\\mathbf{u}\\) and \\(\\mathbf{u}_{\\perp}\\). Then draw all four vectors (\\(\\mathbf{u}\\), \\(\\mathbf{v}\\), \\(\\text{proj}_{\\mathbf{v}}\\mathbf{u}\\), and \\(\\mathbf{u}_{\\perp}\\)) in the plane, and verify geometrically that \\(\\mathbf{u} = \\text{proj}_{\\mathbf{v}}\\mathbf{u} + \\mathbf{u}_{\\perp}\\).\nLet \\(\\mathbf{a} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\end{bmatrix}\\) and \\(\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\). Find \\(\\text{proj}_{\\mathbf{b}}\\mathbf{a}\\).\nFor the vectors in part e, verify that \\(\\mathbf{u}_{\\perp} = \\mathbf{a} - \\text{proj}_{\\mathbf{b}}\\mathbf{a}\\) is orthogonal to \\(\\mathbf{b}\\).\nCompute \\(\\text{proj}_{\\mathbf{e}_1}\\mathbf{u}\\) for an arbitrary vector \\(\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\end{bmatrix}\\). What do you notice?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nWe compute: \\[\n\\mathbf{u} \\cdot \\mathbf{v} = (3)(1) + (4)(0) = 3,\n\\] \\[\n\\mathbf{v} \\cdot \\mathbf{v} = 1^2 + 0^2 = 1.\n\\] Therefore, \\[\n\\text{proj}_{\\mathbf{v}}\\mathbf{u} = \\frac{3}{1}\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}.\n\\]\n\\(\\mathbf{u}_{\\perp} = \\mathbf{u} - \\text{proj}_{\\mathbf{v}}\\mathbf{u} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} - \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 4 \\end{bmatrix}\\)\n\\(\\text{proj}_{\\mathbf{v}}\\mathbf{u} + \\mathbf{u}_{\\perp} = \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} = \\mathbf{u}\\)\nWe compute: \\[\n\\mathbf{u} \\cdot \\mathbf{v} = (3)(2) + (1)(1) = 6 + 1 = 7,\n\\] \\[\n\\mathbf{v} \\cdot \\mathbf{v} = 2^2 + 1^2 = 4 + 1 = 5.\n\\] Therefore, \\[\n\\text{proj}_{\\mathbf{v}}\\mathbf{u} = \\frac{7}{5}\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 14/5 \\\\ 7/5 \\end{bmatrix}.\n\\] And \\[\n\\mathbf{u}_{\\perp} = \\mathbf{u} - \\text{proj}_{\\mathbf{v}}\\mathbf{u} = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 14/5 \\\\ 7/5 \\end{bmatrix} = \\begin{bmatrix} 15/5 - 14/5 \\\\ 5/5 - 7/5 \\end{bmatrix} = \\begin{bmatrix} 1/5 \\\\ -2/5 \\end{bmatrix}.\n\\]\nIn a diagram, you should see that:\n\n\\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) both start at the origin\n\\(\\text{proj}_{\\mathbf{v}}\\mathbf{u}\\) lies along the line defined by \\(\\mathbf{v}\\)\n\\(\\mathbf{u}_{\\perp}\\) is perpendicular to \\(\\mathbf{v}\\)\nWhen you add \\(\\text{proj}_{\\mathbf{v}}\\mathbf{u}\\) and \\(\\mathbf{u}_{\\perp}\\) using the “tip-to-tail” rule, you get \\(\\mathbf{u}\\)\n\nWe compute: \\[\n\\mathbf{a} \\cdot \\mathbf{b} = (2)(1) + (1)(1) + (3)(1) = 2 + 1 + 3 = 6,\n\\] \\[\n\\mathbf{b} \\cdot \\mathbf{b} = 1^2 + 1^2 + 1^2 = 3.\n\\] Therefore, \\[\n\\text{proj}_{\\mathbf{b}}\\mathbf{a} = \\frac{6}{3}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = 2\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix}.\n\\]\nFirst, we compute: \\[\n\\mathbf{u}_{\\perp} = \\mathbf{a} - \\text{proj}_{\\mathbf{b}}\\mathbf{a} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}.\n\\] Now we check orthogonality: \\[\n\\mathbf{u}_{\\perp} \\cdot \\mathbf{b} = (0)(1) + (-1)(1) + (1)(1) = 0 - 1 + 1 = 0.\n\\] Since the dot product is zero, \\(\\mathbf{u}_{\\perp}\\) is indeed orthogonal to \\(\\mathbf{b}\\).\nWe have: \\[\n\\mathbf{u} \\cdot \\mathbf{e}_1 = u_1(1) + u_2(0) + u_3(0) = u_1,\n\\] \\[\n\\mathbf{e}_1 \\cdot \\mathbf{e}_1 = 1.\n\\] Therefore, \\[\n\\text{proj}_{\\mathbf{e}_1}\\mathbf{u} = \\frac{u_1}{1}\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} u_1 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n\\] The projection onto \\(\\mathbf{e}_1\\) simply extracts the first component of \\(\\mathbf{u}\\) and zeros out the other components. This makes sense geometrically: we’re finding the part of \\(\\mathbf{u}\\) that lies along the \\(x\\)-axis."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-1-vector-algebra",
    "href": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-1-vector-algebra",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "A vector in \\(\\mathbb{R}^n\\) is an ordered list of \\(n\\) real numbers, which we typically write in boldface, such as \\(\\mathbf{v}\\). For example, when \\(n=2\\), we might have the \\(2\\)-dimensional vector\n\\[\n\\mathbf{v} = \\begin{bmatrix} 3 \\\\ -2 \\end{bmatrix},\n\\]\nwhich we visualize as an arrow with its tail at the origin and its head at the point \\((3, -2)\\). Technically, though, a vector can be moved and still be the same vector, just as long as it keeps its direction and length intact. So, while it is often convenient to draw vectors with their tails at the origin, they could have their tails anywhere and still be the same vector.\nSimilarly, when \\(n=3\\), a \\(3\\)-dimensional vector might look like\n\\[\n\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 4 \\\\ -3 \\end{bmatrix}.\n\\]\nVectors can be added, subtracted, and scaled (multiplied by real numbers) componentwise. For example, if\n\\[\n\\mathbf{u} = \\begin{bmatrix} 2 \\\\ 5 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{v} = \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix},\n\\]\nthen\n\\[\n\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} 2 + 3 \\\\ 5 + (-1) \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 4 \\end{bmatrix},\n\\]\n\\[\n\\mathbf{u} - \\mathbf{v} = \\begin{bmatrix} 2 - 3 \\\\ 5 - (-1) \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 6 \\end{bmatrix},\n\\]\nand for a scalar \\(c = 3\\),\n\\[\n3\\mathbf{u} = \\begin{bmatrix} 3 \\cdot 2 \\\\ 3 \\cdot 5 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 15 \\end{bmatrix}.\n\\]\n\n\n\n\n\n\nLet \\(\\mathbf{u} = \\begin{bmatrix} 4 \\\\ -1 \\end{bmatrix}\\), \\(\\mathbf{v} = \\begin{bmatrix} -2 \\\\ 3 \\end{bmatrix}\\), \\(\\mathbf{a} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix}\\), and \\(\\mathbf{b} = \\begin{bmatrix} 4 \\\\ 0 \\\\ -2 \\end{bmatrix}\\).\n\nCompute \\(\\mathbf{u} + \\mathbf{v}\\).\nCompute \\(\\mathbf{u} - \\mathbf{v}\\).\nCompute \\(2\\mathbf{u} + 3\\mathbf{v}\\).\nCompute \\(-\\mathbf{u}\\).\nCompute \\(\\mathbf{a} + \\mathbf{b}\\).\nCompute \\(\\mathbf{a} - 2\\mathbf{b}\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} 4 + (-2) \\\\ -1 + 3 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\\)\n\\(\\mathbf{u} - \\mathbf{v} = \\begin{bmatrix} 4 - (-2) \\\\ -1 - 3 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ -4 \\end{bmatrix}\\)\n\\(2\\mathbf{u} + 3\\mathbf{v} = 2\\begin{bmatrix} 4 \\\\ -1 \\end{bmatrix} + 3\\begin{bmatrix} -2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ -2 \\end{bmatrix} + \\begin{bmatrix} -6 \\\\ 9 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 7 \\end{bmatrix}\\)\n\\(-\\mathbf{u} = \\begin{bmatrix} -4 \\\\ 1 \\end{bmatrix}\\)\n\\(\\mathbf{a} + \\mathbf{b} = \\begin{bmatrix} 2 + 4 \\\\ -1 + 0 \\\\ 3 + (-2) \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ -1 \\\\ 1 \\end{bmatrix}\\)\n\\(\\mathbf{a} - 2\\mathbf{b} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix} - 2\\begin{bmatrix} 4 \\\\ 0 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 8 \\\\ 0 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix} -6 \\\\ -1 \\\\ 7 \\end{bmatrix}\\)"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-2-dot-products-angles-and-orthogonality",
    "href": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-2-dot-products-angles-and-orthogonality",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "The dot product (also called the inner product or scalar product) of two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(\\mathbb{R}^n\\) is a real number obtained by multiplying corresponding components and summing the results. For example, if\n\\[\n\\mathbf{u} = \\begin{bmatrix} 2 \\\\ 3 \\\\ -1 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{v} = \\begin{bmatrix} 4 \\\\ 0 \\\\ 5 \\end{bmatrix},\n\\]\nthen the dot product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = (2)(4) + (3)(0) + (-1)(5) = 8 + 0 - 5 = 3.\n\\]\nWe also write this using angle bracket notation as\n\\[\n\\langle \\mathbf{u}, \\mathbf{v} \\rangle = 3.\n\\]\nThe length (or magnitude, or norm) of a vector \\(\\mathbf{v}\\) is defined as\n\\[\n\\|\\mathbf{v}\\| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}} = \\sqrt{\\langle \\mathbf{v}, \\mathbf{v} \\rangle}.\n\\]\nFor example, if \\(\\mathbf{u} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}\\), then\n\\[\n\\|\\mathbf{u}\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5.\n\\]\nThe dot product is closely related to the angle between two vectors. Specifically, if \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are two nonzero vectors in \\(\\mathbb{R}^n\\) and \\(\\theta\\) is the angle between them (with \\(0 \\leq \\theta \\leq \\pi\\)), then\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = \\|\\mathbf{u}\\| \\|\\mathbf{v}\\| \\cos{\\theta}.\n\\]\nRearranging, we get\n\\[\n\\cos{\\theta} = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}.\n\\]\nTwo vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are said to be orthogonal (or perpendicular) if their dot product is zero, i.e., \\(\\mathbf{u} \\cdot \\mathbf{v} = 0\\). This corresponds to the angle between them being \\(\\theta = \\frac{\\pi}{2}\\).\n\n\n\n\n\n\n\nLet \\(\\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix}\\). Compute \\(\\mathbf{u} \\cdot \\mathbf{v}\\).\nCompute \\(\\langle \\mathbf{u}, \\mathbf{u} \\rangle\\) for \\(\\mathbf{u} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix}\\).\nFind the length of the vector \\(\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix}\\).\nLet \\(\\mathbf{a} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\). Find the angle \\(\\theta\\) between \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\).\nDetermine whether the vectors \\(\\mathbf{p} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\) and \\(\\mathbf{q} = \\begin{bmatrix} 6 \\\\ -4 \\end{bmatrix}\\) are orthogonal.\nFind a vector in \\(\\mathbb{R}^2\\) that is orthogonal to \\(\\mathbf{w} = \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix}\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\mathbf{u} \\cdot \\mathbf{v} = (1)(3) + (2)(-4) = 3 - 8 = -5\\)\n\\(\\langle \\mathbf{u}, \\mathbf{u} \\rangle = 2^2 + (-1)^2 + 3^2 = 4 + 1 + 9 = 14\\)\n\\(\\|\\mathbf{v}\\| = \\sqrt{1^2 + 2^2 + 2^2} = \\sqrt{1 + 4 + 4} = \\sqrt{9} = 3\\)\nFirst, we compute the dot product and the magnitudes: \\[\n\\mathbf{a} \\cdot \\mathbf{b} = (1)(1) + (1)(0) = 1,\n\\] \\[\n\\|\\mathbf{a}\\| = \\sqrt{1^2 + 1^2} = \\sqrt{2},\n\\] \\[\n\\|\\mathbf{b}\\| = \\sqrt{1^2 + 0^2} = 1.\n\\] Therefore, \\[\n\\cos{\\theta} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|} = \\frac{1}{\\sqrt{2} \\cdot 1} = \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2}.\n\\] Thus, \\(\\theta = \\arccos\\left(\\frac{\\sqrt{2}}{2}\\right) = \\frac{\\pi}{4}\\).\nWe compute \\(\\mathbf{p} \\cdot \\mathbf{q} = (2)(6) + (3)(-4) = 12 - 12 = 0\\). Since the dot product is zero, the vectors are orthogonal.\nThere are infinitely many correct answers. One simple choice is \\(\\begin{bmatrix} -2 \\\\ 5 \\end{bmatrix}\\), since \\[\n\\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix} \\cdot \\begin{bmatrix} -2 \\\\ 5 \\end{bmatrix} = (5)(-2) + (2)(5) = -10 + 10 = 0.\n\\] Another choice is \\(\\begin{bmatrix} 2 \\\\ -5 \\end{bmatrix}\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-3-matrix-algebra",
    "href": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-3-matrix-algebra",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "A matrix is a rectangular array of numbers arranged in rows and columns. For example, the matrix\n\\[\nA = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\n\\]\nhas 2 rows and 3 columns, so we say \\(A\\) is a \\(2 \\times 3\\) matrix. The entry in row \\(i\\) and column \\(j\\) is often denoted \\(a_{ij}\\).\nMatrices can be added and subtracted (if they have the same number of rows and columns) by adding or subtracting corresponding entries. For example,\n\\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} + \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 6 & 8 \\\\ 10 & 12 \\end{bmatrix}.\n\\]\nMatrices can also be multiplied by scalars by multiplying each entry:\n\\[\n3\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 3 & 6 \\\\ 9 & 12 \\end{bmatrix}.\n\\]\nMatrix multiplication is more intricate. If \\(A\\) is an \\(m \\times n\\) matrix and \\(B\\) is an \\(n \\times p\\) matrix, then their product \\(AB\\) is an \\(m \\times p\\) matrix. The entry in row \\(i\\) and column \\(j\\) of \\(AB\\) is obtained by taking the dot product of the \\(i\\)-th row of \\(A\\) with the \\(j\\)-th column of \\(B\\). For example,\n\\[\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} (1)(5) + (2)(7) & (1)(6) + (2)(8) \\\\ (3)(5) + (4)(7) & (3)(6) + (4)(8) \\end{bmatrix} = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}.\n\\]\nNote: Matrix multiplication is not commutative in general, meaning that \\(AB \\neq BA\\) in most cases.\n\n\n\n\n\n\n\nLet \\(A = \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix}\\) and \\(B = \\begin{bmatrix} 4 & 1 \\\\ -2 & 5 \\end{bmatrix}\\). Compute \\(A + B\\).\nCompute \\(2A - B\\).\nCompute the matrix product \\(AB\\).\nCompute the matrix product \\(BA\\).\nDoes \\(AB = BA\\) for these matrices? What does this tell you about matrix multiplication?\nLet \\(C = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\). Compute \\(AC\\) and \\(CA\\). What do you notice about the matrix \\(C\\)?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(A + B = \\begin{bmatrix} 2 + 4 & -1 + 1 \\\\ 0 + (-2) & 3 + 5 \\end{bmatrix} = \\begin{bmatrix} 6 & 0 \\\\ -2 & 8 \\end{bmatrix}\\)\n\\(2A - B = 2\\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix} - \\begin{bmatrix} 4 & 1 \\\\ -2 & 5 \\end{bmatrix} = \\begin{bmatrix} 4 & -2 \\\\ 0 & 6 \\end{bmatrix} - \\begin{bmatrix} 4 & 1 \\\\ -2 & 5 \\end{bmatrix} = \\begin{bmatrix} 0 & -3 \\\\ 2 & 1 \\end{bmatrix}\\)\n\\(AB = \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix} \\begin{bmatrix} 4 & 1 \\\\ -2 & 5 \\end{bmatrix} = \\begin{bmatrix} (2)(4) + (-1)(-2) & (2)(1) + (-1)(5) \\\\ (0)(4) + (3)(-2) & (0)(1) + (3)(5) \\end{bmatrix} = \\begin{bmatrix} 10 & -3 \\\\ -6 & 15 \\end{bmatrix}\\)\n\\(BA = \\begin{bmatrix} 4 & 1 \\\\ -2 & 5 \\end{bmatrix} \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix} = \\begin{bmatrix} (4)(2) + (1)(0) & (4)(-1) + (1)(3) \\\\ (-2)(2) + (5)(0) & (-2)(-1) + (5)(3) \\end{bmatrix} = \\begin{bmatrix} 8 & -1 \\\\ -4 & 17 \\end{bmatrix}\\)\nNo, \\(AB \\neq BA\\). This demonstrates that matrix multiplication is not commutative—the order of multiplication matters.\n\\(AC = \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix} = A\\)\n\\(CA = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix} = \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix} = A\\)\nThe matrix \\(C\\) is the identity matrix, which has the property that \\(CA = AC = A\\) for any matrix \\(A\\) (of compatible shape). It acts as the “multiplicative identity” for matrices, analogous to the number 1 for real numbers."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-4-determinants-and-areas",
    "href": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-4-determinants-and-areas",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "For a \\(2 \\times 2\\) matrix\n\\[\nA = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix},\n\\]\nthe determinant of \\(A\\) is defined as\n\\[\n\\det(A) = ad - bc.\n\\]\nThe determinant has many important interpretations and applications in mathematics. One of the most geometric interpretations is that \\(|\\det(A)|\\) (the absolute value of the determinant) represents the area of the parallelogram formed by the column vectors of \\(A\\).\nSpecifically, if we think of the matrix \\(A\\) above as having column vectors\n\\[\n\\mathbf{v}_1 = \\begin{bmatrix} a \\\\ c \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{v}_2 = \\begin{bmatrix} b \\\\ d \\end{bmatrix},\n\\]\nthen the parallelogram with sides \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) (with one vertex at the origin) has area \\(|\\det(A)|\\).\nFor example, the matrix\n\\[\nA = \\begin{bmatrix} 3 & 1 \\\\ 0 & 2 \\end{bmatrix}\n\\]\nhas determinant \\(\\det(A) = (3)(2) - (1)(0) = 6\\), so the parallelogram formed by the vectors \\(\\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}\\) and \\(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) has area 6.\nNote: If \\(\\det(A) = 0\\), the matrix is called singular, and geometrically this means the two column vectors are parallel (or one is the zero vector), so they don’t span a two-dimensional region.\n\n\n\n\n\n\n\nCompute the determinant of \\(A = \\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}\\).\nCompute the determinant of \\(B = \\begin{bmatrix} 5 & -2 \\\\ 3 & 1 \\end{bmatrix}\\).\nFind the area of the parallelogram formed by the vectors \\(\\mathbf{v}_1 = \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathbf{v}_2 = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\).\nDetermine whether the matrix \\(C = \\begin{bmatrix} 6 & 2 \\\\ 9 & 3 \\end{bmatrix}\\) is singular.\nFind a value of \\(k\\) such that the matrix \\(D = \\begin{bmatrix} k & 4 \\\\ 3 & 6 \\end{bmatrix}\\) is singular.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\det(A) = (2)(4) - (3)(1) = 8 - 3 = 5\\)\n\\(\\det(B) = (5)(1) - (-2)(3) = 5 + 6 = 11\\)\nThe area is the absolute value of the determinant of the matrix with columns \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\): \\[\n\\left|\\det\\begin{bmatrix} 4 & 2 \\\\ 1 & 3 \\end{bmatrix}\\right| = |(4)(3) - (2)(1)| = |12 - 2| = 10.\n\\]\n\\(\\det(C) = (6)(3) - (2)(9) = 18 - 18 = 0\\). Since the determinant is zero, the matrix is singular. (Note: The second column is exactly \\(\\frac{1}{3}\\) times the first column, so the column vectors are parallel.)\nFor \\(D\\) to be singular, we need \\(\\det(D) = 0\\): \\[\n(k)(6) - (4)(3) = 0\n\\] \\[\n6k - 12 = 0\n\\] \\[\nk = 2.\n\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-5-the-derivative-as-a-difference-quotient-and-linear-approximation",
    "href": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-5-the-derivative-as-a-difference-quotient-and-linear-approximation",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall from single-variable calculus that the derivative of a function \\(f\\) at a point \\(x = a\\) is defined as the limit\n\\[\nf'(a) = \\lim_{h \\to 0} \\frac{f(a + h) - f(a)}{h},\n\\]\nprovided this limit exists. This ratio \\(\\frac{f(a + h) - f(a)}{h}\\) is called a difference quotient, and it represents the slope of the secant line connecting the points \\((a, f(a))\\) and \\((a + h, f(a + h))\\) on the graph of \\(f\\). As \\(h\\) approaches zero, this secant line approaches the tangent line to the graph of \\(f\\) at the point \\((a, f(a))\\).\nThe tangent line at \\(x = a\\) has equation\n\\[\ny = f(a) + f'(a)(x - a).\n\\]\nThis is called the linear approximation (or tangent line approximation) to \\(f\\) at \\(x = a\\). For values of \\(x\\) close to \\(a\\), we have\n\\[\nf(x) \\approx f(a) + f'(a)(x - a).\n\\]\nThis approximation says that, near the point \\(x = a\\), the function \\(f\\) is approximately equal to a linear function with slope \\(f'(a)\\) passing through the point \\((a, f(a))\\). In fact, the tangent line is the best linear approximation to \\(f\\) near \\(x = a\\).\n\n\n\n\n\n\n\nLet \\(f(x) = x^2\\). Write out the difference quotient \\(\\frac{f(3 + h) - f(3)}{h}\\) explicitly (without taking the limit).\nSimplify your expression from part 1 as much as possible.\nNow compute \\(f'(3)\\) by taking the limit as \\(h \\to 0\\) of your simplified expression.\nUse the definition of the derivative as a limit to compute \\(f'(2)\\) for \\(f(x) = \\frac{1}{x}\\).\nFind the equation of the tangent line to \\(f(x) = x^3\\) at the point \\(x = 1\\).\nUse the linear approximation to estimate \\(f(1.1)\\) for \\(f(x) = x^3\\) at \\(x = 1\\).\nCompare your estimate from part 6 to the actual value \\(f(1.1) = (1.1)^3 = 1.331\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nWe have \\[\n\\frac{f(3 + h) - f(3)}{h} = \\frac{(3 + h)^2 - 3^2}{h} = \\frac{(3 + h)^2 - 9}{h}.\n\\]\nExpanding the numerator: \\[\n\\frac{(3 + h)^2 - 9}{h} = \\frac{9 + 6h + h^2 - 9}{h} = \\frac{6h + h^2}{h} = \\frac{h(6 + h)}{h} = 6 + h.\n\\]\nTaking the limit: \\[\nf'(3) = \\lim_{h \\to 0} (6 + h) = 6.\n\\]\nWe compute the difference quotient: \\[\n\\frac{f(2 + h) - f(2)}{h} = \\frac{\\frac{1}{2 + h} - \\frac{1}{2}}{h} = \\frac{\\frac{2 - (2 + h)}{2(2 + h)}}{h} = \\frac{-h}{2h(2 + h)} = \\frac{-1}{2(2 + h)}.\n\\] Taking the limit: \\[\nf'(2) = \\lim_{h \\to 0} \\frac{-1}{2(2 + h)} = \\frac{-1}{2(2)} = -\\frac{1}{4}.\n\\]\nFirst, we need \\(f(1) = 1^3 = 1\\) and \\(f'(1) = 3(1)^2 = 3\\). The tangent line has equation \\[\ny = f(1) + f'(1)(x - 1) = 1 + 3(x - 1) = 1 + 3x - 3 = 3x - 2.\n\\]\nUsing the linear approximation at \\(x = 1\\): \\[\nf(1.1) \\approx f(1) + f'(1)(1.1 - 1) = 1 + 3(0.1) = 1 + 0.3 = 1.3.\n\\]\nThe actual value is \\(f(1.1) = 1.331\\), so our linear approximation gave \\(1.3\\), which is quite close. The error is \\(|1.331 - 1.3| = 0.031\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-6-integrals-and-signed-area",
    "href": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-6-integrals-and-signed-area",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall from single-variable calculus that the definite integral of a function \\(f\\) over an interval \\([a, b]\\) is defined as\n\\[\n\\int_a^b f(x)\\, dx,\n\\]\nand it represents the signed area between the graph of \\(f\\) and the \\(x\\)-axis over the interval \\([a, b]\\). Here, “signed” means that:\n\nAreas above the \\(x\\)-axis (where \\(f(x) &gt; 0\\)) contribute positively to the integral.\nAreas below the \\(x\\)-axis (where \\(f(x) &lt; 0\\)) contribute negatively to the integral.\n\nThe Fundamental Theorem of Calculus states that if \\(F\\) is an antiderivative of \\(f\\) (meaning \\(F'(x) = f(x)\\)), then\n\\[\n\\int_a^b f(x)\\, dx = F(b) - F(a).\n\\]\nThis theorem connects the two main concepts of calculus: derivatives and integrals.\nFor example, if \\(f(x) = x\\) and we integrate from \\(x = 0\\) to \\(x = 2\\), we get\n\\[\n\\int_0^2 x\\, dx = \\left[\\frac{x^2}{2}\\right]_0^2 = \\frac{4}{2} - \\frac{0}{2} = 2.\n\\]\nGeometrically, this represents the area of a triangle with base 2 and height 2, which is \\(\\frac{1}{2}(2)(2) = 2\\).\n\n\n\n\n\n\n\nCompute \\(\\displaystyle\\int_1^3 2x\\, dx\\) using the Fundamental Theorem of Calculus.\nVerify your answer from part 1 by interpreting the integral as the area of a trapezoid.\nCompute \\(\\displaystyle\\int_0^\\pi \\sin{x}\\, dx\\).\nWhat is the geometric interpretation of your answer from part 3?\nCompute \\(\\displaystyle\\int_{-1}^1 x^3\\, dx\\).\nExplain why the answer to part 5 is zero using the concept of signed area.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nAn antiderivative of \\(f(x) = 2x\\) is \\(F(x) = x^2\\). Therefore, \\[\n\\int_1^3 2x\\, dx = F(3) - F(1) = 3^2 - 1^2 = 9 - 1 = 8.\n\\]\nThe region under the curve \\(y = 2x\\) from \\(x = 1\\) to \\(x = 3\\) is a trapezoid with parallel sides of heights \\(f(1) = 2\\) and \\(f(3) = 6\\), and base width \\(3 - 1 = 2\\). The area is \\[\n\\frac{1}{2}(2 + 6)(2) = \\frac{1}{2}(8)(2) = 8.\n\\] This matches our answer from part 1.\nAn antiderivative of \\(\\sin{x}\\) is \\(-\\cos{x}\\). Therefore, \\[\n\\int_0^\\pi \\sin{x}\\, dx = [-\\cos{x}]_0^\\pi = -\\cos{\\pi} - (-\\cos{0}) = -(-1) - (-1) = 1 + 1 = 2.\n\\]\nThis represents the total area between the sine curve and the \\(x\\)-axis from \\(x = 0\\) to \\(x = \\pi\\). Since \\(\\sin{x} &gt; 0\\) for all \\(x \\in (0, \\pi)\\), the entire region is above the \\(x\\)-axis, so the signed area equals the actual geometric area.\nAn antiderivative of \\(x^3\\) is \\(\\frac{x^4}{4}\\). Therefore, \\[\n\\int_{-1}^1 x^3\\, dx = \\left[\\frac{x^4}{4}\\right]_{-1}^1 = \\frac{1^4}{4} - \\frac{(-1)^4}{4} = \\frac{1}{4} - \\frac{1}{4} = 0.\n\\]\nThe function \\(f(x) = x^3\\) is an odd function, meaning \\(f(-x) = -f(x)\\). This means the graph is symmetric about the origin. For \\(x \\in (0, 1)\\), we have \\(f(x) &gt; 0\\), contributing positive area. For \\(x \\in (-1, 0)\\), we have \\(f(x) &lt; 0\\), contributing negative area. Because of the symmetry, these two areas are equal in magnitude but opposite in sign, so they cancel out, giving a total signed area of zero."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-7-rectangular-boxes",
    "href": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-7-rectangular-boxes",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "In this problem, we will work with rectangular boxes in \\(\\mathbb{R}^3\\). We assume that all boxes have eight vertices (i.e., corners), and that their edges are aligned with the coordinate axes. In other words, every edge of a box is parallel to either the \\(x\\)-, \\(y\\)-, or \\(z\\)-axis.\n\n\n\n\n\n\n\nA rectangular box has two opposite corners at \\(P = (0, 0, 0)\\) and \\(Q = (3, 2, 4)\\). Find the coordinates of all eight vertices of the box.\nA rectangular box has three vertices at \\(A = (1, 1, 1)\\), \\(B = (5, 1, 1)\\), and \\(C = (1, 3, 4)\\). Find the coordinates of all eight vertices of the box.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe four vertices of the bottom of the box are at \\[\n(0,0,0), \\quad (3,0,0), \\quad (0,2,0), \\quad (3,2,0).\n\\] The four vertices of the top of the box are at \\[\n(0,0,4), \\quad (3,0,4), \\quad (0,2,4), \\quad (3,2,4).\n\\]\nThe four vertices of the bottom of the box are at \\[\n(1, 1, 1), \\quad (5, 1, 1), \\quad (5, 3, 1), \\quad (1, 3, 1).\n\\] The four vertices of the top of the box are at \\[\n(1, 1, 4), \\quad (5, 1, 4), \\quad (5, 3, 4), \\quad (1, 3, 4).\n\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-8-practice-moving-in-mathbbr3",
    "href": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-8-practice-moving-in-mathbbr3",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall that in \\(\\mathbb{R}^3\\), there are three coordinate planes:\n\nThe \\(xy\\)-plane, where \\(z = 0\\).\nThe \\(xz\\)-plane, where \\(y = 0\\).\nThe \\(yz\\)-plane, where \\(x = 0\\).\n\nOther planes that are parallel with one of these coordinate planes can be described by equations of the form:\n\n\\(x = k\\) for planes parallel to the \\(yz\\)-plane,\n\\(y = k\\) for planes parallel to the \\(xz\\)-plane,\n\\(z = k\\) for planes parallel to the \\(xy\\)-plane,\n\nwhere \\(k\\) is a constant.\nSimilarly, the three coordinate axes are:\n\nThe \\(x\\)-axis, where \\(y = 0\\) and \\(z = 0\\).\nThe \\(y\\)-axis, where \\(x = 0\\) and \\(z = 0\\).\nThe \\(z\\)-axis, where \\(x = 0\\) and \\(y = 0\\).\n\nOther lines that are parallel with one of these coordinate axes can be described by pairs of equations of the form:\n\n\\(y= k_1\\) and \\(z = k_2\\) for lines parallel to the \\(x\\)-axis,\n\\(x = k_1\\) and \\(z = k_2\\) for lines parallel to the \\(y\\)-axis,\n\\(x = k_1\\) and \\(y = k_2\\) for lines parallel to the \\(z\\)-axis,\n\nwhere \\(k_1\\) and \\(k_2\\) are constants.\n\n\n\n\n\n\n\nYou are standing at a point in the \\(yz\\)-plane and you walk \\(3\\) units in the negative \\(x\\)-direction to arrive at the point \\((-3, 1, 2)\\). What point were you at to begin with?\nYou start at a point on the \\(xy\\)-plane and walk \\(4\\) units in the positive \\(z\\)-direction to arrive at \\((2, -1, 4)\\). Find your starting point.\nYou begin at a point on the \\(x\\)-axis and walk \\(2\\) units in the positive \\(y\\)-direction and \\(3\\) units in the negative \\(z\\)-direction to arrive at \\((5, 2, -3)\\). Where did you start?\nYou start at the point \\((1, 4, 2)\\) and walk \\(5\\) units in the negative \\(y\\)-direction. At what point do you arrive?\nStarting from the origin, you walk \\(3\\) units in the positive \\(x\\)-direction, then \\(2\\) units in the negative \\(y\\)-direction, and finally \\(4\\) units in the positive \\(z\\)-direction. What is your final position?\nYou begin at a point on the plane \\(x=2\\) and walk \\(4\\) units in the negative \\(x\\)-direction, \\(2\\) units in the positive \\(y\\)-direction, and \\(3\\) units in the negative \\(z\\)-direction to arrive at \\((-2, 4, -2)\\). What was your starting point?\nYou begin at a point on the line defined by \\(y=3\\) and \\(z=-1\\). You walk \\(1\\) unit in the negative \\(y\\)-direction, then \\(4\\) units in the negative \\(x\\)-direction, and finally \\(6\\) units in the positive \\(z\\)-direction to arrive at \\((-1, 2, 5)\\). Where did you start?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\((0, 1, 2)\\)\n\\((2, -1, 0)\\)\n\\((5, 0, 0)\\)\n\\((1, -1, 2)\\)\n\\((3, -2, 4)\\)\n\\((2, 2, 1)\\)\n\\((3, 3, -1)\\)"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-9-basis-vectors-and-vector-decomposition",
    "href": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-9-basis-vectors-and-vector-decomposition",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall from class that the standard basis vectors in \\(\\mathbb{R}^2\\) are\n\\[\n\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}.\n\\]\nSimilarly, in \\(\\mathbb{R}^3\\), the standard basis vectors are\n\\[\n\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n\\]\nWe saw that any vector in \\(\\mathbb{R}^n\\) can be written as a linear combination of the standard basis vectors. For example, in \\(\\mathbb{R}^3\\),\n\\[\n\\mathbf{v} = \\begin{bmatrix} 2 \\\\ -3 \\\\ 5 \\end{bmatrix} = 2\\mathbf{e}_1 - 3\\mathbf{e}_2 + 5\\mathbf{e}_3.\n\\]\nOne important property of the standard basis vectors is that they are orthonormal, meaning:\n\nThey are all unit vectors: \\(\\|\\mathbf{e}_i\\| = 1\\) for all \\(i\\).\nThey are mutually orthogonal: \\(\\mathbf{e}_i \\cdot \\mathbf{e}_j = 0\\) whenever \\(i \\neq j\\).\n\n\n\n\n\n\n\n\nWrite the vector \\(\\mathbf{v} = \\begin{bmatrix} 4 \\\\ -2 \\\\ 7 \\end{bmatrix}\\) as a linear combination of the standard basis vectors \\(\\mathbf{e}_1\\), \\(\\mathbf{e}_2\\), and \\(\\mathbf{e}_3\\).\nLet \\(\\mathbf{w} = 3\\mathbf{e}_1 - 5\\mathbf{e}_2 + 2\\mathbf{e}_3\\). Write \\(\\mathbf{w}\\) in column vector form.\nVerify that the standard basis vectors in \\(\\mathbb{R}^3\\) are orthonormal by computing all pairwise dot products and all norms.\nLet \\(\\mathbf{u} = \\begin{bmatrix} -1 \\\\ 6 \\\\ 2 \\end{bmatrix}\\). Compute \\(\\mathbf{u} \\cdot \\mathbf{e}_1\\), \\(\\mathbf{u} \\cdot \\mathbf{e}_2\\), and \\(\\mathbf{u} \\cdot \\mathbf{e}_3\\). What do you notice?\nExpress the zero vector \\(\\mathbf{0}\\) in \\(\\mathbb{R}^3\\) as a linear combination of the standard basis vectors.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\mathbf{v} = 4\\mathbf{e}_1 - 2\\mathbf{e}_2 + 7\\mathbf{e}_3\\)\n\\(\\mathbf{w} = \\begin{bmatrix} 3 \\\\ -5 \\\\ 2 \\end{bmatrix}\\)\nComputing the norms: \\[\n\\|\\mathbf{e}_1\\| = \\sqrt{1^2 + 0^2 + 0^2} = 1, \\quad \\|\\mathbf{e}_2\\| = \\sqrt{0^2 + 1^2 + 0^2} = 1, \\quad \\|\\mathbf{e}_3\\| = \\sqrt{0^2 + 0^2 + 1^2} = 1.\n\\] Computing the pairwise dot products: \\[\n\\mathbf{e}_1 \\cdot \\mathbf{e}_2 = (1)(0) + (0)(1) + (0)(0) = 0,\n\\] \\[\n\\mathbf{e}_1 \\cdot \\mathbf{e}_3 = (1)(0) + (0)(0) + (0)(1) = 0,\n\\] \\[\n\\mathbf{e}_2 \\cdot \\mathbf{e}_3 = (0)(0) + (1)(0) + (0)(1) = 0.\n\\] Since all the norms are 1 and all pairwise dot products are 0, the vectors are orthonormal.\n\\(\\mathbf{u} \\cdot \\mathbf{e}_1 = (-1)(1) + (6)(0) + (2)(0) = -1\\)\n\\(\\mathbf{u} \\cdot \\mathbf{e}_2 = (-1)(0) + (6)(1) + (2)(0) = 6\\)\n\\(\\mathbf{u} \\cdot \\mathbf{e}_3 = (-1)(0) + (6)(0) + (2)(1) = 2\\)\nThe dot products give exactly the components of \\(\\mathbf{u}\\)! This shows that \\(\\mathbf{u} = (\\mathbf{u} \\cdot \\mathbf{e}_1)\\mathbf{e}_1 + (\\mathbf{u} \\cdot \\mathbf{e}_2)\\mathbf{e}_2 + (\\mathbf{u} \\cdot \\mathbf{e}_3)\\mathbf{e}_3\\).\n\\(\\mathbf{0} = 0\\mathbf{e}_1 + 0\\mathbf{e}_2 + 0\\mathbf{e}_3\\)"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-10-unit-vectors-and-scaling",
    "href": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-10-unit-vectors-and-scaling",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall that a unit vector is a vector with length (norm) equal to 1. Given any nonzero vector \\(\\mathbf{v}\\), we can always construct a unit vector in the same direction, often denoted \\(\\mathbf{\\hat{v}}\\), by dividing \\(\\mathbf{v}\\) by its length:\n\\[\n\\mathbf{\\hat{v}} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}.\n\\]\nThis process is called normalization of \\(\\mathbf{v}\\).\n\n\n\n\n\n\n\nFind the length of the vector \\(\\mathbf{v} = \\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix}\\).\nFind a unit vector pointing in the same direction as \\(\\mathbf{v} = \\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix}\\).\nFind a vector with length \\(10\\) pointing in the same direction as \\(\\mathbf{v} = \\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix}\\).\nFind a vector with length \\(2\\) pointing in the same direction as \\(\\mathbf{u} = \\mathbf{e}_1 - \\mathbf{e}_2 + 2\\mathbf{e}_3\\).\nLet \\(\\mathbf{a} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\). Find a unit vector in the direction of \\(\\mathbf{a}\\).\nFind a vector of length \\(5\\) pointing in the opposite direction of \\(\\mathbf{b} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 2 \\end{bmatrix}\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\|\\mathbf{v}\\| = \\sqrt{3^2 + (-4)^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\\)\n\\(\\mathbf{\\hat{v}} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|} = \\frac{1}{5}\\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix} 3/5 \\\\ -4/5 \\end{bmatrix}\\)\nThe vector is \\(10 \\cdot \\mathbf{\\hat{v}} = 10 \\cdot \\frac{1}{5}\\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix} = 2\\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ -8 \\end{bmatrix}\\)\nFirst, we write \\(\\mathbf{u}\\) in column form: \\(\\mathbf{u} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}\\). Then we compute its length: \\[\n\\|\\mathbf{u}\\| = \\sqrt{1^2 + (-1)^2 + 2^2} = \\sqrt{1 + 1 + 4} = \\sqrt{6}.\n\\] The desired vector is \\[\n2 \\cdot \\frac{\\mathbf{u}}{\\|\\mathbf{u}\\|} = \\frac{2}{\\sqrt{6}}\\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 2/\\sqrt{6} \\\\ -2/\\sqrt{6} \\\\ 4/\\sqrt{6} \\end{bmatrix}.\n\\]\n\\(\\|\\mathbf{a}\\| = \\sqrt{1^2 + 1^2 + 1^2} = \\sqrt{3}\\). The unit vector is \\[\n\\mathbf{\\hat{a}} = \\frac{1}{\\sqrt{3}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1/\\sqrt{3} \\\\ 1/\\sqrt{3} \\\\ 1/\\sqrt{3} \\end{bmatrix} = \\begin{bmatrix} \\sqrt{3}/3 \\\\ \\sqrt{3}/3 \\\\ \\sqrt{3}/3 \\end{bmatrix}.\n\\]\nFirst, \\(\\|\\mathbf{b}\\| = \\sqrt{2^2 + (-1)^2 + 2^2} = \\sqrt{4 + 1 + 4} = \\sqrt{9} = 3\\). To point in the opposite direction, we need a negative scalar: \\[\n-5 \\cdot \\frac{\\mathbf{b}}{\\|\\mathbf{b}\\|} = -\\frac{5}{3}\\begin{bmatrix} 2 \\\\ -1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} -10/3 \\\\ 5/3 \\\\ -10/3 \\end{bmatrix}.\n\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-11-parallel-vectors",
    "href": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-11-parallel-vectors",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Two nonzero vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are said to be parallel if one is a scalar multiple of the other. That is, \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are parallel if there exists a scalar \\(c\\) such that \\(\\mathbf{u} = c\\mathbf{v}\\) (or \\(\\mathbf{v} = c\\mathbf{u}\\), it doesn’t matter which way you write it).\nGeometrically, parallel vectors point in the same direction (if \\(c &gt; 0\\)) or in opposite directions (if \\(c &lt; 0\\)).\n\n\n\n\n\n\n\nDetermine whether the vectors \\(\\mathbf{u} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}\\) are parallel.\nFind a vector parallel to \\(\\mathbf{w} = \\begin{bmatrix} 1 \\\\ -3 \\\\ 2 \\end{bmatrix}\\) with length \\(7\\).\nFind the values of \\(a\\) that make \\(\\mathbf{v} = 5a\\mathbf{e}_1 - 3\\mathbf{e}_2\\) parallel to \\(\\mathbf{w} = a^2\\mathbf{e}_1 + 6\\mathbf{e}_2\\).\nLet \\(\\mathbf{p} = \\begin{bmatrix} 4 \\\\ -2 \\\\ 6 \\end{bmatrix}\\) and \\(\\mathbf{q} = \\begin{bmatrix} -6 \\\\ 3 \\\\ k \\end{bmatrix}\\). Find the value of \\(k\\) such that \\(\\mathbf{p}\\) and \\(\\mathbf{q}\\) are parallel.\nExplain why the zero vector \\(\\mathbf{0}\\) is considered parallel to every vector.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nWe check if \\(\\mathbf{v} = c\\mathbf{u}\\) for some scalar \\(c\\): \\[\n\\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix} = c\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 2c \\\\ 4c \\end{bmatrix}.\n\\] From the first component, \\(3 = 2c\\), so \\(c = 3/2\\). From the second component, \\(6 = 4c\\), so \\(c = 6/4 = 3/2\\). Since both components give the same value of \\(c\\), the vectors are parallel.\nFirst, we need the length of \\(\\mathbf{w}\\): \\(\\|\\mathbf{w}\\| = \\sqrt{1^2 + (-3)^2 + 2^2} = \\sqrt{1 + 9 + 4} = \\sqrt{14}\\). A parallel vector with length \\(7\\) is \\[\n7 \\cdot \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|} = \\frac{7}{\\sqrt{14}}\\begin{bmatrix} 1 \\\\ -3 \\\\ 2 \\end{bmatrix}.\n\\]\nWriting in column form: \\(\\mathbf{v} = \\begin{bmatrix} 5a \\\\ -3 \\end{bmatrix}\\) and \\(\\mathbf{w} = \\begin{bmatrix} a^2 \\\\ 6 \\end{bmatrix}\\). For these to be parallel, we need \\(\\mathbf{v} = c\\mathbf{w}\\) for some scalar \\(c\\): \\[\n\\begin{bmatrix} 5a \\\\ -3 \\end{bmatrix} = c\\begin{bmatrix} a^2 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} ca^2 \\\\ 6c \\end{bmatrix}.\n\\] From the first component: \\(5a = ca^2\\), so \\(c = 5a/a^2 = 5/a\\) (assuming \\(a \\neq 0\\)).\nFrom the second component: \\(-3 = 6c\\), so \\(c = -1/2\\).\nSetting these equal: \\(5/a = -1/2\\), which gives \\(a = -10\\).\nWe should also check the case \\(a = 0\\): if \\(a = 0\\), then \\(\\mathbf{v} = \\begin{bmatrix} 0 \\\\ -3 \\end{bmatrix}\\) and \\(\\mathbf{w} = \\begin{bmatrix} 0 \\\\ 6 \\end{bmatrix}\\), which are parallel (one is \\(-1/2\\) times the other). So \\(a = 0\\) and \\(a = -10\\) both work.\nFor \\(\\mathbf{p}\\) and \\(\\mathbf{q}\\) to be parallel, we need \\(\\mathbf{q} = c\\mathbf{p}\\) for some scalar \\(c\\): \\[\n\\begin{bmatrix} -6 \\\\ 3 \\\\ k \\end{bmatrix} = c\\begin{bmatrix} 4 \\\\ -2 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 4c \\\\ -2c \\\\ 6c \\end{bmatrix}.\n\\] From the first component: \\(-6 = 4c\\), so \\(c = -3/2\\).\nFrom the second component: \\(3 = -2c\\), so \\(c = -3/2\\). (Consistent!)\nFrom the third component: \\(k = 6c = 6(-3/2) = -9\\).\nThe zero vector can be written as \\(\\mathbf{0} = 0 \\cdot \\mathbf{v}\\) for any vector \\(\\mathbf{v}\\). Since \\(\\mathbf{0}\\) is always a scalar multiple of any vector (with scalar \\(c = 0\\)), it is parallel to every vector by definition."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-12-orthogonal-projections",
    "href": "teaching/multi-calc-sp-26/exercises/01ex-vectors-and-matrices-part-1.html#exercise-12-orthogonal-projections",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Given two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) (with \\(\\mathbf{v} \\neq \\mathbf{0}\\)), we can decompose \\(\\mathbf{u}\\) into two parts:\n\nA component parallel to \\(\\mathbf{v}\\), called the projection of \\(\\mathbf{u}\\) onto \\(\\mathbf{v}\\)\nA component perpendicular (orthogonal) to \\(\\mathbf{v}\\)\n\nThe projection of \\(\\mathbf{u}\\) onto \\(\\mathbf{v}\\) is denoted \\(\\text{proj}_{\\mathbf{v}}\\mathbf{u}\\) and is defined as\n\\[\n\\text{proj}_{\\mathbf{v}}\\mathbf{u} = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\mathbf{v} \\cdot \\mathbf{v}} \\mathbf{v} = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{v}\\|^2} \\mathbf{v}.\n\\]\nThe component of \\(\\mathbf{u}\\) perpendicular to \\(\\mathbf{v}\\) is the vector\n\\[\n\\mathbf{u}_{\\perp} = \\mathbf{u} - \\text{proj}_{\\mathbf{v}}\\mathbf{u}.\n\\]\nThis gives us the decomposition: \\(\\mathbf{u} = \\text{proj}_{\\mathbf{v}}\\mathbf{u} + \\mathbf{u}_{\\perp}\\).\n\n\n\n\n\n\n\nLet \\(\\mathbf{u} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\). Compute the projection of \\(\\mathbf{u}\\) onto \\(\\mathbf{v}\\).\nFor the same vectors as in part a, find the component of \\(\\mathbf{u}\\) perpendicular to \\(\\mathbf{v}\\).\nVerify that \\(\\mathbf{u} = \\text{proj}_{\\mathbf{v}}\\mathbf{u} + \\mathbf{u}_{\\perp}\\) for the vectors in parts a and b.\nLet \\(\\mathbf{u} = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\). Compute \\(\\text{proj}_{\\mathbf{v}}\\mathbf{u}\\) and \\(\\mathbf{u}_{\\perp}\\). Then draw all four vectors (\\(\\mathbf{u}\\), \\(\\mathbf{v}\\), \\(\\text{proj}_{\\mathbf{v}}\\mathbf{u}\\), and \\(\\mathbf{u}_{\\perp}\\)) in the plane, and verify geometrically that \\(\\mathbf{u} = \\text{proj}_{\\mathbf{v}}\\mathbf{u} + \\mathbf{u}_{\\perp}\\).\nLet \\(\\mathbf{a} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\end{bmatrix}\\) and \\(\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\). Find \\(\\text{proj}_{\\mathbf{b}}\\mathbf{a}\\).\nFor the vectors in part e, verify that \\(\\mathbf{u}_{\\perp} = \\mathbf{a} - \\text{proj}_{\\mathbf{b}}\\mathbf{a}\\) is orthogonal to \\(\\mathbf{b}\\).\nCompute \\(\\text{proj}_{\\mathbf{e}_1}\\mathbf{u}\\) for an arbitrary vector \\(\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\end{bmatrix}\\). What do you notice?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nWe compute: \\[\n\\mathbf{u} \\cdot \\mathbf{v} = (3)(1) + (4)(0) = 3,\n\\] \\[\n\\mathbf{v} \\cdot \\mathbf{v} = 1^2 + 0^2 = 1.\n\\] Therefore, \\[\n\\text{proj}_{\\mathbf{v}}\\mathbf{u} = \\frac{3}{1}\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}.\n\\]\n\\(\\mathbf{u}_{\\perp} = \\mathbf{u} - \\text{proj}_{\\mathbf{v}}\\mathbf{u} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} - \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 4 \\end{bmatrix}\\)\n\\(\\text{proj}_{\\mathbf{v}}\\mathbf{u} + \\mathbf{u}_{\\perp} = \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} = \\mathbf{u}\\)\nWe compute: \\[\n\\mathbf{u} \\cdot \\mathbf{v} = (3)(2) + (1)(1) = 6 + 1 = 7,\n\\] \\[\n\\mathbf{v} \\cdot \\mathbf{v} = 2^2 + 1^2 = 4 + 1 = 5.\n\\] Therefore, \\[\n\\text{proj}_{\\mathbf{v}}\\mathbf{u} = \\frac{7}{5}\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 14/5 \\\\ 7/5 \\end{bmatrix}.\n\\] And \\[\n\\mathbf{u}_{\\perp} = \\mathbf{u} - \\text{proj}_{\\mathbf{v}}\\mathbf{u} = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 14/5 \\\\ 7/5 \\end{bmatrix} = \\begin{bmatrix} 15/5 - 14/5 \\\\ 5/5 - 7/5 \\end{bmatrix} = \\begin{bmatrix} 1/5 \\\\ -2/5 \\end{bmatrix}.\n\\]\nIn a diagram, you should see that:\n\n\\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) both start at the origin\n\\(\\text{proj}_{\\mathbf{v}}\\mathbf{u}\\) lies along the line defined by \\(\\mathbf{v}\\)\n\\(\\mathbf{u}_{\\perp}\\) is perpendicular to \\(\\mathbf{v}\\)\nWhen you add \\(\\text{proj}_{\\mathbf{v}}\\mathbf{u}\\) and \\(\\mathbf{u}_{\\perp}\\) using the “tip-to-tail” rule, you get \\(\\mathbf{u}\\)\n\nWe compute: \\[\n\\mathbf{a} \\cdot \\mathbf{b} = (2)(1) + (1)(1) + (3)(1) = 2 + 1 + 3 = 6,\n\\] \\[\n\\mathbf{b} \\cdot \\mathbf{b} = 1^2 + 1^2 + 1^2 = 3.\n\\] Therefore, \\[\n\\text{proj}_{\\mathbf{b}}\\mathbf{a} = \\frac{6}{3}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = 2\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix}.\n\\]\nFirst, we compute: \\[\n\\mathbf{u}_{\\perp} = \\mathbf{a} - \\text{proj}_{\\mathbf{b}}\\mathbf{a} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}.\n\\] Now we check orthogonality: \\[\n\\mathbf{u}_{\\perp} \\cdot \\mathbf{b} = (0)(1) + (-1)(1) + (1)(1) = 0 - 1 + 1 = 0.\n\\] Since the dot product is zero, \\(\\mathbf{u}_{\\perp}\\) is indeed orthogonal to \\(\\mathbf{b}\\).\nWe have: \\[\n\\mathbf{u} \\cdot \\mathbf{e}_1 = u_1(1) + u_2(0) + u_3(0) = u_1,\n\\] \\[\n\\mathbf{e}_1 \\cdot \\mathbf{e}_1 = 1.\n\\] Therefore, \\[\n\\text{proj}_{\\mathbf{e}_1}\\mathbf{u} = \\frac{u_1}{1}\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} u_1 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n\\] The projection onto \\(\\mathbf{e}_1\\) simply extracts the first component of \\(\\mathbf{u}\\) and zeros out the other components. This makes sense geometrically: we’re finding the part of \\(\\mathbf{u}\\) that lies along the \\(x\\)-axis."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/06ex-functions-of-multiple-variables-part-2.html",
    "href": "teaching/multi-calc-sp-26/exercises/06ex-functions-of-multiple-variables-part-2.html",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Plot the graphs of the following functions using cross sections.\n\n\\(f(x,y) =-x^2 - y^2\\)\n\\(g(x,y) = y^3 + xy\\)\n\\(h(x,y) = \\sqrt{1-x^2-y^2}\\)\n\\(k(x,y) = e^{-x^2-y^2}\\)\n\\(m(x,y) = x^2\\)\n\nCheck your plots using Desmos.\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nDesmos\nDesmos\nDesmos\nDesmos\nDesmos\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot contour diagrams of the following functions.\n\n\\(f(x,y) =y-x^2\\)\n\\(g(x,y) = x^2+2y^2\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/06ex-functions-of-multiple-variables-part-2.html#exercise-1-plotting-using-cross-sections",
    "href": "teaching/multi-calc-sp-26/exercises/06ex-functions-of-multiple-variables-part-2.html#exercise-1-plotting-using-cross-sections",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Plot the graphs of the following functions using cross sections.\n\n\\(f(x,y) =-x^2 - y^2\\)\n\\(g(x,y) = y^3 + xy\\)\n\\(h(x,y) = \\sqrt{1-x^2-y^2}\\)\n\\(k(x,y) = e^{-x^2-y^2}\\)\n\\(m(x,y) = x^2\\)\n\nCheck your plots using Desmos.\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nDesmos\nDesmos\nDesmos\nDesmos\nDesmos"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/06ex-functions-of-multiple-variables-part-2.html#exercise-2-plotting-using-contour-diagrams",
    "href": "teaching/multi-calc-sp-26/exercises/06ex-functions-of-multiple-variables-part-2.html#exercise-2-plotting-using-contour-diagrams",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Plot contour diagrams of the following functions.\n\n\\(f(x,y) =y-x^2\\)\n\\(g(x,y) = x^2+2y^2\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html",
    "href": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall that the point-slope equation for a plane through a point \\((x_0,y_0,z_0)\\) with slope \\(m\\) in the positive \\(x\\)-direction and slope \\(n\\) in the positive \\(y\\)-direction is\n\\[\nz = m(x-x_0) + n(y-y_0) + z_0.\n\\]\n\n\n\n\n\n\n\nFind the point-slope equation for a plane through the point \\((2, -1, 4)\\) with slope \\(3\\) in the positive \\(x\\)-direction and slope \\(-2\\) in the positive \\(y\\)-direction.\nFind the point-slope equation for a plane through the origin with slope \\(1\\) in the positive \\(x\\)-direction and slope \\(5\\) in the positive \\(y\\)-direction.\nThe point-slope equation of a plane is \\[\nz = -2(x+3) + 4(y-1) + 7.\n\\] Through what point does this plane pass? What are the slopes in the \\(+x\\) and \\(+y\\) directions?\nThe point-slope equation of a plane is \\[\nz = 5y - 2.\n\\] Through what point does this plane pass? What are the slopes in the \\(+x\\) and \\(+y\\) directions?\nFind the point-slope equation for a plane through the point \\((1, 1, 1)\\) that is parallel to the \\(xy\\)-plane.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(z = 3(x-2) - 2(y+1) + 4\\)\n\\(z = x + 5y\\)\nThe plane passes through \\((-3, 1, 7)\\). The slope in the \\(+x\\) direction is \\(-2\\), and the slope in the \\(+y\\) direction is \\(4\\).\nThe plane passes through \\((0, 0, -2)\\), has slope \\(0\\) in the \\(+x\\) direction, and slope \\(5\\) in the \\(+y\\) direction.\nA plane parallel to the \\(xy\\)-plane has slope \\(0\\) in both the \\(+x\\) and \\(+y\\) directions. So the equation is \\(z = 0(x-1) + 0(y-1) + 1\\), or simply \\(z = 1\\).\n\n\n\n\n\n\n\nRecall that a vector \\(\\mathbf{n}\\) is called a normal vector to a plane if it is orthogonal to all vectors that lie in the plane. For a plane with point-slope equation\n\\[\nz = m(x-x_0) + n(y-y_0) + z_0,\n\\]\na normal vector is given by \\(\\mathbf{n} = \\begin{bmatrix} -m \\\\ -n \\\\ 1 \\end{bmatrix}\\).\n\n\n\n\n\n\n\nFind a normal vector to the plane \\(z = 3(x-2) - 2(y+1) + 4\\).\nFind a normal vector to the plane \\(z = x + 5y\\).\nFind a normal vector to the plane \\(z = -4x + 2y - 1\\).\nFind a normal vector to the plane \\(z = 7\\).\nFind a normal vector to the plane \\(y = 3\\).\nIs \\(\\mathbf{n} = \\begin{bmatrix} -6 \\\\ 4 \\\\ 2 \\end{bmatrix}\\) also a normal vector to the plane \\(z = 3(x-2) - 2(y+1) + 4\\)? Explain.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\mathbf{n} = \\begin{bmatrix} -3 \\\\ 2 \\\\ 1 \\end{bmatrix}\\)\n\\(\\mathbf{n} = \\begin{bmatrix} -1 \\\\ -5 \\\\ 1 \\end{bmatrix}\\)\n\\(\\mathbf{n} = \\begin{bmatrix} 4 \\\\ -2 \\\\ 1 \\end{bmatrix}\\)\n\\(\\mathbf{n} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\).\n\\(\\mathbf{n} = \\mathbf{e}_2\\).\nYes. Notice that \\(\\begin{bmatrix} -6 \\\\ 4 \\\\ 2 \\end{bmatrix} = 2\\begin{bmatrix} -3 \\\\ 2 \\\\ 1 \\end{bmatrix}\\). Any scalar multiple (except zero) of a normal vector is also a normal vector.\n\n\n\n\n\n\n\nRecall that the vector equation of a plane with normal vector \\(\\mathbf{n}\\) passing through the point with position vector \\(\\mathbf{r}_0\\) is\n\\[\n\\langle \\mathbf{n}, \\mathbf{r}-\\mathbf{r}_0 \\rangle = 0,\n\\]\nwhere \\(\\mathbf{r} = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}\\) is a variable position vector in \\(\\mathbb{R}^3\\).\n\n\n\n\n\n\n\nFind the point-slope equation of the plane with normal vector \\(\\mathbf{n} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix}\\) passing through the point \\((1, 0, -2)\\).\nFind the point-slope equation of the plane with normal vector \\(\\mathbf{n} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\) passing through the origin.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(z = -\\frac{2}{3}(x-1) + \\frac{1}{3}(y-0) - 2\\)\n\\(z = -x - y\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFind a point-slope equation of the plane passing through the points \\((1, 0, 2)\\), \\((2, 1, 3)\\), and \\((0, 1, 1)\\).\nFind a point-slope equation of the plane that contains the point \\((0, 1, 1)\\) and the \\(x\\)-axis.\nFind a plane that passes through the point \\((2, -1, 3)\\) and is perpendicular to the vector \\[\n\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}.\n\\]\nFind the point-slope equation of the plane that is parallel to the \\(xz\\)-plane and passes through the point \\((1, 3, 2)\\), if possible. If no such equation exists, explain why, and which equation you would use instead.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nLet \\[\n\\mathbf{r}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{r}_2 = \\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{r}_3 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}\n\\] be position vectors for the three points. Two vectors in the plane are \\[\n\\mathbf{v}_1 = \\mathbf{r}_2 - \\mathbf{r}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\mathbf{r}_3 - \\mathbf{r}_1 = \\begin{bmatrix} -1 \\\\ 1 \\\\ -1 \\end{bmatrix}.\n\\] We need a vector \\[\n\\mathbf{n} = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix}\n\\] orthogonal to both: \\[\n\\langle \\mathbf{n}, \\mathbf{v}_1 \\rangle = a + b + c = 0, \\quad \\langle \\mathbf{n}, \\mathbf{v}_2 \\rangle = -a + b - c = 0.\n\\] From the second equation, \\(b = a + c\\). Substituting into the first: \\(a + (a+c) + c = 0\\), so \\(2a + 2c = 0\\), giving \\(c = -a\\). Then \\(b = a + (-a) = 0\\). Choosing \\(a = 1\\), we get the normal vector \\[\n\\mathbf{n} = \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\end{bmatrix}.\n\\] Then, a point-slope equation of the plane is \\[\nz = (x-1) + 2.\n\\]\nA normal vector to the plane is \\[\n\\mathbf{n} = \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}.\n\\] Using this, we get a point-slope equation \\[\nz = (y-1) + 1\n\\]\nIf the plane is perpendicular to \\(\\mathbf{v}\\), then \\(\\mathbf{v}\\) is a normal vector. So \\[\n\\mathbf{n} = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}.\n\\] Then a point-slope equation of the plane is \\[\nz = (x-2) + 2(y+1) + 3\n\\]\nA plane parallel to the \\(xz\\)-plane has normal vector \\[\n\\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}.\n\\] But this can’t be written in the form \\[\n\\mathbf{n} = \\begin{bmatrix} -m \\\\ -n \\\\ 1 \\end{bmatrix},\n\\] so no point-slope equation exists for this plane. Instead, we use the standard form: \\[\ny-3 = 0.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe temperature \\(T\\) (in degrees Celsius) at a point \\((x, y, z)\\) in a room is modeled by a linear function. Measurements show that:\n\nAt the point \\((0, 0, 0)\\), the temperature is \\(20°C\\).\nAs you move 1 meter in the positive \\(x\\)-direction, the temperature increases by \\(2°C\\) per meter.\nAs you move 1 meter in the positive \\(y\\)-direction, the temperature decreases by \\(1°C\\) per meter.\nThe temperature does not change as you move in the \\(z\\)-direction.\n\nBecause of the third bullet point, the temperature is actually a function of the form \\(T = f(x,y)\\), independent of \\(z\\).\n\nFind a formula for \\(T = f(x, y)\\).\nWhat is the temperature at the point \\((3, 2, 5)\\)?\nFind all points where the temperature is exactly \\(25°C\\).\nDoes the set of solutions in part (c) form a plane? If so, find a normal vector.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nSince temperature changes linearly with position and increases at \\(2°C\\)/meter in the \\(+x\\) direction and decreases at \\(1°C\\)/meter in the \\(+y\\) direction, and doesn’t change with \\(z\\), we have \\[\nT = f(x,y)= 2x - y + 20.\n\\] (Since \\(f(0,0,0) = 20\\).)\n\\(T = f(3, 2) = 24°C\\).\nSetting \\(T = 25\\) and solving \\(f(x,y)=25\\), we get: \\[\n\\begin{align*}\n2x - y + 20 = 25\\\\\n2x - y = 5.\n\\end{align*}\n\\]\nYes, the set of solutions forms a plane. Note that the equation can be rewritten in the standard form \\[\n2(x-0) - (y+5)=0.\n\\] Thus, a normal vector is \\[\n\\mathbf{n} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 0 \\end{bmatrix}.\n\\]\n\n\n\n\n\n\n\nRecall that a standard equation of a plane in \\(\\mathbb{R}^3\\) passing through the point \\((x_0,y_0,z_0)\\) with normal vector \\[\n\\mathbf{n} = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix}\n\\] is given by \\[\na(x-x_0) + b(y-y_0) + c(z-z_0) = 0.\n\\]\n\n\n\n\n\n\n\nFind a standard equation of the plane with normal vector \\[\n\\mathbf{n} = \\begin{bmatrix} 2 \\\\ -3 \\\\ 1 \\end{bmatrix}\n\\] passing through the point \\((1, 2, -1)\\).\nA standard equation of a plane is \\[\n3(x-1) - 2(y+2) + 4z = 0.\n\\] Find a normal vector to this plane and identify a point on the plane.\nExpand the standard equation from part (a) to obtain an equation of the form \\(ax + by + cz = d\\).\nConsider the plane with point-slope equation \\(z = 2(x-1) - 3(y+1) + 4\\). Write this in standard form.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nUsing the formula with \\(a = 2\\), \\(b = -3\\), \\(c = 1\\) and the point \\((1, 2, -1)\\): \\[\n2(x-1) - 3(y-2) + (z+1) = 0.\n\\]\nThe normal vector is \\[\n\\mathbf{n} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 4 \\end{bmatrix}.\n\\] One point on the plane is \\((1, -2, 0)\\).\n\\(2x - 3y + z = -5\\)\n\\(2(x-1) - 3(y+1) - (z-4) = 0\\)\n\n\n\n\n\n\n\nRecall that an affine equation for a plane is given by\n\\[\nax + by + cz = d,\n\\]\nwhere \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are constants (and not all of \\(a\\), \\(b\\) and \\(c\\) are \\(0\\)).\n\n\n\n\n\n\n\nRewrite the point-slope equation \\[\nz = 2(x-1) - 3(y+1) + 4\n\\] as an affine equation.\nRewrite the affine equation \\[\n3x - 2y +4z = 5\n\\] as both a point-slope equation. What is the slope in the \\(+x\\)-direction? What is the slope in the \\(+y\\)-direction?\nRewrite the affine equation \\[\n2x + 4y = 8\n\\] as a point-slope equation, if possible. If it isn’t possible, explain why.\nFind a normal vector to the plane with affine equation \\[\n2x - y + 3z = 7.\n\\]\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(2x - 3y - z = 1\\)\nEquation: \\(z = - \\frac{3}{4}x + \\frac{1}{2}y + \\frac{5}{4}\\). Slope in \\(+x\\)-direction: \\(-\\frac{3}{4}\\). Slope in \\(+y\\)-direction: \\(\\frac{1}{2}\\).\n\\(2x + 4y = 8\\) cannot be rewritten as a point-slope equation because it does not define \\(z\\) as a function of \\(x\\) and \\(y\\).\nA normal vector to the plane is \\[\n\\mathbf{n} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose you are standing at the point \\((1, 2, 3)\\) on the plane with equation \\[\nz = 2(x-1) + 3(y-2) + 3.\n\\]\n\nIf you step \\(2\\) units in the \\(+x\\)-direction, what point are you at?\nIf you step 1 unit in the \\(-x\\)-direction, what point are you at?\nIf you step 3 units in the \\(-x\\)-direction and \\(4\\) in the \\(-y\\)-direction, what point are you at?\n\nSuppose you are standing at \\((1, -2, 0)\\) on the plane with equation \\[\n2(x-1) -4 (y+2) +2z = 0.\n\\]\n\nIf you step \\(1\\) unit in the \\(-x\\)-direction, what point are you at?\nIf you step \\(2\\) units in the \\(+y\\) direction, what point are you at?\n\nConsider the plane with equation \\[\n2x-3y + 3z = 5.\n\\] This plane has a unique normal vector that points in the positive \\(+z\\)-direction (i.e., the \\(z\\)-component of the normal vector is positive). If you begin at the point \\((1,0,1)\\) and step \\(2\\) units along this vector, what point are you at?\nConsider the plane with equation \\[\n2x + 3y + 4z = 1.\n\\] What is the point on this plane closest to the point \\((2,3,-1)\\)?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\((3, 2, 7)\\), \\((0, 2, 1)\\), \\((-2, -2, -15)\\)\n\\((0, -2, 1)\\), \\((1, 0, 4)\\)\nA normal vector with positive \\(z\\)-component is \\[\n\\mathbf{n} = \\begin{bmatrix}\n2 \\\\\n-3 \\\\\n3\n\\end{bmatrix}\n\\] Now, we want to step \\(2\\) units along this vector, and for this, it will be helpful to first normalize \\(\\mathbf{n}\\) to get a unit normal vector: \\[\n\\hat{\\mathbf{n}} = \\frac{\\mathbf{n}}{\\|\\mathbf{n}\\|} = \\frac{1}{\\sqrt{22}} \\begin{bmatrix} 2 \\\\ -3 \\\\ 3 \\end{bmatrix}.\n\\] So, if we begin at the point \\((1,0,1)\\) and step \\(2\\) units along \\(\\hat{\\mathbf{n}}\\), the position vector of our new point will be \\[\n\\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} + \\frac{2}{\\sqrt{22}} \\begin{bmatrix} 2 \\\\ -3 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 1 + \\frac{4}{\\sqrt{22}} \\\\ 0 - \\frac{6}{\\sqrt{22}} \\\\ 1 + \\frac{6}{\\sqrt{22}} \\end{bmatrix} \\approx \\begin{bmatrix} 1.85 \\\\ -1.28 \\\\ 2.28 \\end{bmatrix}.\n\\] Thus, the new point is approximately \\((1.85, -1.28, 2.28)\\).\nLet \\(\\mathbf{r}\\) be the position vector of the point \\((2,3,-1)\\), and let \\(\\mathbf{r}_0\\) be the position vector of the point \\((x_0,y_0,z_0)\\) on the plane closest to \\((2,3,-1)\\). Then it follows that the difference \\(\\mathbf{r}-\\mathbf{r_0}\\) is orthogonal to the plane. But a normal vector to the plane is \\[\n\\mathbf{n} = \\begin{bmatrix} 2 \\\\ 3 \\\\ 4 \\end{bmatrix},\n\\] so the difference \\(\\mathbf{r}-\\mathbf{r_0}\\) must be parallel to \\(\\mathbf{n}\\), which means there is a scalar \\(t\\) such that \\(\\mathbf{r}-\\mathbf{r_0} = t \\mathbf{n}\\). Then we have \\(\\mathbf{r}_0 = \\mathbf{r} - t \\mathbf{n}\\): \\[\n\\begin{bmatrix} x_0 \\\\ y_0 \\\\ z_0 \\end{bmatrix} = \\begin{bmatrix} 2 - 2t \\\\ 3 - 3t \\\\ -1 - 4t \\end{bmatrix}.\n\\] But since \\((x_0,y_0,z_0)\\) lies on the plane, it must be a solution to the equation of the plane: \\[\n2(2 - 2t) + 3(3 - 3t) + 4(-1 - 4t) = 1.\n\\] Solving for \\(t\\), we get \\(t = 8/29\\). Therefore, the point on the plane closest to \\((2,3,-1)\\) has position vector \\[\n\\mathbf{r_0} = \\mathbf{r} - \\frac{8}{29}\\mathbf{n} = \\begin{bmatrix} \\frac{42}{29} \\\\ \\frac{63}{29} \\\\ -\\frac{61}{29} \\end{bmatrix} \\approx\n\\begin{bmatrix} 1.45 \\\\ 2.17 \\\\ -2.10 \\end{bmatrix}. \\] Thus, the point on the plane closest to \\((2,3,-1)\\) is approximately \\((1.45, 2.17, -2.10)\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html#exercise-1-point-slope-equations-for-planes",
    "href": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html#exercise-1-point-slope-equations-for-planes",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall that the point-slope equation for a plane through a point \\((x_0,y_0,z_0)\\) with slope \\(m\\) in the positive \\(x\\)-direction and slope \\(n\\) in the positive \\(y\\)-direction is\n\\[\nz = m(x-x_0) + n(y-y_0) + z_0.\n\\]\n\n\n\n\n\n\n\nFind the point-slope equation for a plane through the point \\((2, -1, 4)\\) with slope \\(3\\) in the positive \\(x\\)-direction and slope \\(-2\\) in the positive \\(y\\)-direction.\nFind the point-slope equation for a plane through the origin with slope \\(1\\) in the positive \\(x\\)-direction and slope \\(5\\) in the positive \\(y\\)-direction.\nThe point-slope equation of a plane is \\[\nz = -2(x+3) + 4(y-1) + 7.\n\\] Through what point does this plane pass? What are the slopes in the \\(+x\\) and \\(+y\\) directions?\nThe point-slope equation of a plane is \\[\nz = 5y - 2.\n\\] Through what point does this plane pass? What are the slopes in the \\(+x\\) and \\(+y\\) directions?\nFind the point-slope equation for a plane through the point \\((1, 1, 1)\\) that is parallel to the \\(xy\\)-plane.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(z = 3(x-2) - 2(y+1) + 4\\)\n\\(z = x + 5y\\)\nThe plane passes through \\((-3, 1, 7)\\). The slope in the \\(+x\\) direction is \\(-2\\), and the slope in the \\(+y\\) direction is \\(4\\).\nThe plane passes through \\((0, 0, -2)\\), has slope \\(0\\) in the \\(+x\\) direction, and slope \\(5\\) in the \\(+y\\) direction.\nA plane parallel to the \\(xy\\)-plane has slope \\(0\\) in both the \\(+x\\) and \\(+y\\) directions. So the equation is \\(z = 0(x-1) + 0(y-1) + 1\\), or simply \\(z = 1\\)."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html#exercise-2-normal-vectors-to-planes",
    "href": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html#exercise-2-normal-vectors-to-planes",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall that a vector \\(\\mathbf{n}\\) is called a normal vector to a plane if it is orthogonal to all vectors that lie in the plane. For a plane with point-slope equation\n\\[\nz = m(x-x_0) + n(y-y_0) + z_0,\n\\]\na normal vector is given by \\(\\mathbf{n} = \\begin{bmatrix} -m \\\\ -n \\\\ 1 \\end{bmatrix}\\).\n\n\n\n\n\n\n\nFind a normal vector to the plane \\(z = 3(x-2) - 2(y+1) + 4\\).\nFind a normal vector to the plane \\(z = x + 5y\\).\nFind a normal vector to the plane \\(z = -4x + 2y - 1\\).\nFind a normal vector to the plane \\(z = 7\\).\nFind a normal vector to the plane \\(y = 3\\).\nIs \\(\\mathbf{n} = \\begin{bmatrix} -6 \\\\ 4 \\\\ 2 \\end{bmatrix}\\) also a normal vector to the plane \\(z = 3(x-2) - 2(y+1) + 4\\)? Explain.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(\\mathbf{n} = \\begin{bmatrix} -3 \\\\ 2 \\\\ 1 \\end{bmatrix}\\)\n\\(\\mathbf{n} = \\begin{bmatrix} -1 \\\\ -5 \\\\ 1 \\end{bmatrix}\\)\n\\(\\mathbf{n} = \\begin{bmatrix} 4 \\\\ -2 \\\\ 1 \\end{bmatrix}\\)\n\\(\\mathbf{n} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\).\n\\(\\mathbf{n} = \\mathbf{e}_2\\).\nYes. Notice that \\(\\begin{bmatrix} -6 \\\\ 4 \\\\ 2 \\end{bmatrix} = 2\\begin{bmatrix} -3 \\\\ 2 \\\\ 1 \\end{bmatrix}\\). Any scalar multiple (except zero) of a normal vector is also a normal vector."
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html#exercise-3-vector-equations-of-planes",
    "href": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html#exercise-3-vector-equations-of-planes",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall that the vector equation of a plane with normal vector \\(\\mathbf{n}\\) passing through the point with position vector \\(\\mathbf{r}_0\\) is\n\\[\n\\langle \\mathbf{n}, \\mathbf{r}-\\mathbf{r}_0 \\rangle = 0,\n\\]\nwhere \\(\\mathbf{r} = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}\\) is a variable position vector in \\(\\mathbb{R}^3\\).\n\n\n\n\n\n\n\nFind the point-slope equation of the plane with normal vector \\(\\mathbf{n} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix}\\) passing through the point \\((1, 0, -2)\\).\nFind the point-slope equation of the plane with normal vector \\(\\mathbf{n} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\) passing through the origin.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(z = -\\frac{2}{3}(x-1) + \\frac{1}{3}(y-0) - 2\\)\n\\(z = -x - y\\)"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html#exercise-4-finding-planes-from-geometric-information",
    "href": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html#exercise-4-finding-planes-from-geometric-information",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Find a point-slope equation of the plane passing through the points \\((1, 0, 2)\\), \\((2, 1, 3)\\), and \\((0, 1, 1)\\).\nFind a point-slope equation of the plane that contains the point \\((0, 1, 1)\\) and the \\(x\\)-axis.\nFind a plane that passes through the point \\((2, -1, 3)\\) and is perpendicular to the vector \\[\n\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}.\n\\]\nFind the point-slope equation of the plane that is parallel to the \\(xz\\)-plane and passes through the point \\((1, 3, 2)\\), if possible. If no such equation exists, explain why, and which equation you would use instead.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nLet \\[\n\\mathbf{r}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{r}_2 = \\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{r}_3 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}\n\\] be position vectors for the three points. Two vectors in the plane are \\[\n\\mathbf{v}_1 = \\mathbf{r}_2 - \\mathbf{r}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\mathbf{r}_3 - \\mathbf{r}_1 = \\begin{bmatrix} -1 \\\\ 1 \\\\ -1 \\end{bmatrix}.\n\\] We need a vector \\[\n\\mathbf{n} = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix}\n\\] orthogonal to both: \\[\n\\langle \\mathbf{n}, \\mathbf{v}_1 \\rangle = a + b + c = 0, \\quad \\langle \\mathbf{n}, \\mathbf{v}_2 \\rangle = -a + b - c = 0.\n\\] From the second equation, \\(b = a + c\\). Substituting into the first: \\(a + (a+c) + c = 0\\), so \\(2a + 2c = 0\\), giving \\(c = -a\\). Then \\(b = a + (-a) = 0\\). Choosing \\(a = 1\\), we get the normal vector \\[\n\\mathbf{n} = \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\end{bmatrix}.\n\\] Then, a point-slope equation of the plane is \\[\nz = (x-1) + 2.\n\\]\nA normal vector to the plane is \\[\n\\mathbf{n} = \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}.\n\\] Using this, we get a point-slope equation \\[\nz = (y-1) + 1\n\\]\nIf the plane is perpendicular to \\(\\mathbf{v}\\), then \\(\\mathbf{v}\\) is a normal vector. So \\[\n\\mathbf{n} = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}.\n\\] Then a point-slope equation of the plane is \\[\nz = (x-2) + 2(y+1) + 3\n\\]\nA plane parallel to the \\(xz\\)-plane has normal vector \\[\n\\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}.\n\\] But this can’t be written in the form \\[\n\\mathbf{n} = \\begin{bmatrix} -m \\\\ -n \\\\ 1 \\end{bmatrix},\n\\] so no point-slope equation exists for this plane. Instead, we use the standard form: \\[\ny-3 = 0.\n\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html#exercise-5-temperature-distribution",
    "href": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html#exercise-5-temperature-distribution",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "The temperature \\(T\\) (in degrees Celsius) at a point \\((x, y, z)\\) in a room is modeled by a linear function. Measurements show that:\n\nAt the point \\((0, 0, 0)\\), the temperature is \\(20°C\\).\nAs you move 1 meter in the positive \\(x\\)-direction, the temperature increases by \\(2°C\\) per meter.\nAs you move 1 meter in the positive \\(y\\)-direction, the temperature decreases by \\(1°C\\) per meter.\nThe temperature does not change as you move in the \\(z\\)-direction.\n\nBecause of the third bullet point, the temperature is actually a function of the form \\(T = f(x,y)\\), independent of \\(z\\).\n\nFind a formula for \\(T = f(x, y)\\).\nWhat is the temperature at the point \\((3, 2, 5)\\)?\nFind all points where the temperature is exactly \\(25°C\\).\nDoes the set of solutions in part (c) form a plane? If so, find a normal vector.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nSince temperature changes linearly with position and increases at \\(2°C\\)/meter in the \\(+x\\) direction and decreases at \\(1°C\\)/meter in the \\(+y\\) direction, and doesn’t change with \\(z\\), we have \\[\nT = f(x,y)= 2x - y + 20.\n\\] (Since \\(f(0,0,0) = 20\\).)\n\\(T = f(3, 2) = 24°C\\).\nSetting \\(T = 25\\) and solving \\(f(x,y)=25\\), we get: \\[\n\\begin{align*}\n2x - y + 20 = 25\\\\\n2x - y = 5.\n\\end{align*}\n\\]\nYes, the set of solutions forms a plane. Note that the equation can be rewritten in the standard form \\[\n2(x-0) - (y+5)=0.\n\\] Thus, a normal vector is \\[\n\\mathbf{n} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 0 \\end{bmatrix}.\n\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html#exercise-6-standard-equations-of-planes",
    "href": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html#exercise-6-standard-equations-of-planes",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall that a standard equation of a plane in \\(\\mathbb{R}^3\\) passing through the point \\((x_0,y_0,z_0)\\) with normal vector \\[\n\\mathbf{n} = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix}\n\\] is given by \\[\na(x-x_0) + b(y-y_0) + c(z-z_0) = 0.\n\\]\n\n\n\n\n\n\n\nFind a standard equation of the plane with normal vector \\[\n\\mathbf{n} = \\begin{bmatrix} 2 \\\\ -3 \\\\ 1 \\end{bmatrix}\n\\] passing through the point \\((1, 2, -1)\\).\nA standard equation of a plane is \\[\n3(x-1) - 2(y+2) + 4z = 0.\n\\] Find a normal vector to this plane and identify a point on the plane.\nExpand the standard equation from part (a) to obtain an equation of the form \\(ax + by + cz = d\\).\nConsider the plane with point-slope equation \\(z = 2(x-1) - 3(y+1) + 4\\). Write this in standard form.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nUsing the formula with \\(a = 2\\), \\(b = -3\\), \\(c = 1\\) and the point \\((1, 2, -1)\\): \\[\n2(x-1) - 3(y-2) + (z+1) = 0.\n\\]\nThe normal vector is \\[\n\\mathbf{n} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 4 \\end{bmatrix}.\n\\] One point on the plane is \\((1, -2, 0)\\).\n\\(2x - 3y + z = -5\\)\n\\(2(x-1) - 3(y+1) - (z-4) = 0\\)"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html#exercise-7-affine-equations-of-planes",
    "href": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html#exercise-7-affine-equations-of-planes",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Recall that an affine equation for a plane is given by\n\\[\nax + by + cz = d,\n\\]\nwhere \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are constants (and not all of \\(a\\), \\(b\\) and \\(c\\) are \\(0\\)).\n\n\n\n\n\n\n\nRewrite the point-slope equation \\[\nz = 2(x-1) - 3(y+1) + 4\n\\] as an affine equation.\nRewrite the affine equation \\[\n3x - 2y +4z = 5\n\\] as both a point-slope equation. What is the slope in the \\(+x\\)-direction? What is the slope in the \\(+y\\)-direction?\nRewrite the affine equation \\[\n2x + 4y = 8\n\\] as a point-slope equation, if possible. If it isn’t possible, explain why.\nFind a normal vector to the plane with affine equation \\[\n2x - y + 3z = 7.\n\\]\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(2x - 3y - z = 1\\)\nEquation: \\(z = - \\frac{3}{4}x + \\frac{1}{2}y + \\frac{5}{4}\\). Slope in \\(+x\\)-direction: \\(-\\frac{3}{4}\\). Slope in \\(+y\\)-direction: \\(\\frac{1}{2}\\).\n\\(2x + 4y = 8\\) cannot be rewritten as a point-slope equation because it does not define \\(z\\) as a function of \\(x\\) and \\(y\\).\nA normal vector to the plane is \\[\n\\mathbf{n} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix}.\n\\]"
  },
  {
    "objectID": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html#exercise-8-more-practice-with-planes",
    "href": "teaching/multi-calc-sp-26/exercises/03ex-planes-and-linear-spaces-part-1.html#exercise-8-more-practice-with-planes",
    "title": "Multivariable calculus: Exercises",
    "section": "",
    "text": "Suppose you are standing at the point \\((1, 2, 3)\\) on the plane with equation \\[\nz = 2(x-1) + 3(y-2) + 3.\n\\]\n\nIf you step \\(2\\) units in the \\(+x\\)-direction, what point are you at?\nIf you step 1 unit in the \\(-x\\)-direction, what point are you at?\nIf you step 3 units in the \\(-x\\)-direction and \\(4\\) in the \\(-y\\)-direction, what point are you at?\n\nSuppose you are standing at \\((1, -2, 0)\\) on the plane with equation \\[\n2(x-1) -4 (y+2) +2z = 0.\n\\]\n\nIf you step \\(1\\) unit in the \\(-x\\)-direction, what point are you at?\nIf you step \\(2\\) units in the \\(+y\\) direction, what point are you at?\n\nConsider the plane with equation \\[\n2x-3y + 3z = 5.\n\\] This plane has a unique normal vector that points in the positive \\(+z\\)-direction (i.e., the \\(z\\)-component of the normal vector is positive). If you begin at the point \\((1,0,1)\\) and step \\(2\\) units along this vector, what point are you at?\nConsider the plane with equation \\[\n2x + 3y + 4z = 1.\n\\] What is the point on this plane closest to the point \\((2,3,-1)\\)?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\((3, 2, 7)\\), \\((0, 2, 1)\\), \\((-2, -2, -15)\\)\n\\((0, -2, 1)\\), \\((1, 0, 4)\\)\nA normal vector with positive \\(z\\)-component is \\[\n\\mathbf{n} = \\begin{bmatrix}\n2 \\\\\n-3 \\\\\n3\n\\end{bmatrix}\n\\] Now, we want to step \\(2\\) units along this vector, and for this, it will be helpful to first normalize \\(\\mathbf{n}\\) to get a unit normal vector: \\[\n\\hat{\\mathbf{n}} = \\frac{\\mathbf{n}}{\\|\\mathbf{n}\\|} = \\frac{1}{\\sqrt{22}} \\begin{bmatrix} 2 \\\\ -3 \\\\ 3 \\end{bmatrix}.\n\\] So, if we begin at the point \\((1,0,1)\\) and step \\(2\\) units along \\(\\hat{\\mathbf{n}}\\), the position vector of our new point will be \\[\n\\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} + \\frac{2}{\\sqrt{22}} \\begin{bmatrix} 2 \\\\ -3 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 1 + \\frac{4}{\\sqrt{22}} \\\\ 0 - \\frac{6}{\\sqrt{22}} \\\\ 1 + \\frac{6}{\\sqrt{22}} \\end{bmatrix} \\approx \\begin{bmatrix} 1.85 \\\\ -1.28 \\\\ 2.28 \\end{bmatrix}.\n\\] Thus, the new point is approximately \\((1.85, -1.28, 2.28)\\).\nLet \\(\\mathbf{r}\\) be the position vector of the point \\((2,3,-1)\\), and let \\(\\mathbf{r}_0\\) be the position vector of the point \\((x_0,y_0,z_0)\\) on the plane closest to \\((2,3,-1)\\). Then it follows that the difference \\(\\mathbf{r}-\\mathbf{r_0}\\) is orthogonal to the plane. But a normal vector to the plane is \\[\n\\mathbf{n} = \\begin{bmatrix} 2 \\\\ 3 \\\\ 4 \\end{bmatrix},\n\\] so the difference \\(\\mathbf{r}-\\mathbf{r_0}\\) must be parallel to \\(\\mathbf{n}\\), which means there is a scalar \\(t\\) such that \\(\\mathbf{r}-\\mathbf{r_0} = t \\mathbf{n}\\). Then we have \\(\\mathbf{r}_0 = \\mathbf{r} - t \\mathbf{n}\\): \\[\n\\begin{bmatrix} x_0 \\\\ y_0 \\\\ z_0 \\end{bmatrix} = \\begin{bmatrix} 2 - 2t \\\\ 3 - 3t \\\\ -1 - 4t \\end{bmatrix}.\n\\] But since \\((x_0,y_0,z_0)\\) lies on the plane, it must be a solution to the equation of the plane: \\[\n2(2 - 2t) + 3(3 - 3t) + 4(-1 - 4t) = 1.\n\\] Solving for \\(t\\), we get \\(t = 8/29\\). Therefore, the point on the plane closest to \\((2,3,-1)\\) has position vector \\[\n\\mathbf{r_0} = \\mathbf{r} - \\frac{8}{29}\\mathbf{n} = \\begin{bmatrix} \\frac{42}{29} \\\\ \\frac{63}{29} \\\\ -\\frac{61}{29} \\end{bmatrix} \\approx\n\\begin{bmatrix} 1.45 \\\\ 2.17 \\\\ -2.10 \\end{bmatrix}. \\] Thus, the point on the plane closest to \\((2,3,-1)\\) is approximately \\((1.45, 2.17, -2.10)\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/02-separation-of-var.html#a-theorem-from-odes",
    "href": "teaching/pde-sp-26/slides/02-separation-of-var.html#a-theorem-from-odes",
    "title": "02 Separation of variables",
    "section": "A theorem from ODEs",
    "text": "A theorem from ODEs\n\n\n\n\nTheorem (General solutions to a special ODE).\n\n\nLet \\(f\\) be a function with continuous first and second derivatives. Consider the differential equation \\[\nf'' + af' + bf = 0,\n\\] where \\(a\\) and \\(b\\) are constants. If \\(r_1\\) and \\(r_2\\) are the roots of the characteristic equation \\[\nr^2 + ar + b = 0,\n\\]\nthen the general solution to the ODE is\n\\[\nf( x ) = C_1 e^{r_1 x} + C_2 e^{r_2 x},\n\\]\nwhere \\(C_1\\) and \\(C_2\\) are arbitrary constants determined by the initial and boundary conditions.\n\n\n\n\n\nThis ODE is called homogeneous because it has no terms that are functions of \\(x\\) alone, i.e., the right-hand side is zero.\nThis ODE is called linear because the differential operator on the left, \\[\n  L(f) = f'' + af' + bf,\n  \\] preserves linear combinations, i.e., we have \\(L(f+g)= L(f) + L(g)\\) and \\(L(cf) = cL(f)\\), for all functions \\(f\\) and \\(g\\) and any scalar \\(c\\).\nThis ODE is called second-order because the highest derivative that appears is the second derivative, \\(f''\\).\nThis ODE is called constant-coefficient because the coefficients \\(a\\) and \\(b\\) are constants, not functions of \\(x\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/02-separation-of-var.html#complex-roots-to-the-characteristic-equation",
    "href": "teaching/pde-sp-26/slides/02-separation-of-var.html#complex-roots-to-the-characteristic-equation",
    "title": "02 Separation of variables",
    "section": "Complex roots to the characteristic equation",
    "text": "Complex roots to the characteristic equation\n\nThe roots \\(r_1\\) and \\(r_2\\) might be complex! In this case, they are conjugates of each other: \\[\n  r_1 = \\alpha + i\\beta, \\quad r_2 = \\alpha - i\\beta\n  \\] Thus:\n\n\n\n\n\nConvenient expression for the general solutions\n\n\nThe general solution can be written as \\[\nf(x) = e^{\\alpha x} \\left( C_1 \\sin(\\beta x) + C_2 \\cos(\\beta x) \\right),\n\\] where \\(C_1\\) and \\(C_2\\) are arbitrary constants."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/02-separation-of-var.html#separation-of-variables-part-1",
    "href": "teaching/pde-sp-26/slides/02-separation-of-var.html#separation-of-variables-part-1",
    "title": "02 Separation of variables",
    "section": "Separation of variables, part 1",
    "text": "Separation of variables, part 1\n\nRecall that the heat equation is \\(\\displaystyle\\frac{\\partial u}{\\partial t} = k \\frac{\\partial^2 u}{\\partial x^2}\\) where \\(k\\) is a positive (known) constant.\nWe assume the boundary conditions \\[\nu(0,t) = 0, \\quad u(L,t) = 0, \\quad t &gt; 0,\n\\] where \\(L\\) is some positive (known) constant.\nWe ignore initial conditions for the moment.\n\n\n\n\n\nThe “separation of variables” strategy\n\n\n\nWe look for solutions of the form \\[\nu(x,t) = X(x)T(t),\n\\] where \\(X\\) is a function of \\(x\\) alone and \\(T\\) is a function of \\(t\\) alone.\n\n\n\n\n\n\nSubstituting \\(u(x,t) = X(x)T(t)\\) into the heat equation gives \\[\nX(x) T'(t) = k T(t) X''(x),\n\\] along with \\[\nX(0) = 0, \\quad X(L) = 0.\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/02-separation-of-var.html#separation-of-variables-part-2",
    "href": "teaching/pde-sp-26/slides/02-separation-of-var.html#separation-of-variables-part-2",
    "title": "02 Separation of variables",
    "section": "Separation of variables, part 2",
    "text": "Separation of variables, part 2\n\nDividing both sides of the equation by \\(k X(x) T(t)\\) gives \\[\n\\frac{T'(t)}{k T(t)} = \\frac{X''(x)}{X(x)}.\n\\]\nNotice that the left-hand side depends only on \\(t\\) and the right-hand side depends only on \\(x\\). Therefore, both sides must be equal to a constant, which we denote by \\(-\\lambda\\): \\[\n\\frac{T'(t)}{k T(t)} = \\frac{X''(x)}{X(x)} = -\\lambda.\n\\]\nThis leads to a system of two ODEs: \\[\nT'(t) = - k \\lambda T(t) \\tag{1}\n\\] and \\[\nX''(x) + \\lambda X(x) = 0, \\quad X(0) = 0, \\quad X(L) = 0, \\tag{2}\n\\]\nThe general solution to (1) is \\[\nT(t) = C_0 \\exp\\left(-k \\lambda t\\right),\n\\] where \\(C_0\\) is an arbitrary constant.\nBased on our knowledge of ODEs (from the first slide), the general solution to (2) is \\[\nX(x) = C_1 \\sin(\\sqrt{\\lambda} x) + C_2 \\cos(\\sqrt{\\lambda} x),\n\\] where \\(C_1\\) and \\(C_2\\) are arbitrary constants."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/02-separation-of-var.html#separation-of-variables-part-3",
    "href": "teaching/pde-sp-26/slides/02-separation-of-var.html#separation-of-variables-part-3",
    "title": "02 Separation of variables",
    "section": "Separation of variables, part 3",
    "text": "Separation of variables, part 3\n\nBut now the boundary condition \\(X(0)=0\\) forces \\(C_2 = 0\\), so \\[\nX(x) = C_1 \\sin(\\sqrt{\\lambda} x).\n\\]\nThe boundary condition \\(X(L)=0\\) then gives \\[\nC_1 \\sin(\\sqrt{\\lambda} L) = 0.\n\\]\nSince \\(C_1 \\neq 0\\) (otherwise the solution is trivial), we must have \\[\n\\sin(\\sqrt{\\lambda} L) = 0,\n\\] which implies \\[\n\\sqrt{\\lambda} L = n \\pi, \\quad n = 0, \\pm 1, \\pm 2, \\pm3, \\dots\n\\]\nThus, \\[\n\\lambda = \\left( \\frac{n \\pi}{L} \\right)^2.\n\\]\nTherefore, for each \\(n=1,2,3,\\dots\\), we have a solution \\[\nX_n(x) = \\sin\\left( \\frac{n \\pi x}{L} \\right)\n\\] to (2). (We ignore the negative values of \\(n\\)—why?)"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/02-separation-of-var.html#basic-solutions-to-the-heat-equation",
    "href": "teaching/pde-sp-26/slides/02-separation-of-var.html#basic-solutions-to-the-heat-equation",
    "title": "02 Separation of variables",
    "section": "“Basic solutions” to the heat equation",
    "text": "“Basic solutions” to the heat equation\n\nIf we remember that \\(u(x,t) = X(x)T(t)\\), then for each \\(n=1,2,3,\\dots\\), we have a “basic solution” to the heat equation:\n\n\n\n\n\nBasic solutions to the heat equation (w/zero temperature at the boundaries)\n\n\nConsider the heat equation \\[\n\\frac{\\partial u}{\\partial t} = k \\frac{\\partial^2 u}{\\partial x^2}\n\\] with boundary conditions \\(u(0,t) = 0\\) and \\(u(L,t) = 0\\) for all \\(t&gt;0\\). Then for each \\(n=1,2,3,\\dots\\), the function \\[\nu_n(x,t) = \\exp\\left(\\frac{-n^2 \\pi^2 kt}{L^2} \\right) \\sin\\left( \\frac{n \\pi x}{L} \\right), \\quad 0 \\leq x \\leq L, \\ t\\geq 0\n\\] is a solution to the boundary-value problem above."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/02-separation-of-var.html#more-complex-solutions-superposition-principle",
    "href": "teaching/pde-sp-26/slides/02-separation-of-var.html#more-complex-solutions-superposition-principle",
    "title": "02 Separation of variables",
    "section": "More complex solutions: Superposition principle",
    "text": "More complex solutions: Superposition principle\n\nHow might we build up more “complex” solutions using the basic equations to the heat equation?\n\n\n\n\n\nTheorem (Superposition principle for the heat equation).\n\n\nIf \\(u_1(x,t), u_2(x,t), \\dots, u_n(x,t)\\) are solutions to the heat equation, then any linear combination \\[\nu(x,t) = a_1 u_1(x,t) + a_2 u_2(x,t) + \\dots + a_n u_n(x,t)\n\\] is also a solution, where \\(a_1, a_2, \\dots, a_n\\) are constants.\n\n\n\n\n\nThought: If linear combinations of finitely many solutions are also solutions, what about infinite sums?\nIf \\(u_1(x,t), u_2(x,t), \\dots\\) are solutions to the heat equation, then perhaps the infinite sum \\[\nu(x,t) = \\sum_{n=1}^{\\infty} a_n u_n(x,t)\n\\] is also a solution, where \\(a_n\\) are constants.\nIn particular, if we take the basic solutions \\[\nu_n(x,t) = \\exp\\left(\\frac{-n^2 \\pi^2 kt}{L^2} \\right) \\sin\\left( \\frac{n \\pi x}{L} \\right),\n\\] to the heat equation, then is the infinite sum \\[\nu(x,t) = \\sum_{n=1}^{\\infty} a_n \\exp\\left(\\frac{-n^2 \\pi^2 kt}{L^2} \\right) \\sin\\left( \\frac{n \\pi x}{L} \\right)\n\\] also a solution? (Even more pressing—what about convergence of the series?)"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/02-separation-of-var.html#bringing-in-the-initial-condition",
    "href": "teaching/pde-sp-26/slides/02-separation-of-var.html#bringing-in-the-initial-condition",
    "title": "02 Separation of variables",
    "section": "Bringing in the initial condition",
    "text": "Bringing in the initial condition\n\nIf we now bring in an initial condition specifying that \\[\nu(x,0) = f(x), \\quad 0 \\leq x \\leq L,\n\\] for some initial temperature distribution \\(f(x)\\), then our “infinite superposition” solution must satisfy \\[\nf(x) = \\sum_{n=1}^{\\infty} a_n \\sin\\left( \\frac{n \\pi x}{L} \\right), \\quad 0 \\leq x \\leq L.\n\\]\nBut for what functions \\(f(x)\\) does this series converge? What are the \\(a_n\\)’s?"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/02-separation-of-var.html#conclusion-open-questions",
    "href": "teaching/pde-sp-26/slides/02-separation-of-var.html#conclusion-open-questions",
    "title": "02 Separation of variables",
    "section": "Conclusion: Open questions",
    "text": "Conclusion: Open questions\n\n\n\n\nQuetions raised by separation of variables\n\n\n\nAssuming it converges, is an infinite linear combination of solutions to the heat equation also a solution?\nFor what types of functions \\(f(x)\\) can we write \\[\nf(x) = \\sum_{n=1}^{\\infty} a_n \\sin\\left( nx \\right), \\quad (0 \\leq x \\leq \\pi)?\n\\] (This is just a “rescaled” version of the series on the previous slide.)\n\nWhen does it converge? What type of convergence?\nHow do we determine the coefficients \\(a_n\\)?"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/04-general-pdes-and-boundary-conditions.html#domains-of-pdes",
    "href": "teaching/pde-sp-26/slides/04-general-pdes-and-boundary-conditions.html#domains-of-pdes",
    "title": "04 General PDEs and boundary conditions",
    "section": "Domains of PDEs",
    "text": "Domains of PDEs\n\nSolutions to PDEs are functions, often of the form \\[\nu(x,t),\\quad u(x,y,t), \\quad u(x,y,z,t), \\quad u(x,y,z), \\ldots.\n\\]\nPDEs are only considered over a certain domain \\(D\\), given to us as part of the problem statement.\nIf \\(u\\) is “time-dependent”, this domain consists of a “spatial slice” that is extended through time."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/04-general-pdes-and-boundary-conditions.html#exercise-1-domains-of-pdes",
    "href": "teaching/pde-sp-26/slides/04-general-pdes-and-boundary-conditions.html#exercise-1-domains-of-pdes",
    "title": "04 General PDEs and boundary conditions",
    "section": "Exercise 1: Domains of PDEs",
    "text": "Exercise 1: Domains of PDEs\n\n\n\na. Domains of the heat equation\n\n\nFor each of the following heat equation scenarios, describe the domain of the PDE, including its dimension. If the solutions are time-dependent, identify the spatial slices of the domain and the initial spatial slice.\n\nThe temperature \\(u\\) at a certain position and time of a metal rod of length \\(L\\).\nThe temperature \\(u\\) at a certain position and time of a metal plate of width \\(x_0\\) and height \\(y_0\\).\nThe temperature \\(u\\) at a certain position and time of a solid metal sphere of radius \\(r\\).\n\n\n\n\n\n\n\nb. Domains of the wave equation\n\n\nFor each of the following wave equation scenarios, describe the domain of the PDE, including its dimension. If the solutions are time-dependent, identify the spatial slices of the domain and the initial spatial slice.\n\nThe displacement \\(u\\) at a certain position and time of a vibrating string of length \\(L\\).\nThe displacement \\(u\\) at a certain position and time of a vibrating drum of radius \\(r\\).\n\n\n\n\n\n\n\nc. Domains of Laplace’s equation\n\n\nFor each of the following Laplace’s equation scenarios, describe the domain of the PDE, including its dimension. If the solutions are time-dependent, identify the spatial slices of the domain and the initial spatial slice.\n\nThe steady-state temperature \\(u\\) at a certain position of a metal plate of width \\(x_0\\) and height \\(y_0\\). (Hint: “steady-state” means time-independent.)\nThe steady-state temperature \\(u\\) at a certain position of a solid metal sphere of radius \\(r\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/04-general-pdes-and-boundary-conditions.html#linear-differential-operators",
    "href": "teaching/pde-sp-26/slides/04-general-pdes-and-boundary-conditions.html#linear-differential-operators",
    "title": "04 General PDEs and boundary conditions",
    "section": "Linear differential operators",
    "text": "Linear differential operators\n\nRecall (from the exercises!) that a linear differential operator \\(L\\) is something of the form \\[\nL(u) = au + \\sum_{i=1}^n b_i \\frac{\\partial u}{\\partial x_i} + \\sum_{i,j=1}^n c_{ij}\\frac{\\partial^2 u}{\\partial x_i \\partial x_j} + \\cdots,\n\\] where the sum only includes finitely many terms, and the coefficients \\(a, b_i, c_{ij}, \\ldots\\) are functions only of the independent variables.\nFor example, the Laplacian is such an operator: \\[\nL(u) = \\nabla^2 u = \\sum_{i=1}^n \\frac{\\partial^2 u}{\\partial x_i^2}.\n\\] If we set \\(L(u)=0\\), we get Laplace’s equation.\nFor another example, the operator \\[\nL(u) = \\frac{\\partial u}{\\partial t} - k \\nabla^2 u,\n\\] is linear. If we set \\(L(u)=0\\), we get the heat equation.\nFor a third example, the operator \\[\nL(u) = \\frac{\\partial^2 u}{\\partial t^2} - a^2 \\nabla^2 u,\n\\] is linear. If we set \\(L(u)=0\\), we get the wave equation."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/04-general-pdes-and-boundary-conditions.html#linear-pdes",
    "href": "teaching/pde-sp-26/slides/04-general-pdes-and-boundary-conditions.html#linear-pdes",
    "title": "04 General PDEs and boundary conditions",
    "section": "Linear PDEs",
    "text": "Linear PDEs\n\n\n\n\nLinear PDEs\n\n\nA linear partial differential equation consists of the following:\n\nAn equation \\(L(u) = f\\), where \\(L\\) is a linear differential operator and \\(f\\) is some function of the independent variables (but not of \\(u\\) or its derivatives).\nA specification of the domain \\(D\\) over which the equation is considered, i.e., the domains of the solution \\(u\\) and the function \\(f\\).\n\n\n\n\n\n\nIf \\(f=0\\), the PDE is called homogeneous. If \\(f \\neq 0\\), the PDE is called non-homogeneous.\n\n\n\n\n\nTheorem (Superposition principle for homogeneous linear PDEs).\n\n\nIf \\(u_1, u_2, \\dots, u_n\\) are solutions to a homogeneous linear PDE, then any linear combination \\[\nu = a_1 u_1 + a_2 u_2 + \\dots + a_n u_n\n\\] is also a solution, where \\(a_1, a_2, \\dots, a_n\\) are constants."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/04-general-pdes-and-boundary-conditions.html#exercise-2-solutions-to-non-homogeneous-linear-pdes",
    "href": "teaching/pde-sp-26/slides/04-general-pdes-and-boundary-conditions.html#exercise-2-solutions-to-non-homogeneous-linear-pdes",
    "title": "04 General PDEs and boundary conditions",
    "section": "Exercise 2: Solutions to non-homogeneous linear PDEs",
    "text": "Exercise 2: Solutions to non-homogeneous linear PDEs\n\n\n\nLet \\(L(u)=f\\) be a non-homogeneous linear PDE, and let \\(u_p\\) be a particular solution to this PDE (i.e., \\(L(u_p)=f\\)).\n\nLet \\(u_h\\) be a solution to the corresponding homogeneous PDE (i.e., \\(L(u_h)=0\\)). Show that \\(u = u_p + u_h\\) is also a solution to the non-homogeneous PDE.\nShow that if \\(u\\) is any solution to the non-homogeneous PDE, then \\(u - u_p\\) is a solution to the homogeneous PDE.\nConclude that any solution \\(u\\) to the non-homogeneous PDE can be expressed as \\[\nu = u_p + u_h,\n\\] where \\(u_h\\) is a solution to the homogeneous PDE."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/04-general-pdes-and-boundary-conditions.html#boundary-and-initial-conditions",
    "href": "teaching/pde-sp-26/slides/04-general-pdes-and-boundary-conditions.html#boundary-and-initial-conditions",
    "title": "04 General PDEs and boundary conditions",
    "section": "Boundary and initial conditions",
    "text": "Boundary and initial conditions\n\nConsider a linear PDE \\(L(u) = f\\) defined over a fixed domain \\(D\\).\n\n\n\n\n\nBoundary and initial conditions\n\n\n\nSuppose first \\(u\\) is time-independent:\n\nA boundary condition is a specification of the values of the solution \\(u\\) (and possibly its derivatives) on the boundary of \\(D\\).\n\nIf \\(u\\) is time-dependent:\n\nA boundary condition is a specification of the values of the solution \\(u\\) (and possibly its derivatives) on the boundary of the spatial slices of \\(D\\).\nAn initial condition is a specification of the values of the solution \\(u\\) (and possibly its derivatives) on the initial spatial slice of \\(D\\).\n\n\n\n\n\n\n\n\n\n\nTypes of boundary conditions\n\n\n\nSuppose first \\(u\\) is time-independent:\n\nDirichlet boundary condition: specifies the value of \\(u\\) on the boundary of \\(D\\).\nNeumann boundary condition: specifies the value of the normal derivative of \\(u\\) on the boundary of \\(D\\).\n\nIf \\(u\\) is time-dependent:\n\nDirichlet boundary condition: specifies the value of \\(u\\) on the boundary of the spatial slices of \\(D\\).\nNeumann boundary condition: specifies the value of the normal derivative of \\(u\\) on the boundary of the spatial slices of \\(D\\).\n\n\n\n\n\n\n\n\n\n\nBoundary and initial value problems\n\n\n\nA boundary value problem is a linear PDE together with a specification of boundary conditions.\nAn initial value problem is a linear PDE together with a specification of initial conditions."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/04-general-pdes-and-boundary-conditions.html#exercise-3-boundary-and-initial-conditions",
    "href": "teaching/pde-sp-26/slides/04-general-pdes-and-boundary-conditions.html#exercise-3-boundary-and-initial-conditions",
    "title": "04 General PDEs and boundary conditions",
    "section": "Exercise 3: Boundary and initial conditions",
    "text": "Exercise 3: Boundary and initial conditions\n\n\n\nFor each of the following scenarios, identify the type of PDE (e.g., heat equation, wave equation, Laplace’s equation), whether the solution are time-dependent or time-independent, the domain \\(D\\) of the problem, and describe the boundary and/or initial conditions that would be appropriate for the scenario.\n\nThe temperature \\(u\\) at a certain position and time of a metal rod of length \\(L\\) that is insulated at both ends and is known to have an initial temperature profile.\nThe displacement \\(u\\) at a certain position and time of a vibrating string of length \\(L\\) that is fixed at both ends, given that the string is initially at rest but has an initial displacement profile.\nThe steady-state temperature \\(u\\) at a certain position of a metal plate of width \\(x_0\\) and height \\(y_0\\) that is held at a constant temperature along its boundary and has an initial temperature distribution."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/03-the-laplacian-and-laplaces-equation.html#gradient",
    "href": "teaching/pde-sp-26/slides/03-the-laplacian-and-laplaces-equation.html#gradient",
    "title": "03 The Laplacian and Laplace’s equation",
    "section": "Gradient",
    "text": "Gradient\n\n\n\n\nGradient operator in two dimensions\n\n\nLet \\(u:\\mathbb{R}^2\\to \\mathbb{R}\\) be a differentiable function. The gradient of \\(u\\), denoted \\(\\nabla u\\), is the vector field with\n\\[\n(\\nabla u)(x,y) = \\begin{bmatrix} \\displaystyle \\frac{\\partial u}{\\partial x}(x,y) \\\\ \\displaystyle \\frac{\\partial u}{\\partial y}(x,y) \\end{bmatrix}.\n\\]\n\n\n\n\n\nThe fundamental properties of gradient fields are:\n\nThe dot product \\((\\nabla u)(x,y) \\cdot \\mathbf{v}\\) is the (approximate) change in \\(u\\), at the point \\((x,y)\\), in the direction of a vector \\(\\mathbf{v}\\).\nThe gradient vector at a point \\((x,y)\\) is orthogonal to the level curve of \\(u\\) through \\((x,y)\\).\nThe gradient vector at a point \\((x,y)\\) points in the direction of maximum increase of \\(u\\).\n\nAs long as \\(\\|\\mathbf{v}\\|\\) is small, we have: \\[\n(\\nabla u)(x,y) \\cdot \\mathbf{v} \\approx u(x+v_1, y+v_2) - u(x,y).\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/03-the-laplacian-and-laplaces-equation.html#divergence",
    "href": "teaching/pde-sp-26/slides/03-the-laplacian-and-laplaces-equation.html#divergence",
    "title": "03 The Laplacian and Laplace’s equation",
    "section": "Divergence",
    "text": "Divergence\n\n\n\n\nDivergence operator in two dimensions\n\n\nLet \\(\\mathbf{F}:\\mathbb{R}^2\\to \\mathbb{R}^2\\) be a continuously differentiable vector field with components \\(F_1\\) and \\(F_2\\). The divergence of \\(\\mathbf{F}\\), denoted \\(\\nabla \\cdot \\mathbf{F}\\), is the scalar field with\n\\[\n(\\nabla \\cdot \\mathbf{F})(x,y) = \\frac{\\partial F_1}{\\partial x}(x,y) + \\frac{\\partial F_2}{\\partial y}(x,y).\n\\]\n\n\n\n\n\nHow should you think of the divergence?\n\nImagine a small circle \\(C\\) of radius \\(r\\) centered at \\((x_0,y_0)\\). Then the integral \\[\n  \\int_C \\mathbf{F} \\cdot \\mathbf{n} \\, ds\n  \\] is the net flux of \\(\\mathbf{F}\\) across the circle. (Think of \\(\\mathbf{F}\\) as a velocity field of some “flow”.)\nAs \\(r\\to 0\\), this integral certainly vanishes since the circle shrinks to a point.\nIn fact, it goes to \\(0\\) as fast as the area of the circle, and the divergence is the rate: \\[\n  (\\nabla \\cdot \\mathbf{F})(x_0,y_0) = \\lim_{r\\to 0^+} \\frac{1}{\\pi r^2} \\int_C \\mathbf{F} \\cdot \\mathbf{n} \\, ds.\n  \\]"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/03-the-laplacian-and-laplaces-equation.html#the-divergence-theorem",
    "href": "teaching/pde-sp-26/slides/03-the-laplacian-and-laplaces-equation.html#the-divergence-theorem",
    "title": "03 The Laplacian and Laplace’s equation",
    "section": "The Divergence Theorem",
    "text": "The Divergence Theorem\n\n\n\n\nTheorem (Divergence Theorem).\n\n\nLet \\(\\mathbf{F}:\\mathbb{R}^2\\to \\mathbb{R}^2\\) be a continuously differentiable vector field, and let \\(R\\) be a region in \\(\\mathbb{R}^2\\) with a piecewise smooth boundary \\(\\partial R\\). Then \\[\n\\int_{\\partial R} \\mathbf{F} \\cdot \\mathbf{n} \\, ds = \\int_R (\\nabla \\cdot \\mathbf{F}) \\, dA.\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/03-the-laplacian-and-laplaces-equation.html#laplacian",
    "href": "teaching/pde-sp-26/slides/03-the-laplacian-and-laplaces-equation.html#laplacian",
    "title": "03 The Laplacian and Laplace’s equation",
    "section": "Laplacian",
    "text": "Laplacian\n\n\n\n\nLaplacian operator in two dimensions\n\n\nLet \\(u:\\mathbb{R}^2\\to \\mathbb{R}\\) be a function with continuous second-order partial derivatives. The Laplacian of \\(u\\), denoted \\(\\nabla^2 u\\), is the scalar field with\n\\[\n(\\nabla^2 u)(x,y) = \\frac{\\partial^2 u}{\\partial x^2}(x,y) + \\frac{\\partial^2 u}{\\partial y^2}(x,y).\n\\]\n\n\n\n\n\nThe motivation for the notation comes from: \\[\n\\nabla^2 u = \\nabla \\cdot (\\nabla u),\n\\] which shows that the Laplacian is the divergence of the gradient of \\(u\\).\nHow should you think of the Laplacian?\n\nImagine a small circle \\(C\\) of radius \\(r\\) centered at \\((x_0,y_0)\\). Then \\[\n  \\frac{1}{2\\pi r} \\int_C u(x,y) \\, ds - u(x_0,y_0)\n  \\] is the difference between the average value of \\(u\\) on the circle and the value at the center.\nAs \\(r\\to 0\\), this quantity certainly vanishes since \\(u(x,y) \\to u(x_0,y_0)\\) for all \\((x,y)\\in C\\).\nIn fact, it goes to \\(0\\) as fast as \\(r^2\\), and the Laplacian is (proportional to) the rate: \\[\n  (\\nabla^2 u)(x_0,y_0) = \\lim_{r\\to 0^+} \\frac{4}{r^2} \\left( \\frac{1}{2\\pi r} \\int_C u(x,y) \\, ds - u(x_0,y_0)\\right).\n  =\n  \\]"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/03-the-laplacian-and-laplaces-equation.html#differential-operators-in-arbitrary-dimensions",
    "href": "teaching/pde-sp-26/slides/03-the-laplacian-and-laplaces-equation.html#differential-operators-in-arbitrary-dimensions",
    "title": "03 The Laplacian and Laplace’s equation",
    "section": "Differential operators in arbitrary dimensions",
    "text": "Differential operators in arbitrary dimensions\n\nThe gradient, divergence, and Laplacian all have natural generalizations to higher dimensions: \\[\n\\nabla u = \\begin{bmatrix} \\frac{\\partial u}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial u}{\\partial x_n} \\end{bmatrix}, \\quad\n\\nabla \\cdot \\mathbf{F} = \\sum_{i=1}^n \\frac{\\partial F_i}{\\partial x_i}, \\quad\n\\nabla^2 u = \\sum_{i=1}^n \\frac{\\partial^2 u}{\\partial x_i^2}.\n\\]\nBut the Laplacian has a slightly different interpretation if \\(u\\) is “time-dependent.”\n\nFor example, let \\(u:\\mathbb{R}^3 \\to \\mathbb{R}\\), and write the third variable as \\(t\\). So, our function is \\(u(x,y,t)\\).\nThen the Laplacian only acts on the spatial variables \\(x\\) and \\(y\\), not on \\(t\\): \\[\n  \\nabla^2 u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}.\n  \\]\nIf our function is \\(u(x,t)\\), then: \\[\n  \\nabla^2 u = \\frac{\\partial^2 u}{\\partial x^2}.\n  \\]"
  },
  {
    "objectID": "teaching/pde-sp-26/slides/03-the-laplacian-and-laplaces-equation.html#the-heat-and-wave-equations-revisited",
    "href": "teaching/pde-sp-26/slides/03-the-laplacian-and-laplaces-equation.html#the-heat-and-wave-equations-revisited",
    "title": "03 The Laplacian and Laplace’s equation",
    "section": "The heat and wave equations revisited",
    "text": "The heat and wave equations revisited\n\nThis means we can rewrite the heat and wave equations in terms of the Laplacian acting only on the spatial variables!\n\n\n\n\n\nThe heat equation in arbitrary dimensions\n\n\nLet \\(u:\\mathbb{R}^{n+1} \\to \\mathbb{R}\\) be function with continuous second-order partial derivatives, and write \\[\nu = u(x_1,\\dots,x_n,t).\n\\] Then the heat equation is \\[\n\\frac{\\partial u}{\\partial t} = k \\nabla^2 u,\n\\] where \\(\\nabla^2\\) is the Laplacian acting only on the first \\(n\\) spatial variables and \\(k\\) is a positive constant.\n\n\n\n\n\n\n\n\nThe wave equation in arbitrary dimensions\n\n\nLet \\(u:\\mathbb{R}^{n+1} \\to \\mathbb{R}\\) be function with continuous second-order partial derivatives, and write \\[\nu = u(x_1,\\dots,x_n,t).\n\\] Then the wave equation is \\[\n\\frac{\\partial^2 u}{\\partial t^2} = a^2 \\nabla^2 u,\n\\] where \\(\\nabla^2\\) is the Laplacian acting only on the first \\(n\\) spatial variables and \\(a\\) is a positive constant."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/03-the-laplacian-and-laplaces-equation.html#laplaces-equation",
    "href": "teaching/pde-sp-26/slides/03-the-laplacian-and-laplaces-equation.html#laplaces-equation",
    "title": "03 The Laplacian and Laplace’s equation",
    "section": "Laplace’s equation",
    "text": "Laplace’s equation\n\n\n\n\nLaplace’s equation in arbitrary dimensions\n\n\nLet \\(u:\\mathbb{R}^{n} \\to \\mathbb{R}\\) be function with continuous second-order partial derivatives, and write \\[\nu = u(x_1,\\dots,x_n).\n\\] Then Laplace’s equation is \\[\n\\nabla^2 u = 0,\n\\] where \\(\\nabla^2\\) is the Laplacian acting on all \\(n\\) spatial variables.\n\n\n\n\n\nAs you may check, \\(\\nabla^2\\) is a linear differential operator: \\[\n\\nabla^2 (u+v) = \\nabla^2 u + \\nabla^2 v, \\quad \\nabla^2 (cu) = c \\nabla^2 u.\n\\]\nThus, solutions to Laplace’s equation are functions in the kernel of the Laplacian operator.\nSolutions are called harmonic functions.\nUsing our intuition for the Laplacian, we see that harmonic functions are those that are equal to their average values in every small neighborhood.\nAs David Griffiths in Introduction to Electrodynamics says: harmonic functions are “as boring as they possibly could be”."
  },
  {
    "objectID": "teaching/pde-sp-26/slides/03-the-laplacian-and-laplaces-equation.html#exercise-harmonic-functions-in-one-dimension",
    "href": "teaching/pde-sp-26/slides/03-the-laplacian-and-laplaces-equation.html#exercise-harmonic-functions-in-one-dimension",
    "title": "03 The Laplacian and Laplace’s equation",
    "section": "Exercise: Harmonic functions in one dimension",
    "text": "Exercise: Harmonic functions in one dimension\n\n\n\nFind all solutions \\(u(x)\\) to Laplace’s equation \\(\\nabla^2 u =0\\) over the unit interval \\([0,1]\\), subject to the boundary conditions \\[\nu(0) = a \\quad \\text{and} \\quad u(1) = b,\n\\] where \\(a\\) and \\(b\\) are given constants.\n\nShow that no solution has a maximum or minimum in the interior of the interval. (Assume \\(a\\neq b\\).)\nLet \\(u(x)\\) be a solution. For each \\(x_0\\) in the interior of \\([0,1]\\), show that \\(u(x_0)\\) is the average of its values on the boundary of the subinterval \\([x_0-\\epsilon, x_0+\\epsilon]\\) for all \\(\\epsilon &gt; 0\\) such that \\([x_0-\\epsilon, x_0+\\epsilon] \\subset [0,1]\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/05ex-a-first-look-at-fourier-series-part-1.html",
    "href": "teaching/pde-sp-26/exercises/05ex-a-first-look-at-fourier-series-part-1.html",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "This exercise refers to the Fourier series listed in rows 1-20 of the file here.\n\n\n\n\n\n\n\nVerify, by hand, that the Fourier series for the function in row 3 of the table in the file above is the one shown.\nVerify, by hand, that the Fourier series for the function in row 4 of the table in the file above is the one shown.\nVerify, by hand, that the Fourier series for the function in row 6 of the table in the file above is the one shown.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nThe solutions are given in the PDF itself! Make sure your calculations match the Fourier series shown in the table."
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/05ex-a-first-look-at-fourier-series-part-1.html#exercise-1-practice-computing-fourier-series",
    "href": "teaching/pde-sp-26/exercises/05ex-a-first-look-at-fourier-series-part-1.html#exercise-1-practice-computing-fourier-series",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "This exercise refers to the Fourier series listed in rows 1-20 of the file here.\n\n\n\n\n\n\n\nVerify, by hand, that the Fourier series for the function in row 3 of the table in the file above is the one shown.\nVerify, by hand, that the Fourier series for the function in row 4 of the table in the file above is the one shown.\nVerify, by hand, that the Fourier series for the function in row 6 of the table in the file above is the one shown.\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nThe solutions are given in the PDF itself! Make sure your calculations match the Fourier series shown in the table."
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html",
    "href": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "Recall that a complex number \\(z\\) can be written in the form \\(z = x + iy\\), where \\(x\\) and \\(y\\) are real numbers and \\(i\\) is the imaginary unit, satisfying \\(i^2 = -1\\). The numbers \\(x\\) and \\(y\\) are called the real and imaginary parts of \\(z\\), respectively. For example, the complex number\n\\[\nz = 3 + 4i\n\\]\nhas real part \\(3\\) and imaginary part \\(4\\).\nThe set of all complex numbers is denoted by \\(\\mathbb{C}\\). The set of all ordered pairs \\((x,y)\\) of real numbers is denoted \\(\\mathbb{R}^2\\). As you well know, we can visualize \\(\\mathbb{R}^2\\) as the Cartesian plane, with the first coordinate \\(x\\) representing the horizontal axis and the second coordinate \\(y\\) representing the vertical axis.\nThere is a natural way to visualize complex numbers as points in the Cartesian plane as well, by identifying the complex number \\(z = x + iy\\) with the point \\((x,y)\\) in \\(\\mathbb{R}^2\\). In this way, the real part of the complex number corresponds to the horizontal coordinate of the point, and the imaginary part corresponds to the vertical coordinate of the point. This visualization of \\(\\mathbb{C}\\) is called the complex plane.\nThe absolute value (or magnitude, or modulus) of a complex number \\(z = x + iy\\) is defined by\n\\[\n|z| = \\sqrt{x^2 + y^2}.\n\\]\nVisually, this corresponds to the distance from the origin \\((0,0)\\) to the point \\((x,y)\\) in the complex plane.\nThe argument (or angle, or phase) of a complex number \\(z = x + iy\\) is defined as the angle \\(\\theta\\) in \\((-\\pi, \\pi]\\) (measured in radians, measured counterclockwise) between the positive real axis and the line segment connecting the origin to the point \\((x,y)\\). It is denoted \\(\\arg{z}\\).\nFinally, the complex conjugate of a complex number \\(z = x + iy\\) is defined as the complex number \\(\\overline{z} = x - iy\\). Visually, this corresponds to reflecting the point \\((x,y)\\) across the real axis to get the point \\((x,-y)\\).\n\n\n\n\n\n\n\nPlot the complex number \\(2 + 3i\\) in the complex plane.\nPlot the complex number \\(-1 + 4i\\) in the complex plane.\nPlot the complex number \\(-2 - 2i\\) in the complex plane.\nPlot the complex number \\(3\\) (in other words, \\(3 + 0i\\)) in the complex plane.\nPlot the complex number \\(-5i\\) (in other words, \\(0 - 5i\\)) in the complex plane.\nFind the absolute value, argument, and complex conjugate of the complex number \\(1 + i\\).\nFind the absolute value, argument, and complex conjugate of the complex number \\(-1 + i\\).\nFind the absolute value, argument, and complex conjugate of the complex number \\(2\\).\nFind the absolute value, argument, and complex conjugate of the complex number \\(-3\\).\nFind the absolute value, argument, and complex conjugate of the complex number \\(-5i\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nI’ll leave plotting exercises a-e to you.\n\n\\(|1 + i| = \\sqrt{2}\\), \\(\\arg(1 + i) = \\frac{\\pi}{4}\\), \\(\\overline{1 + i} = 1 - i\\).\n\\(|-1 + i| = \\sqrt{2}\\), \\(\\arg(-1 + i) = \\frac{3\\pi}{4}\\), \\(\\overline{-1 + i} = -1 - i\\).\n\\(|2| = 2\\), \\(\\arg(2) = 0\\), \\(\\overline{2} = 2\\).\n\\(|-3| = 3\\), \\(\\arg(-3) = \\pi\\), \\(\\overline{-3} = -3\\).\n\\(|-5i| = 5\\), \\(\\arg(-5i) = -\\frac{\\pi}{2}\\), \\(\\overline{-5i} = 5i\\).\n\n\n\n\n\n\n\nAddition and subtraction of complex numbers work exactly as you would expect, essentially by collecting like terms. For example:\n\\[\n(2 + 3i) + (4 + 5i) = 6 + 8i\n\\]\nand\n\\[\n(2 - 2i) - (1 + 3i) = 1 - 5i.\n\\]\nTo multiply complex numbers, you use the distributive property (i.e., “FOIL”) and the fact that \\(i^2 = -1\\). For example:\n\\[\n(2+3i)(4+5i) = 8 + 10i + 12i + 15i^2 = 8 + 22i - 15 = -7 + 22i.\n\\]\nFor division of complex numbers, it is first helpful to define the complex conjugate of a complex number \\(z = x + iy\\) as the number \\(\\overline{z} = x - iy\\). The complex conjugate has the property that\n\\[\nz\\overline{z} = |z|^2,\n\\]\nwhere \\(|z| = \\sqrt{x^2 + y^2}\\) is the magnitude of the complex number, which is a real number. Thus,\n\\[\n\\frac{1}{z} = \\frac{\\overline{z}}{|z|^2}.\n\\]\nThis, then, gives us a formula for division of complex numbers: If \\(z\\) and \\(w\\) are two complex numbers, with \\(z\\neq 0\\), then\n\\[\n\\frac{w}{z} = \\frac{w\\overline{z}}{z\\overline{z}} = \\frac{w\\overline{z}}{|z|^2}.\n\\]\nFor example:\n\\[\n\\frac{2 + 3i}{4 + 5i} = \\frac{(2 + 3i)(4 - 5i)}{(4 + 5i)(4 - 5i)} = \\frac{8 - 10i + 12i - 15i^2}{16 + 25} = \\frac{23 + 2i}{41} = \\frac{23}{41} + \\frac{2}{41}i.\n\\]\n\n\n\n\n\n\nCompute the following.\n\n\\((1 + 2i) + (3 + 4i)\\)\n\\((5 + 6i) - (7 + 8i)\\)\n\\((1 + 2i)(4 - i)\\)\n\\(\\displaystyle\\frac{2 + 3i}{1 + 4i}\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\((1 + 2i) + (3 + 4i) = 4 + 6i\\)\n\\((5 + 6i) - (7 + 8i) = -2 - 2i\\)\n\\((1 + 2i)(4 - i) = 6 + 7i\\)\n\\(\\displaystyle\\frac{2 + 3i}{1 + 4i} = \\frac{14}{17} - \\frac{5}{17}i\\)\n\n\n\n\n\n\n\nIf \\(z\\in \\mathbb{C}\\) is a complex number, we define the number \\(e^z\\) via the power series expansion:\n\\[\ne^z = \\sum_{n=0}^\\infty \\frac{z^n}{n!} = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\cdots.\n\\]\nSimilarly, we define the sine and cosine of \\(z\\) via their power series expansions:\n\\[\n\\sin{z} = \\sum_{n=0}^\\infty \\frac{(-1)^n z^{2n+1}}{(2n+1)!} = z - \\frac{z^3}{3!} + \\frac{z^5}{5!} - \\cdots\n\\]\nand\n\\[\n\\cos{z} = \\sum_{n=0}^\\infty \\frac{(-1)^n z^{2n}}{(2n)!} = 1 - \\frac{z^2}{2!} + \\frac{z^4}{4!} - \\cdots.\n\\]\nYou might never have seen power series with complex arguments before, but all of the usual rules for manipulating power series still apply. In particular, these series all converge for every complex number \\(z\\).\n\n\n\n\n\n\n\nOne of the most important relationships involving complex exponentials is Euler’s formula, which says that \\[\n  e^{iz} = \\cos{z} + i\\sin{z}\n  \\] for all \\(z\\in \\mathbb{C}\\). Use the power series definitions above to verify Euler’s formula.\nUse Euler’s formula to derive the following formulas for cosine and sine in terms of complex exponentials: \\[\n  \\cos{z} = \\frac{e^{iz} + e^{-iz}}{2}\n  \\] and \\[\n  \\sin{z} = \\frac{e^{iz} - e^{-iz}}{2i}.\n  \\]\n\nNote: One of the most important properties of the complex exponential function is that it satisfies the same exponential rules as the real exponential function. In particular, for any complex numbers \\(z\\) and \\(w\\), we have \\[\ne^{z+w} = e^z e^w.\n\\]\nYou will not need to prove this fact, but it is useful to keep in mind.\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe solution is just a matter of substituting \\(iz\\) into the power series definition of the exponential function and then separating the resulting series into real and imaginary parts: \\[\n  \\begin{align*}\n  e^{iz} &= \\sum_{n=0}^\\infty \\frac{(iz)^n}{n!} \\\\\n  &= \\sum_{n=0}^\\infty \\frac{i^n z^n}{n!} \\\\\n  &= \\sum_{m=0}^\\infty \\frac{i^{2m} z^{2m}}{(2m)!} + \\sum_{m=0}^\\infty \\frac{i^{2m+1} z^{2m+1}}{(2m+1)!} \\\\\n  &= \\sum_{m=0}^\\infty \\frac{(-1)^m z^{2m}}{(2m)!} + i\\sum_{m=0}^\\infty \\frac{(-1)^m z^{2m+1}}{(2m+1)!} \\\\\n  &= \\cos{z} + i\\sin{z}.\n  \\end{align*}\n  \\] In the last equality, we used the power series definitions of sine and cosine.\nFirst note that \\[\n  e^{-iz} = \\cos{-z} + i\\sin{-z} = \\cos{z} - i\\sin{z},\n  \\] since all powers of \\(z\\) in the cosine series are even and all powers of \\(z\\) in the sine series are odd. Now we can add and subtract Euler’s formula and this new formula to get the desired results: \\[\n  \\begin{align*}\n  e^{iz} + e^{-iz} &= (\\cos{z} + i\\sin{z}) + (\\cos{z} - i\\sin{z}) = 2\\cos{z}, \\\\\n  e^{iz} - e^{-iz} &= (\\cos{z} + i\\sin{z}) - (\\cos{z} - i\\sin{z}) = 2i\\sin{z}.\n  \\end{align*}\n  \\] Dividing both equations by the appropriate constants gives the desired formulas.\n\n\n\n\n\n\n\nSuppose that we have a function \\(f:\\mathbb{R} \\to \\mathbb{C}\\), which means that the inputs to \\(f\\) are real numbers and the outputs are complex numbers. For example, the function\n\\[\nf(x) = x + 3x^2 i\n\\]\ntakes a real number \\(x\\) and outputs the complex number \\(x + 3x^2 i\\). The graph of such a function is a curve in the complex plane traced out by the points \\(f(x)\\) as \\(x\\) varies over the real numbers. For example, the graph of the function above is simply the parabola \\(y=3x^2\\) traced out from left to right, when you think of \\(\\mathbb{C}\\) as \\(\\mathbb{R}^2\\). (Can you see why?) The direction in which the curve is traced out as \\(x\\) increases is called the orientation of the graph.\n\n\n\n\n\n\nDescribe the graphs of the follwing functions \\(f:\\mathbb{R} \\to \\mathbb{C}\\), along with their orientations.\n\n\\(f(x) = 2 + ix\\)\n\\(f(x) = -x^2 - x\\)\n\\(f(x) = e^{ix}\\)\n\\(f(x) = e^{-ix}\\)\n\\(f(x) = e^{2\\pi ix}\\)\n\\(f(x) = e^{-3\\pi ix}\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe graph is a vertical line at \\(x=2\\) in the complex plane, traced from bottom to top as \\(x\\) increases.\nThe graph is a downward-opening parabola in the complex plane, traced from right to left as \\(x\\) increases.\nThe graph is the unit circle in the complex plane, traced counterclockwise as \\(x\\) increases, completing one revolution over the interval \\([0,2\\pi]\\). This is because, using Euler’s formula, we have \\(e^{ix} = \\cos{x} + i\\sin{x}\\), and it is well-known from trigonometry that the point \\((\\cos{x}, \\sin{x})\\) traces out the unit circle as \\(x\\) varies over the real numbers.\nThe graph is the unit circle in the complex plane, traced clockwise as \\(x\\) increases, completing one revolution over the interval \\([0,2\\pi]\\).\nThe graph is the unit circle in the complex plane, traced counterclockwise as \\(x\\) increases, completing one revolution over the interval \\([0,1]\\).\nThe graph is the unit circle in the complex plane, traced clockwise as \\(x\\) increases, completing one revolution over the interval \\([0, \\frac{2}{3}]\\).\n\n\n\n\n\n\n\nA function \\(f:\\mathbb{R} \\to \\mathbb{C}\\) can be written as\n\\[\nf(x) = u(x) + iv(x),\n\\]\nwhere \\(u(x)\\) and \\(v(x)\\) are real-valued functions of the real variable \\(x\\) called the real and imaginary parts of \\(f\\), respectively. For example, the function \\[\nf(x) = 2x^2 + i\\sin{x}\n\\]\nhas real part \\(u(x) = 2x^2\\) and imaginary part \\(v(x) = \\sin{x}\\).\nWe shall say \\(f\\) is continuous at a point \\(x = a\\) if both \\(u\\) and \\(v\\) are continuous at \\(x = a\\). Similarly, we say \\(f\\) is differentiable at \\(x = a\\) if both \\(u\\) and \\(v\\) are differentiable at \\(x = a\\). In this case, we define the derivative of \\(f\\) at \\(x = a\\) by\n\\[\nf'(x) = u'(x) + iv'(x).\n\\]\nIn the example above, we have\n\\[\nf'(x) = 4x + i\\cos{x}.\n\\]\nIntegrals of complex-valued functions of a real variable are defined similarly. We say that \\(f\\) is integrable on an interval \\([a,b]\\) if both \\(u\\) and \\(v\\) are integrable on \\([a,b]\\). In this case, we define the integral of \\(f\\) on \\([a,b]\\) by\n\\[\n\\int_a^b f(x)\\ dx = \\int_a^b u(x)\\ dx + i\\int_a^b v(x)\\ dx.\n\\]\nIn the example above, we have\n\\[\n\\int_0^\\pi f(x)\\ dx = \\int_0^\\pi 2x^2\\ dx + i\\int_0^\\pi \\sin{x}\\ dx = \\frac{2\\pi^3}{3} + 2i.\n\\]\n\n\n\n\n\n\nCompute the following.\n\n\\(\\displaystyle\\frac{d}{dx} \\left( \\cos{x} + i(2x^3-5) \\right)\\).\n\\(\\displaystyle\\int_0^{\\pi/4} \\left( \\sec^2{x} + i e^x \\right)\\ dx\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(-\\sin{x} + i6x^2\\)\n\\(1 + i\\left( e^{\\pi/4} - 1 \\right)\\)\n\n\n\n\n\n\n\nA function of the form \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) means that \\(f\\) takes \\(n\\)-tuples \\((x_1,x_2,\\ldots,x_n)\\) of real numbers as inputs, and outputs a single real number. We think of \\(f\\) as a function of multiple variables, namely the \\(n\\) input variables \\(x_1,x_2,\\ldots,x_n\\).\nFor example, the function \\(f:\\mathbb{R}^2 \\to \\mathbb{R}\\) defined by \\[\nf(x,y) = x^2 e^{-y} + \\sin{(xy)}\n\\]\ntakes the orded pair \\((x,y)\\) as input and returns a real number.\nWhen dealing with functions of several variables, we often want to measure how the function changes as we vary just one of the input variables, while holding the others constant. This leads us to the concept of partial derivatives.\nFor simplicity, suppose we have only two input variables, \\(x\\) and \\(y\\), so that we are dealing with a function \\(f:\\mathbb{R}^2 \\to \\mathbb{R}\\). Recall that the partial derivative of \\(f\\) with respect to \\(x\\) is defined via the usual limit of a difference quotient, treating \\(y\\) as a constant:\n\\[\n\\frac{\\partial f}{\\partial x}(x,y) = \\lim_{h\\to 0} \\frac{f(x+h,y) - f(x,y)}{h}.\n\\]\nSimilarly, the partial derivative of \\(f\\) with respect to \\(y\\) is defined via the usual limit of a difference quotient, treating \\(x\\) as a constant:\n\\[\n\\frac{\\partial f}{\\partial y}(x,y) = \\lim_{h\\to 0} \\frac{f(x,y+h) - f(x,y)}{h}.\n\\]\nPhysically, \\(\\frac{\\partial f}{\\partial x}(x,y)\\) measures the (instantaneous) rate of change of \\(f\\) with respect to \\(x\\) at the point \\((x,y)\\) while \\(y\\) is held constant, while \\(\\frac{\\partial f}{\\partial y}(x,y)\\) measures the (instantaneous) rate of change of \\(f\\) with respect to \\(y\\) at the point \\((x,y)\\) while \\(x\\) is held constant.\nPartial derivatives may be computed using the same rules as ordinary derivatives, treating all other variables as constants. For example, for the function above, we have\n\\[\n\\frac{\\partial f}{\\partial x}(x,y) = 2x e^{-y} + y\\cos{(xy)}\n\\]\nand\n\\[\n\\frac{\\partial f}{\\partial y}(x,y) = -x^2 e^{-y} + x\\cos{(xy)}.\n\\]\n\n\n\n\n\n\nCompute the following.\n\n\\(\\displaystyle \\frac{\\partial}{\\partial x} \\left( x^3 y + \\ln{(xy)} \\right)\\).\n\\(\\displaystyle \\frac{\\partial}{\\partial y} \\left( e^{xy} + x^2 y^2 \\right)\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(3x^2 y + \\frac{1}{x}\\)\n\\(x e^{xy} + 2x^2 y\\)\n\n\n\n\n\n\n\nLet \\(V\\) be a vector space, which means that, roughly speaking, \\(V\\) is a set of objects (called vectors) that can be added together and multiplied by scalars (numbers, either real or complex) in a way that satisfies certain axioms (like commutativity, associativity, distributivity, etc.).\nThe central example of a finite-dimensional (real) vector space is \\(\\mathbb{R}^n\\), the set of all \\(n\\)-tuples of real numbers. Vectors in \\(\\mathbb{R}^n\\) can be added componentwise and multiplied by real numbers componentwise. This should be familiar to you from your course on linear algebra, though in that course vectors in \\(\\mathbb{R}^n\\) were often represented as column vectors.\nOn the other hand, the central example of an infinite-dimensional (real) vector space is the set of all real-valued functions defined on some interval \\([a,b]\\). Very often, the functions will be required to satisfy some additional properties, such as continuity, differentiability, or integrability, but we won’t focus on those details here. In these “function spaces,” two functions \\(f\\) and \\(g\\) can be added together to form a new function \\(f + g\\), which is just the “pointwise” sum of the two functions: evaluating \\(f+g\\) at a point \\(x\\) goes like this:\n\\[\n(f + g)(x) = f(x) + g(x).\n\\]\nSimilarly, a function \\(f\\) can be multiplied by a scalar \\(c\\) to form a new function \\(cf\\), defined by\n\\[\n(cf)(x) = c \\cdot f(x).\n\\]\nNow, if \\(V\\) and \\(W\\) are any two vector spaces, recall that a function \\(T: V \\to W\\) is called a linear transformation if for all vectors \\(u,v \\in V\\) and all scalars \\(c\\), we have\n\nPreservation of addition: \\(T(u + v) = T(u) + T(v)\\).\nPreservation of scaling: \\(T(cu) = cT(u)\\).\n\nIf \\(W\\) coincides with \\(V\\), then \\(T\\) is often called a linear operator on \\(V\\).\nSuppose now that \\(V\\) is a vector space of functions \\(f:\\mathbb{R}^2 \\to \\mathbb{R}\\), with input variables \\(x\\) and \\(y\\), and with suitable differentiability properties. Convince yourself that all partial derivative operators are linear transformations on \\(V\\): for example, we have the function\n\\[\n\\frac{\\partial}{\\partial x}: V\\to V\n\\]\nthat carries a function \\(f\\) to its partial derivative with respect to \\(x\\), \\(\\frac{\\partial f}{\\partial x}\\). You know very well that this operator satisfies both properties of a linear transformation:\n\\[\n\\frac{\\partial}{\\partial x} (f + g) = \\frac{\\partial f}{\\partial x} + \\frac{\\partial g}{\\partial x}\n\\]\nand\n\\[\n\\frac{\\partial}{\\partial x} (cf) = c \\frac{\\partial f}{\\partial x}.\n\\]\nLikewise, all second-order partial derivative operators are linear operators on \\(V\\): for example, the transformations\n\\[\n\\frac{\\partial^2}{\\partial x^2}: V\\to V, \\quad \\text{and} \\quad \\frac{\\partial^2}{\\partial x \\partial y}: V\\to V,\n\\]\nhave the properties\n\\[\n\\frac{\\partial^2}{\\partial x^2} (f + g) = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 g}{\\partial x^2} \\quad \\text{and} \\quad \\frac{\\partial^2}{\\partial x \\partial y} (f + g) = \\frac{\\partial^2 f}{\\partial x \\partial y} + \\frac{\\partial^2 g}{\\partial x \\partial y}\n\\]\nas well as\n\\[\n\\frac{\\partial^2}{\\partial x^2} (cf) = c \\frac{\\partial^2 f}{\\partial x^2}\\quad \\text{and} \\quad \\frac{\\partial^2}{\\partial x \\partial y} (cf) = c \\frac{\\partial^2 f}{\\partial x \\partial y}.\n\\]\nThis pattern continues: all partial differential operators, of all degrees and no matter if they are mixed or non-mixed—all are linear operators.\nThe following exercise gives you a chance to check linearity for a more complicated partial differential operator, which is expressed as a “linear combination” of several partial derivative operators.\n\n\n\n\n\n\nLet \\(V\\) be the function space above, and consider the operator \\(L: V\\to V\\), defined by\n\\[\nL = \\frac{\\partial}{\\partial y} + x^2 \\frac{\\partial^2}{\\partial x^2} - \\sin{y} \\frac{\\partial}{\\partial x \\partial y}.\n\\]\n\nCompute \\(L(x^2 y + y^3\\cos{x})\\).\nNow compute \\(L(x^2y) + L(y^3\\cos{x})\\).\nMake sure that your answers in parts (a) and (b) agree.\nCompute \\(L(2x^2 y)\\).\nNow compute \\(2L(x^2 y)\\).\nMake sure that your answers in parts (d) and (e) agree.\nDo you think \\(L\\) is a linear operator on \\(V\\)? Why or why not? What would you need to check to be sure?\n\n\n\n\n\n\n\n\n\n\nNoteSolution.\n\n\n\n\n\n\nWe have \\[\n  \\begin{align*}\n  L(x^2 y + y^3\\cos{x}) &= \\frac{\\partial}{\\partial y} (x^2 y + y^3\\cos{x}) + x^2 \\frac{\\partial^2}{\\partial x^2} (x^2 y + y^3\\cos{x}) - \\sin{y} \\frac{\\partial}{\\partial x \\partial y} (x^2 y + y^3\\cos{x}) \\\\\n  &= x^2 + 3y^2 \\cos{x} + x^2(2y - y^3 \\cos{x}) - \\sin{y}(2x - 3y^2 \\sin{x}) \\\\\n  &= x^2 + 3y^2 \\cos{x} + 2x^2 y - x^2y^3 \\cos{x} - 2x\\sin{y} + 3y^2 \\sin{x}\\sin{y}.\n  \\end{align*}\n  \\]\nWe have \\[\n  \\begin{align*}\n  L(x^2 y) + L(y^3\\cos{x}) &= \\left[ \\frac{\\partial}{\\partial y} (x^2 y) + x^2 \\frac{\\partial^2}{\\partial x^2} (x^2 y) - \\sin{y} \\frac{\\partial}{\\partial x \\partial y} (x^2 y) \\right] \\\\\n  &\\quad + \\left[ \\frac{\\partial}{\\partial y} (y^3\\cos{x}) + x^2 \\frac{\\partial^2}{\\partial x^2} (y^3\\cos{x}) - \\sin{y} \\frac{\\partial}{\\partial x \\partial y} (y^3\\cos{x}) \\right] \\\\\n  &= (x^2 + 2x^2 y - 2x\\sin{y}) + (3y^2 \\cos{x} - x^2 y^3 \\cos{x} + 3y^2 \\sin{x}\\sin{y}) \\\\\n  &= x^2 + 3y^2 \\cos{x} + 2x^2 y - x^2y^3 \\cos{x} - 2x\\sin{y} + 3y^2 \\sin{x}\\sin{y}.\n  \\end{align*}\n  \\]\nYes, the answers in parts (a) and (b) agree.\nWe have \\[\n  \\begin{align*}\n  L(2x^2 y) &= \\frac{\\partial}{\\partial y} (2x^2 y) + x^2 \\frac{\\partial^2}{\\partial x^2} (2x^2 y) - \\sin{y} \\frac{\\partial}{\\partial x \\partial y} (2x^2 y) \\\\\n  &= 2x^2 + 4x^2 y - 4x\\sin{y}.\n  \\end{align*}\n  \\]\nWe have \\[\n  \\begin{align*}\n  2L(x^2 y) &= 2 \\left[ \\frac{\\partial}{\\partial y} (x^2 y) + x^2 \\frac{\\partial^2}{\\partial x^2} (x^2 y) - \\sin{y} \\frac{\\partial}{\\partial x \\partial y} (x^2 y) \\right] \\\\\n  &= 2(x^2 + 2x^2 y - 2x\\sin{y}) \\\\\n  &= 2x^2 + 4x^2 y - 4x\\sin{y}.\n  \\end{align*}\n  \\]\nYes, the answers in parts (d) and (e) agree.\nIf \\(L\\) were linear, then it need to preserve addition, i.e., \\(L(f+g) = L(f) + L(g)\\) for all \\(f,g\\in V\\), and it would need to preserve scaling, i.e., \\(L(cf) = cL(f)\\) for all \\(f\\in V\\) and all scalars \\(c\\). We have verified both of these properties for the specific functions \\(f(x,t) = x^2 t\\) and \\(g(x,t) = t^3 \\cos{x}\\), and for the scalar \\(c=2\\). From this evidence, it seems likely that \\(L\\) is a linear operator on \\(V\\). To be sure, we would need to verify these properties for all functions in \\(V\\) and all scalars.\n\n\n\n\n\n\n\nLet \\(V\\) be a vector space and \\(T:V\\to V\\) a linear operator. A nonzero vector \\(v\\in V\\) is called an eigenvector of \\(T\\) if there exists a scalar \\(\\lambda\\) such that\n\\[\nT(v) = \\lambda v.\n\\]\nIf \\(V\\) happens to be a function space, then eigenvectors of \\(T\\) are often called eigenfunctions of \\(T\\).\nThis definition is the generalization to arbitrary vector spaces of the familiar notion of eigenvalues and eigenvectors from your linear algebra course.\n\n\n\n\n\n\nLet \\(V\\) be a vector space of all functions \\(f:\\mathbb{R} \\to \\mathbb{R}\\), with input variable \\(x\\) (and with suitable differentiability properties).\n\nConsider the first-order differential operator \\[\n  \\frac{d}{dx}: V\\to V.\n  \\] For each real number \\(c\\), there is an eigenfunction of this operator associated to the eigenvalue \\(c\\). Find such an eigenfunction.\nConsider the second-order differential operator \\[\n  \\frac{d^2}{dx^2}: V\\to V.\n  \\] For each negative real number \\(c\\), there is an eigenfunction of this operator associated to the eigenvalue \\(c\\). Find such an eigenfunction.\n\n\n\n\n\n\n\n\n\n\nNoteSolution.\n\n\n\n\n\n\nWe are looking for a nonzero function \\(f(x)\\) such that \\[\n  \\frac{df}{dx} = c f(x).\n  \\] This is a (first-order, linear, homogeneous) ordinary differential equation, and its general solution is \\[\n  f(x) = Ae^{cx},\n  \\] where \\(A\\) is any nonzero constant. Thus, for each real number \\(c\\), the function \\(f(x) = e^{cx}\\) is an eigenfunction of the operator \\(\\frac{d}{dx}\\) associated to the eigenvalue \\(c\\).\nWe are looking for a nonzero function \\(f(x)\\) such that \\[\n  \\frac{d^2 f}{dx^2} = c f(x).\n  \\] This is a (second-order, linear, homogeneous) ordinary differential equation. The general solution to this equation is \\[\n  f(x) = A\\cos{(\\sqrt{-c} x)} + B\\sin{(\\sqrt{-c} x)},\n  \\] where \\(A\\) and \\(B\\) are constants, not both zero. Thus, for each negative real number \\(c\\), the functions \\(f(x) = \\cos{(\\sqrt{-c} x)}\\) and \\(f(x) = \\sin{(\\sqrt{-c} x)}\\) are eigenfunctions of the operator \\(\\frac{d^2}{dx^2}\\) associated to the eigenvalue \\(c\\).\n\n\n\n\n\n\n\nRecall the wave equation\n\\[\n\\frac{\\partial^2 y}{\\partial t^2} = a^2 \\frac{\\partial^2 y}{\\partial x^2},\n\\]\nfrom class, where \\(a\\) is a known constant.\n\n\n\n\n\n\nShow that the function\n\\[\ny(x,t) = \\sin{\\left(\\frac{n\\pi x}{L}\\right)} \\cos{\\left(\\frac{n\\pi a t}{L}\\right)}\n\\]\nis a solution to the wave equation, for any fixed positive integer \\(n\\). (Note: This is the formula for the \\(n\\)-th harmonic of a vibrating string of length \\(L\\) with fixed endpoints.)\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nTo show that \\(y(x,t)\\) is a solution to the wave equation, we need to compute the second partial derivatives of \\(y\\) with respect to \\(t\\) and \\(x\\), and then verify that they satisfy the wave equation.\nThe first derivatives are:\n\\[\n\\begin{align*}\n\\frac{\\partial y}{\\partial t} &=\\left(-\\frac{n\\pi a}{L}\\right)\\cdot \\sin{\\left(\\frac{n\\pi x}{L}\\right)} \\sin{\\left(\\frac{n\\pi a t}{L}\\right)}, \\\\\n\\frac{\\partial y}{\\partial x} &= \\left(\\frac{n\\pi}{L}\\right)\\cdot  \\cos{\\left(\\frac{n\\pi x}{L}\\right)}  \\cos{\\left(\\frac{n\\pi a t}{L}\\right)}.\n\\end{align*}\n\\]\nThen, the second derivatives are:\n\\[\n\\begin{align*}\n\\frac{\\partial^2 y}{\\partial t^2} &= -\\left(\\frac{n\\pi a}{L}\\right)^2 \\sin{\\left(\\frac{n\\pi x}{L}\\right)} \\cos{\\left(\\frac{n\\pi a t}{L}\\right)}, \\\\\n\\frac{\\partial^2 y}{\\partial x^2} &= -\\left(\\frac{n\\pi}{L}\\right)^2 \\sin{\\left(\\frac{n\\pi x}{L}\\right)} \\cos{\\left(\\frac{n\\pi a t}{L}\\right)}.\n\\end{align*}\n\\]\nTherefore:\n\\[\n\\begin{align*}\na^2 \\frac{\\partial^2 y}{\\partial x^2} &= -a^2 \\left(\\frac{n\\pi}{L}\\right)^2 \\sin{\\left(\\frac{n\\pi x}{L}\\right)} \\cos{\\left(\\frac{n\\pi a t}{L}\\right)} \\\\\n&= -\\left(\\frac{n\\pi a}{L}\\right)^2 \\sin{\\left(\\frac{n\\pi x}{L}\\right)} \\cos{\\left(\\frac{n\\pi a t}{L}\\right)} \\\\\n&= \\frac{\\partial^2 y}{\\partial t^2}.\n\\end{align*}\n\\]\n\n\n\n\n\n\nRecall the heat equation\n\\[\n\\frac{\\partial u}{\\partial t} = k \\frac{\\partial^2 u}{\\partial x^2},\n\\]\nfrom class, where \\(k\\) is a known constant.\n\n\n\n\n\n\nShow that the function\n\\[\nu(x,t) = t^{-1/2} e^{-x^2/(4kt)}\n\\]\nis a solution to the heat equation, for all \\(t &gt; 0\\).\n\n\n\n\n\n\n\n\n\nNoteSolution.\n\n\n\n\n\nAs in the previous exercise, we begin with the first derivatives:\n\\[\n\\begin{align*}\n\\frac{\\partial u}{\\partial t} &= - \\frac{1}{2} t^{-3/2} e^{-x^2/(4kt)} + t^{-1/2} e^{-x^2/(4kt)} \\cdot \\frac{x^2}{4kt^2}, \\\\\n\\frac{\\partial u}{\\partial x} &= t^{-1/2} e^{-x^2/(4kt)} \\cdot \\left( -\\frac{x}{2kt} \\right).\n\\end{align*}\n\\]\nThen, the second derivative with respect to \\(x\\) is:\n\\[\n\\begin{align*}\n\\frac{\\partial^2 u}{\\partial x^2} &=t^{-1/2} e^{-x^2/(4kt)} \\cdot \\left( -\\frac{1}{2kt} \\right) + t^{-1/2} e^{-x^2/(4kt)} \\cdot \\left( -\\frac{x}{2kt} \\right)^2 \\\\\n&= -\\frac{1}{2k} t^{-3/2} e^{-x^2/(4kt)} + t^{-1/2} e^{-x^2/(4kt)} \\cdot \\left( \\frac{x^2}{4k^2t^2} \\right).\n\\end{align*}\n\\]\nThus:\n\\[\nk \\frac{\\partial^2 u}{\\partial x^2} = -\\frac{1}{2} t^{-3/2} e^{-x^2/(4kt)} + t^{-1/2} e^{-x^2/(4kt)} \\cdot \\left( \\frac{x^2}{4kt^2} \\right) = \\frac{\\partial u}{\\partial t}.\n\\]\n\n\n\n\n\n\nConsider the wave equation\n\\[\n\\frac{\\partial^2 y}{\\partial t^2} = a^2 \\frac{\\partial^2 y}{\\partial x^2},\n\\]\nwhere \\(a\\) is a known (nonzero) constant. Very often, it is convenient to eliminate the constant \\(a\\) by rescaling the time variable. Specifically, we introduce a new variable \\(\\tau\\) defined by \\(\\tau = at\\), so that\n\\[\ny(x,\\tau) = y(x, at).\n\\]\n\n\n\n\n\n\nShow that if \\(y(x,t)\\) is a solution to the wave equation, then we have \\[\n\\frac{\\partial^2 y}{\\partial \\tau^2} = \\frac{\\partial^2 y}{\\partial x^2},\n\\]\nwhich is the “rescaled” wave equation with no constant of proportionality.\n\n\n\n\n\n\n\n\n\nNoteSolution.\n\n\n\n\n\nWe begin by noting that if \\(f(x, \\tau)\\) is any function of \\(x\\) and \\(\\tau\\), then by the chain rule we have\n\\[\n\\frac{\\partial f}{\\partial t} = \\frac{\\partial f}{\\partial \\tau} \\cdot \\frac{\\partial \\tau}{\\partial t} = a \\frac{\\partial f}{\\partial \\tau}.\n\\]\nIn particular, with \\(f(x,\\tau) = y(x,\\tau)\\), we have\n\\[\n\\frac{\\partial y}{\\partial t} = a \\frac{\\partial y}{\\partial \\tau}.\n\\]\nBut then, applying the chain rule again with \\(f(x,\\tau) = \\frac{\\partial y}{\\partial \\tau}(x,\\tau)\\), we get\n\\[\n\\frac{\\partial^2 y}{\\partial t^2} = \\frac{\\partial}{\\partial t} \\left( \\frac{\\partial y}{\\partial t} \\right) = \\frac{\\partial}{\\partial t} \\left( a\\frac{\\partial y}{\\partial \\tau} \\right)  = a \\left[ \\frac{\\partial}{\\partial t} \\left( \\frac{\\partial y}{\\partial \\tau} \\right)\\right] = a \\left[ a \\frac{\\partial}{\\partial \\tau} \\left( \\frac{\\partial y}{\\partial \\tau}\\right) \\right] = a^2 \\frac{\\partial^2 y}{\\partial \\tau^2}.\n\\]\nTherefore, we have\n\\[\n\\frac{\\partial^2 y}{\\partial \\tau^2} = \\frac{1}{a^2} \\frac{\\partial^2 y}{\\partial t^2} = \\frac{1}{a^2} \\left( a^2 \\frac{\\partial^2 y}{\\partial \\tau^2} \\right) = \\frac{\\partial^2 y}{\\partial x^2},\n\\]\nas desired."
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-1-visualizing-complex-numbers",
    "href": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-1-visualizing-complex-numbers",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "Recall that a complex number \\(z\\) can be written in the form \\(z = x + iy\\), where \\(x\\) and \\(y\\) are real numbers and \\(i\\) is the imaginary unit, satisfying \\(i^2 = -1\\). The numbers \\(x\\) and \\(y\\) are called the real and imaginary parts of \\(z\\), respectively. For example, the complex number\n\\[\nz = 3 + 4i\n\\]\nhas real part \\(3\\) and imaginary part \\(4\\).\nThe set of all complex numbers is denoted by \\(\\mathbb{C}\\). The set of all ordered pairs \\((x,y)\\) of real numbers is denoted \\(\\mathbb{R}^2\\). As you well know, we can visualize \\(\\mathbb{R}^2\\) as the Cartesian plane, with the first coordinate \\(x\\) representing the horizontal axis and the second coordinate \\(y\\) representing the vertical axis.\nThere is a natural way to visualize complex numbers as points in the Cartesian plane as well, by identifying the complex number \\(z = x + iy\\) with the point \\((x,y)\\) in \\(\\mathbb{R}^2\\). In this way, the real part of the complex number corresponds to the horizontal coordinate of the point, and the imaginary part corresponds to the vertical coordinate of the point. This visualization of \\(\\mathbb{C}\\) is called the complex plane.\nThe absolute value (or magnitude, or modulus) of a complex number \\(z = x + iy\\) is defined by\n\\[\n|z| = \\sqrt{x^2 + y^2}.\n\\]\nVisually, this corresponds to the distance from the origin \\((0,0)\\) to the point \\((x,y)\\) in the complex plane.\nThe argument (or angle, or phase) of a complex number \\(z = x + iy\\) is defined as the angle \\(\\theta\\) in \\((-\\pi, \\pi]\\) (measured in radians, measured counterclockwise) between the positive real axis and the line segment connecting the origin to the point \\((x,y)\\). It is denoted \\(\\arg{z}\\).\nFinally, the complex conjugate of a complex number \\(z = x + iy\\) is defined as the complex number \\(\\overline{z} = x - iy\\). Visually, this corresponds to reflecting the point \\((x,y)\\) across the real axis to get the point \\((x,-y)\\).\n\n\n\n\n\n\n\nPlot the complex number \\(2 + 3i\\) in the complex plane.\nPlot the complex number \\(-1 + 4i\\) in the complex plane.\nPlot the complex number \\(-2 - 2i\\) in the complex plane.\nPlot the complex number \\(3\\) (in other words, \\(3 + 0i\\)) in the complex plane.\nPlot the complex number \\(-5i\\) (in other words, \\(0 - 5i\\)) in the complex plane.\nFind the absolute value, argument, and complex conjugate of the complex number \\(1 + i\\).\nFind the absolute value, argument, and complex conjugate of the complex number \\(-1 + i\\).\nFind the absolute value, argument, and complex conjugate of the complex number \\(2\\).\nFind the absolute value, argument, and complex conjugate of the complex number \\(-3\\).\nFind the absolute value, argument, and complex conjugate of the complex number \\(-5i\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nI’ll leave plotting exercises a-e to you.\n\n\\(|1 + i| = \\sqrt{2}\\), \\(\\arg(1 + i) = \\frac{\\pi}{4}\\), \\(\\overline{1 + i} = 1 - i\\).\n\\(|-1 + i| = \\sqrt{2}\\), \\(\\arg(-1 + i) = \\frac{3\\pi}{4}\\), \\(\\overline{-1 + i} = -1 - i\\).\n\\(|2| = 2\\), \\(\\arg(2) = 0\\), \\(\\overline{2} = 2\\).\n\\(|-3| = 3\\), \\(\\arg(-3) = \\pi\\), \\(\\overline{-3} = -3\\).\n\\(|-5i| = 5\\), \\(\\arg(-5i) = -\\frac{\\pi}{2}\\), \\(\\overline{-5i} = 5i\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-2-algebra-with-complex-numbers",
    "href": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-2-algebra-with-complex-numbers",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "Addition and subtraction of complex numbers work exactly as you would expect, essentially by collecting like terms. For example:\n\\[\n(2 + 3i) + (4 + 5i) = 6 + 8i\n\\]\nand\n\\[\n(2 - 2i) - (1 + 3i) = 1 - 5i.\n\\]\nTo multiply complex numbers, you use the distributive property (i.e., “FOIL”) and the fact that \\(i^2 = -1\\). For example:\n\\[\n(2+3i)(4+5i) = 8 + 10i + 12i + 15i^2 = 8 + 22i - 15 = -7 + 22i.\n\\]\nFor division of complex numbers, it is first helpful to define the complex conjugate of a complex number \\(z = x + iy\\) as the number \\(\\overline{z} = x - iy\\). The complex conjugate has the property that\n\\[\nz\\overline{z} = |z|^2,\n\\]\nwhere \\(|z| = \\sqrt{x^2 + y^2}\\) is the magnitude of the complex number, which is a real number. Thus,\n\\[\n\\frac{1}{z} = \\frac{\\overline{z}}{|z|^2}.\n\\]\nThis, then, gives us a formula for division of complex numbers: If \\(z\\) and \\(w\\) are two complex numbers, with \\(z\\neq 0\\), then\n\\[\n\\frac{w}{z} = \\frac{w\\overline{z}}{z\\overline{z}} = \\frac{w\\overline{z}}{|z|^2}.\n\\]\nFor example:\n\\[\n\\frac{2 + 3i}{4 + 5i} = \\frac{(2 + 3i)(4 - 5i)}{(4 + 5i)(4 - 5i)} = \\frac{8 - 10i + 12i - 15i^2}{16 + 25} = \\frac{23 + 2i}{41} = \\frac{23}{41} + \\frac{2}{41}i.\n\\]\n\n\n\n\n\n\nCompute the following.\n\n\\((1 + 2i) + (3 + 4i)\\)\n\\((5 + 6i) - (7 + 8i)\\)\n\\((1 + 2i)(4 - i)\\)\n\\(\\displaystyle\\frac{2 + 3i}{1 + 4i}\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\((1 + 2i) + (3 + 4i) = 4 + 6i\\)\n\\((5 + 6i) - (7 + 8i) = -2 - 2i\\)\n\\((1 + 2i)(4 - i) = 6 + 7i\\)\n\\(\\displaystyle\\frac{2 + 3i}{1 + 4i} = \\frac{14}{17} - \\frac{5}{17}i\\)"
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-3-complex-exponentials-trigonometric-functions-and-eulers-formula",
    "href": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-3-complex-exponentials-trigonometric-functions-and-eulers-formula",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "If \\(z\\in \\mathbb{C}\\) is a complex number, we define the number \\(e^z\\) via the power series expansion:\n\\[\ne^z = \\sum_{n=0}^\\infty \\frac{z^n}{n!} = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\cdots.\n\\]\nSimilarly, we define the sine and cosine of \\(z\\) via their power series expansions:\n\\[\n\\sin{z} = \\sum_{n=0}^\\infty \\frac{(-1)^n z^{2n+1}}{(2n+1)!} = z - \\frac{z^3}{3!} + \\frac{z^5}{5!} - \\cdots\n\\]\nand\n\\[\n\\cos{z} = \\sum_{n=0}^\\infty \\frac{(-1)^n z^{2n}}{(2n)!} = 1 - \\frac{z^2}{2!} + \\frac{z^4}{4!} - \\cdots.\n\\]\nYou might never have seen power series with complex arguments before, but all of the usual rules for manipulating power series still apply. In particular, these series all converge for every complex number \\(z\\).\n\n\n\n\n\n\n\nOne of the most important relationships involving complex exponentials is Euler’s formula, which says that \\[\n  e^{iz} = \\cos{z} + i\\sin{z}\n  \\] for all \\(z\\in \\mathbb{C}\\). Use the power series definitions above to verify Euler’s formula.\nUse Euler’s formula to derive the following formulas for cosine and sine in terms of complex exponentials: \\[\n  \\cos{z} = \\frac{e^{iz} + e^{-iz}}{2}\n  \\] and \\[\n  \\sin{z} = \\frac{e^{iz} - e^{-iz}}{2i}.\n  \\]\n\nNote: One of the most important properties of the complex exponential function is that it satisfies the same exponential rules as the real exponential function. In particular, for any complex numbers \\(z\\) and \\(w\\), we have \\[\ne^{z+w} = e^z e^w.\n\\]\nYou will not need to prove this fact, but it is useful to keep in mind.\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe solution is just a matter of substituting \\(iz\\) into the power series definition of the exponential function and then separating the resulting series into real and imaginary parts: \\[\n  \\begin{align*}\n  e^{iz} &= \\sum_{n=0}^\\infty \\frac{(iz)^n}{n!} \\\\\n  &= \\sum_{n=0}^\\infty \\frac{i^n z^n}{n!} \\\\\n  &= \\sum_{m=0}^\\infty \\frac{i^{2m} z^{2m}}{(2m)!} + \\sum_{m=0}^\\infty \\frac{i^{2m+1} z^{2m+1}}{(2m+1)!} \\\\\n  &= \\sum_{m=0}^\\infty \\frac{(-1)^m z^{2m}}{(2m)!} + i\\sum_{m=0}^\\infty \\frac{(-1)^m z^{2m+1}}{(2m+1)!} \\\\\n  &= \\cos{z} + i\\sin{z}.\n  \\end{align*}\n  \\] In the last equality, we used the power series definitions of sine and cosine.\nFirst note that \\[\n  e^{-iz} = \\cos{-z} + i\\sin{-z} = \\cos{z} - i\\sin{z},\n  \\] since all powers of \\(z\\) in the cosine series are even and all powers of \\(z\\) in the sine series are odd. Now we can add and subtract Euler’s formula and this new formula to get the desired results: \\[\n  \\begin{align*}\n  e^{iz} + e^{-iz} &= (\\cos{z} + i\\sin{z}) + (\\cos{z} - i\\sin{z}) = 2\\cos{z}, \\\\\n  e^{iz} - e^{-iz} &= (\\cos{z} + i\\sin{z}) - (\\cos{z} - i\\sin{z}) = 2i\\sin{z}.\n  \\end{align*}\n  \\] Dividing both equations by the appropriate constants gives the desired formulas."
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-4-graphs-of-functions-fmathbbr-to-mathbbc",
    "href": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-4-graphs-of-functions-fmathbbr-to-mathbbc",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "Suppose that we have a function \\(f:\\mathbb{R} \\to \\mathbb{C}\\), which means that the inputs to \\(f\\) are real numbers and the outputs are complex numbers. For example, the function\n\\[\nf(x) = x + 3x^2 i\n\\]\ntakes a real number \\(x\\) and outputs the complex number \\(x + 3x^2 i\\). The graph of such a function is a curve in the complex plane traced out by the points \\(f(x)\\) as \\(x\\) varies over the real numbers. For example, the graph of the function above is simply the parabola \\(y=3x^2\\) traced out from left to right, when you think of \\(\\mathbb{C}\\) as \\(\\mathbb{R}^2\\). (Can you see why?) The direction in which the curve is traced out as \\(x\\) increases is called the orientation of the graph.\n\n\n\n\n\n\nDescribe the graphs of the follwing functions \\(f:\\mathbb{R} \\to \\mathbb{C}\\), along with their orientations.\n\n\\(f(x) = 2 + ix\\)\n\\(f(x) = -x^2 - x\\)\n\\(f(x) = e^{ix}\\)\n\\(f(x) = e^{-ix}\\)\n\\(f(x) = e^{2\\pi ix}\\)\n\\(f(x) = e^{-3\\pi ix}\\)\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nThe graph is a vertical line at \\(x=2\\) in the complex plane, traced from bottom to top as \\(x\\) increases.\nThe graph is a downward-opening parabola in the complex plane, traced from right to left as \\(x\\) increases.\nThe graph is the unit circle in the complex plane, traced counterclockwise as \\(x\\) increases, completing one revolution over the interval \\([0,2\\pi]\\). This is because, using Euler’s formula, we have \\(e^{ix} = \\cos{x} + i\\sin{x}\\), and it is well-known from trigonometry that the point \\((\\cos{x}, \\sin{x})\\) traces out the unit circle as \\(x\\) varies over the real numbers.\nThe graph is the unit circle in the complex plane, traced clockwise as \\(x\\) increases, completing one revolution over the interval \\([0,2\\pi]\\).\nThe graph is the unit circle in the complex plane, traced counterclockwise as \\(x\\) increases, completing one revolution over the interval \\([0,1]\\).\nThe graph is the unit circle in the complex plane, traced clockwise as \\(x\\) increases, completing one revolution over the interval \\([0, \\frac{2}{3}]\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-5-derivatives-and-integrals-of-functions-fmathbbr-to-mathbbc",
    "href": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-5-derivatives-and-integrals-of-functions-fmathbbr-to-mathbbc",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "A function \\(f:\\mathbb{R} \\to \\mathbb{C}\\) can be written as\n\\[\nf(x) = u(x) + iv(x),\n\\]\nwhere \\(u(x)\\) and \\(v(x)\\) are real-valued functions of the real variable \\(x\\) called the real and imaginary parts of \\(f\\), respectively. For example, the function \\[\nf(x) = 2x^2 + i\\sin{x}\n\\]\nhas real part \\(u(x) = 2x^2\\) and imaginary part \\(v(x) = \\sin{x}\\).\nWe shall say \\(f\\) is continuous at a point \\(x = a\\) if both \\(u\\) and \\(v\\) are continuous at \\(x = a\\). Similarly, we say \\(f\\) is differentiable at \\(x = a\\) if both \\(u\\) and \\(v\\) are differentiable at \\(x = a\\). In this case, we define the derivative of \\(f\\) at \\(x = a\\) by\n\\[\nf'(x) = u'(x) + iv'(x).\n\\]\nIn the example above, we have\n\\[\nf'(x) = 4x + i\\cos{x}.\n\\]\nIntegrals of complex-valued functions of a real variable are defined similarly. We say that \\(f\\) is integrable on an interval \\([a,b]\\) if both \\(u\\) and \\(v\\) are integrable on \\([a,b]\\). In this case, we define the integral of \\(f\\) on \\([a,b]\\) by\n\\[\n\\int_a^b f(x)\\ dx = \\int_a^b u(x)\\ dx + i\\int_a^b v(x)\\ dx.\n\\]\nIn the example above, we have\n\\[\n\\int_0^\\pi f(x)\\ dx = \\int_0^\\pi 2x^2\\ dx + i\\int_0^\\pi \\sin{x}\\ dx = \\frac{2\\pi^3}{3} + 2i.\n\\]\n\n\n\n\n\n\nCompute the following.\n\n\\(\\displaystyle\\frac{d}{dx} \\left( \\cos{x} + i(2x^3-5) \\right)\\).\n\\(\\displaystyle\\int_0^{\\pi/4} \\left( \\sec^2{x} + i e^x \\right)\\ dx\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(-\\sin{x} + i6x^2\\)\n\\(1 + i\\left( e^{\\pi/4} - 1 \\right)\\)"
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-6-partial-derivatives-of-functions-fmathbbrn-to-mathbbr",
    "href": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-6-partial-derivatives-of-functions-fmathbbrn-to-mathbbr",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "A function of the form \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) means that \\(f\\) takes \\(n\\)-tuples \\((x_1,x_2,\\ldots,x_n)\\) of real numbers as inputs, and outputs a single real number. We think of \\(f\\) as a function of multiple variables, namely the \\(n\\) input variables \\(x_1,x_2,\\ldots,x_n\\).\nFor example, the function \\(f:\\mathbb{R}^2 \\to \\mathbb{R}\\) defined by \\[\nf(x,y) = x^2 e^{-y} + \\sin{(xy)}\n\\]\ntakes the orded pair \\((x,y)\\) as input and returns a real number.\nWhen dealing with functions of several variables, we often want to measure how the function changes as we vary just one of the input variables, while holding the others constant. This leads us to the concept of partial derivatives.\nFor simplicity, suppose we have only two input variables, \\(x\\) and \\(y\\), so that we are dealing with a function \\(f:\\mathbb{R}^2 \\to \\mathbb{R}\\). Recall that the partial derivative of \\(f\\) with respect to \\(x\\) is defined via the usual limit of a difference quotient, treating \\(y\\) as a constant:\n\\[\n\\frac{\\partial f}{\\partial x}(x,y) = \\lim_{h\\to 0} \\frac{f(x+h,y) - f(x,y)}{h}.\n\\]\nSimilarly, the partial derivative of \\(f\\) with respect to \\(y\\) is defined via the usual limit of a difference quotient, treating \\(x\\) as a constant:\n\\[\n\\frac{\\partial f}{\\partial y}(x,y) = \\lim_{h\\to 0} \\frac{f(x,y+h) - f(x,y)}{h}.\n\\]\nPhysically, \\(\\frac{\\partial f}{\\partial x}(x,y)\\) measures the (instantaneous) rate of change of \\(f\\) with respect to \\(x\\) at the point \\((x,y)\\) while \\(y\\) is held constant, while \\(\\frac{\\partial f}{\\partial y}(x,y)\\) measures the (instantaneous) rate of change of \\(f\\) with respect to \\(y\\) at the point \\((x,y)\\) while \\(x\\) is held constant.\nPartial derivatives may be computed using the same rules as ordinary derivatives, treating all other variables as constants. For example, for the function above, we have\n\\[\n\\frac{\\partial f}{\\partial x}(x,y) = 2x e^{-y} + y\\cos{(xy)}\n\\]\nand\n\\[\n\\frac{\\partial f}{\\partial y}(x,y) = -x^2 e^{-y} + x\\cos{(xy)}.\n\\]\n\n\n\n\n\n\nCompute the following.\n\n\\(\\displaystyle \\frac{\\partial}{\\partial x} \\left( x^3 y + \\ln{(xy)} \\right)\\).\n\\(\\displaystyle \\frac{\\partial}{\\partial y} \\left( e^{xy} + x^2 y^2 \\right)\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\n\\(3x^2 y + \\frac{1}{x}\\)\n\\(x e^{xy} + 2x^2 y\\)"
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-7-linear-transformations-on-vector-spaces",
    "href": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-7-linear-transformations-on-vector-spaces",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "Let \\(V\\) be a vector space, which means that, roughly speaking, \\(V\\) is a set of objects (called vectors) that can be added together and multiplied by scalars (numbers, either real or complex) in a way that satisfies certain axioms (like commutativity, associativity, distributivity, etc.).\nThe central example of a finite-dimensional (real) vector space is \\(\\mathbb{R}^n\\), the set of all \\(n\\)-tuples of real numbers. Vectors in \\(\\mathbb{R}^n\\) can be added componentwise and multiplied by real numbers componentwise. This should be familiar to you from your course on linear algebra, though in that course vectors in \\(\\mathbb{R}^n\\) were often represented as column vectors.\nOn the other hand, the central example of an infinite-dimensional (real) vector space is the set of all real-valued functions defined on some interval \\([a,b]\\). Very often, the functions will be required to satisfy some additional properties, such as continuity, differentiability, or integrability, but we won’t focus on those details here. In these “function spaces,” two functions \\(f\\) and \\(g\\) can be added together to form a new function \\(f + g\\), which is just the “pointwise” sum of the two functions: evaluating \\(f+g\\) at a point \\(x\\) goes like this:\n\\[\n(f + g)(x) = f(x) + g(x).\n\\]\nSimilarly, a function \\(f\\) can be multiplied by a scalar \\(c\\) to form a new function \\(cf\\), defined by\n\\[\n(cf)(x) = c \\cdot f(x).\n\\]\nNow, if \\(V\\) and \\(W\\) are any two vector spaces, recall that a function \\(T: V \\to W\\) is called a linear transformation if for all vectors \\(u,v \\in V\\) and all scalars \\(c\\), we have\n\nPreservation of addition: \\(T(u + v) = T(u) + T(v)\\).\nPreservation of scaling: \\(T(cu) = cT(u)\\).\n\nIf \\(W\\) coincides with \\(V\\), then \\(T\\) is often called a linear operator on \\(V\\).\nSuppose now that \\(V\\) is a vector space of functions \\(f:\\mathbb{R}^2 \\to \\mathbb{R}\\), with input variables \\(x\\) and \\(y\\), and with suitable differentiability properties. Convince yourself that all partial derivative operators are linear transformations on \\(V\\): for example, we have the function\n\\[\n\\frac{\\partial}{\\partial x}: V\\to V\n\\]\nthat carries a function \\(f\\) to its partial derivative with respect to \\(x\\), \\(\\frac{\\partial f}{\\partial x}\\). You know very well that this operator satisfies both properties of a linear transformation:\n\\[\n\\frac{\\partial}{\\partial x} (f + g) = \\frac{\\partial f}{\\partial x} + \\frac{\\partial g}{\\partial x}\n\\]\nand\n\\[\n\\frac{\\partial}{\\partial x} (cf) = c \\frac{\\partial f}{\\partial x}.\n\\]\nLikewise, all second-order partial derivative operators are linear operators on \\(V\\): for example, the transformations\n\\[\n\\frac{\\partial^2}{\\partial x^2}: V\\to V, \\quad \\text{and} \\quad \\frac{\\partial^2}{\\partial x \\partial y}: V\\to V,\n\\]\nhave the properties\n\\[\n\\frac{\\partial^2}{\\partial x^2} (f + g) = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 g}{\\partial x^2} \\quad \\text{and} \\quad \\frac{\\partial^2}{\\partial x \\partial y} (f + g) = \\frac{\\partial^2 f}{\\partial x \\partial y} + \\frac{\\partial^2 g}{\\partial x \\partial y}\n\\]\nas well as\n\\[\n\\frac{\\partial^2}{\\partial x^2} (cf) = c \\frac{\\partial^2 f}{\\partial x^2}\\quad \\text{and} \\quad \\frac{\\partial^2}{\\partial x \\partial y} (cf) = c \\frac{\\partial^2 f}{\\partial x \\partial y}.\n\\]\nThis pattern continues: all partial differential operators, of all degrees and no matter if they are mixed or non-mixed—all are linear operators.\nThe following exercise gives you a chance to check linearity for a more complicated partial differential operator, which is expressed as a “linear combination” of several partial derivative operators.\n\n\n\n\n\n\nLet \\(V\\) be the function space above, and consider the operator \\(L: V\\to V\\), defined by\n\\[\nL = \\frac{\\partial}{\\partial y} + x^2 \\frac{\\partial^2}{\\partial x^2} - \\sin{y} \\frac{\\partial}{\\partial x \\partial y}.\n\\]\n\nCompute \\(L(x^2 y + y^3\\cos{x})\\).\nNow compute \\(L(x^2y) + L(y^3\\cos{x})\\).\nMake sure that your answers in parts (a) and (b) agree.\nCompute \\(L(2x^2 y)\\).\nNow compute \\(2L(x^2 y)\\).\nMake sure that your answers in parts (d) and (e) agree.\nDo you think \\(L\\) is a linear operator on \\(V\\)? Why or why not? What would you need to check to be sure?\n\n\n\n\n\n\n\n\n\n\nNoteSolution.\n\n\n\n\n\n\nWe have \\[\n  \\begin{align*}\n  L(x^2 y + y^3\\cos{x}) &= \\frac{\\partial}{\\partial y} (x^2 y + y^3\\cos{x}) + x^2 \\frac{\\partial^2}{\\partial x^2} (x^2 y + y^3\\cos{x}) - \\sin{y} \\frac{\\partial}{\\partial x \\partial y} (x^2 y + y^3\\cos{x}) \\\\\n  &= x^2 + 3y^2 \\cos{x} + x^2(2y - y^3 \\cos{x}) - \\sin{y}(2x - 3y^2 \\sin{x}) \\\\\n  &= x^2 + 3y^2 \\cos{x} + 2x^2 y - x^2y^3 \\cos{x} - 2x\\sin{y} + 3y^2 \\sin{x}\\sin{y}.\n  \\end{align*}\n  \\]\nWe have \\[\n  \\begin{align*}\n  L(x^2 y) + L(y^3\\cos{x}) &= \\left[ \\frac{\\partial}{\\partial y} (x^2 y) + x^2 \\frac{\\partial^2}{\\partial x^2} (x^2 y) - \\sin{y} \\frac{\\partial}{\\partial x \\partial y} (x^2 y) \\right] \\\\\n  &\\quad + \\left[ \\frac{\\partial}{\\partial y} (y^3\\cos{x}) + x^2 \\frac{\\partial^2}{\\partial x^2} (y^3\\cos{x}) - \\sin{y} \\frac{\\partial}{\\partial x \\partial y} (y^3\\cos{x}) \\right] \\\\\n  &= (x^2 + 2x^2 y - 2x\\sin{y}) + (3y^2 \\cos{x} - x^2 y^3 \\cos{x} + 3y^2 \\sin{x}\\sin{y}) \\\\\n  &= x^2 + 3y^2 \\cos{x} + 2x^2 y - x^2y^3 \\cos{x} - 2x\\sin{y} + 3y^2 \\sin{x}\\sin{y}.\n  \\end{align*}\n  \\]\nYes, the answers in parts (a) and (b) agree.\nWe have \\[\n  \\begin{align*}\n  L(2x^2 y) &= \\frac{\\partial}{\\partial y} (2x^2 y) + x^2 \\frac{\\partial^2}{\\partial x^2} (2x^2 y) - \\sin{y} \\frac{\\partial}{\\partial x \\partial y} (2x^2 y) \\\\\n  &= 2x^2 + 4x^2 y - 4x\\sin{y}.\n  \\end{align*}\n  \\]\nWe have \\[\n  \\begin{align*}\n  2L(x^2 y) &= 2 \\left[ \\frac{\\partial}{\\partial y} (x^2 y) + x^2 \\frac{\\partial^2}{\\partial x^2} (x^2 y) - \\sin{y} \\frac{\\partial}{\\partial x \\partial y} (x^2 y) \\right] \\\\\n  &= 2(x^2 + 2x^2 y - 2x\\sin{y}) \\\\\n  &= 2x^2 + 4x^2 y - 4x\\sin{y}.\n  \\end{align*}\n  \\]\nYes, the answers in parts (d) and (e) agree.\nIf \\(L\\) were linear, then it need to preserve addition, i.e., \\(L(f+g) = L(f) + L(g)\\) for all \\(f,g\\in V\\), and it would need to preserve scaling, i.e., \\(L(cf) = cL(f)\\) for all \\(f\\in V\\) and all scalars \\(c\\). We have verified both of these properties for the specific functions \\(f(x,t) = x^2 t\\) and \\(g(x,t) = t^3 \\cos{x}\\), and for the scalar \\(c=2\\). From this evidence, it seems likely that \\(L\\) is a linear operator on \\(V\\). To be sure, we would need to verify these properties for all functions in \\(V\\) and all scalars."
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-8-eigenfunctions-of-differential-operators",
    "href": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-8-eigenfunctions-of-differential-operators",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "Let \\(V\\) be a vector space and \\(T:V\\to V\\) a linear operator. A nonzero vector \\(v\\in V\\) is called an eigenvector of \\(T\\) if there exists a scalar \\(\\lambda\\) such that\n\\[\nT(v) = \\lambda v.\n\\]\nIf \\(V\\) happens to be a function space, then eigenvectors of \\(T\\) are often called eigenfunctions of \\(T\\).\nThis definition is the generalization to arbitrary vector spaces of the familiar notion of eigenvalues and eigenvectors from your linear algebra course.\n\n\n\n\n\n\nLet \\(V\\) be a vector space of all functions \\(f:\\mathbb{R} \\to \\mathbb{R}\\), with input variable \\(x\\) (and with suitable differentiability properties).\n\nConsider the first-order differential operator \\[\n  \\frac{d}{dx}: V\\to V.\n  \\] For each real number \\(c\\), there is an eigenfunction of this operator associated to the eigenvalue \\(c\\). Find such an eigenfunction.\nConsider the second-order differential operator \\[\n  \\frac{d^2}{dx^2}: V\\to V.\n  \\] For each negative real number \\(c\\), there is an eigenfunction of this operator associated to the eigenvalue \\(c\\). Find such an eigenfunction.\n\n\n\n\n\n\n\n\n\n\nNoteSolution.\n\n\n\n\n\n\nWe are looking for a nonzero function \\(f(x)\\) such that \\[\n  \\frac{df}{dx} = c f(x).\n  \\] This is a (first-order, linear, homogeneous) ordinary differential equation, and its general solution is \\[\n  f(x) = Ae^{cx},\n  \\] where \\(A\\) is any nonzero constant. Thus, for each real number \\(c\\), the function \\(f(x) = e^{cx}\\) is an eigenfunction of the operator \\(\\frac{d}{dx}\\) associated to the eigenvalue \\(c\\).\nWe are looking for a nonzero function \\(f(x)\\) such that \\[\n  \\frac{d^2 f}{dx^2} = c f(x).\n  \\] This is a (second-order, linear, homogeneous) ordinary differential equation. The general solution to this equation is \\[\n  f(x) = A\\cos{(\\sqrt{-c} x)} + B\\sin{(\\sqrt{-c} x)},\n  \\] where \\(A\\) and \\(B\\) are constants, not both zero. Thus, for each negative real number \\(c\\), the functions \\(f(x) = \\cos{(\\sqrt{-c} x)}\\) and \\(f(x) = \\sin{(\\sqrt{-c} x)}\\) are eigenfunctions of the operator \\(\\frac{d^2}{dx^2}\\) associated to the eigenvalue \\(c\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-9-a-solution-to-the-wave-equation",
    "href": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-9-a-solution-to-the-wave-equation",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "Recall the wave equation\n\\[\n\\frac{\\partial^2 y}{\\partial t^2} = a^2 \\frac{\\partial^2 y}{\\partial x^2},\n\\]\nfrom class, where \\(a\\) is a known constant.\n\n\n\n\n\n\nShow that the function\n\\[\ny(x,t) = \\sin{\\left(\\frac{n\\pi x}{L}\\right)} \\cos{\\left(\\frac{n\\pi a t}{L}\\right)}\n\\]\nis a solution to the wave equation, for any fixed positive integer \\(n\\). (Note: This is the formula for the \\(n\\)-th harmonic of a vibrating string of length \\(L\\) with fixed endpoints.)\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nTo show that \\(y(x,t)\\) is a solution to the wave equation, we need to compute the second partial derivatives of \\(y\\) with respect to \\(t\\) and \\(x\\), and then verify that they satisfy the wave equation.\nThe first derivatives are:\n\\[\n\\begin{align*}\n\\frac{\\partial y}{\\partial t} &=\\left(-\\frac{n\\pi a}{L}\\right)\\cdot \\sin{\\left(\\frac{n\\pi x}{L}\\right)} \\sin{\\left(\\frac{n\\pi a t}{L}\\right)}, \\\\\n\\frac{\\partial y}{\\partial x} &= \\left(\\frac{n\\pi}{L}\\right)\\cdot  \\cos{\\left(\\frac{n\\pi x}{L}\\right)}  \\cos{\\left(\\frac{n\\pi a t}{L}\\right)}.\n\\end{align*}\n\\]\nThen, the second derivatives are:\n\\[\n\\begin{align*}\n\\frac{\\partial^2 y}{\\partial t^2} &= -\\left(\\frac{n\\pi a}{L}\\right)^2 \\sin{\\left(\\frac{n\\pi x}{L}\\right)} \\cos{\\left(\\frac{n\\pi a t}{L}\\right)}, \\\\\n\\frac{\\partial^2 y}{\\partial x^2} &= -\\left(\\frac{n\\pi}{L}\\right)^2 \\sin{\\left(\\frac{n\\pi x}{L}\\right)} \\cos{\\left(\\frac{n\\pi a t}{L}\\right)}.\n\\end{align*}\n\\]\nTherefore:\n\\[\n\\begin{align*}\na^2 \\frac{\\partial^2 y}{\\partial x^2} &= -a^2 \\left(\\frac{n\\pi}{L}\\right)^2 \\sin{\\left(\\frac{n\\pi x}{L}\\right)} \\cos{\\left(\\frac{n\\pi a t}{L}\\right)} \\\\\n&= -\\left(\\frac{n\\pi a}{L}\\right)^2 \\sin{\\left(\\frac{n\\pi x}{L}\\right)} \\cos{\\left(\\frac{n\\pi a t}{L}\\right)} \\\\\n&= \\frac{\\partial^2 y}{\\partial t^2}.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-10-a-solution-to-the-heat-equation",
    "href": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-10-a-solution-to-the-heat-equation",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "Recall the heat equation\n\\[\n\\frac{\\partial u}{\\partial t} = k \\frac{\\partial^2 u}{\\partial x^2},\n\\]\nfrom class, where \\(k\\) is a known constant.\n\n\n\n\n\n\nShow that the function\n\\[\nu(x,t) = t^{-1/2} e^{-x^2/(4kt)}\n\\]\nis a solution to the heat equation, for all \\(t &gt; 0\\).\n\n\n\n\n\n\n\n\n\nNoteSolution.\n\n\n\n\n\nAs in the previous exercise, we begin with the first derivatives:\n\\[\n\\begin{align*}\n\\frac{\\partial u}{\\partial t} &= - \\frac{1}{2} t^{-3/2} e^{-x^2/(4kt)} + t^{-1/2} e^{-x^2/(4kt)} \\cdot \\frac{x^2}{4kt^2}, \\\\\n\\frac{\\partial u}{\\partial x} &= t^{-1/2} e^{-x^2/(4kt)} \\cdot \\left( -\\frac{x}{2kt} \\right).\n\\end{align*}\n\\]\nThen, the second derivative with respect to \\(x\\) is:\n\\[\n\\begin{align*}\n\\frac{\\partial^2 u}{\\partial x^2} &=t^{-1/2} e^{-x^2/(4kt)} \\cdot \\left( -\\frac{1}{2kt} \\right) + t^{-1/2} e^{-x^2/(4kt)} \\cdot \\left( -\\frac{x}{2kt} \\right)^2 \\\\\n&= -\\frac{1}{2k} t^{-3/2} e^{-x^2/(4kt)} + t^{-1/2} e^{-x^2/(4kt)} \\cdot \\left( \\frac{x^2}{4k^2t^2} \\right).\n\\end{align*}\n\\]\nThus:\n\\[\nk \\frac{\\partial^2 u}{\\partial x^2} = -\\frac{1}{2} t^{-3/2} e^{-x^2/(4kt)} + t^{-1/2} e^{-x^2/(4kt)} \\cdot \\left( \\frac{x^2}{4kt^2} \\right) = \\frac{\\partial u}{\\partial t}.\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-11-eliminating-constants-of-proportionality",
    "href": "teaching/pde-sp-26/exercises/01ex-an-introduction-to-pdes.html#exercise-11-eliminating-constants-of-proportionality",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "Consider the wave equation\n\\[\n\\frac{\\partial^2 y}{\\partial t^2} = a^2 \\frac{\\partial^2 y}{\\partial x^2},\n\\]\nwhere \\(a\\) is a known (nonzero) constant. Very often, it is convenient to eliminate the constant \\(a\\) by rescaling the time variable. Specifically, we introduce a new variable \\(\\tau\\) defined by \\(\\tau = at\\), so that\n\\[\ny(x,\\tau) = y(x, at).\n\\]\n\n\n\n\n\n\nShow that if \\(y(x,t)\\) is a solution to the wave equation, then we have \\[\n\\frac{\\partial^2 y}{\\partial \\tau^2} = \\frac{\\partial^2 y}{\\partial x^2},\n\\]\nwhich is the “rescaled” wave equation with no constant of proportionality.\n\n\n\n\n\n\n\n\n\nNoteSolution.\n\n\n\n\n\nWe begin by noting that if \\(f(x, \\tau)\\) is any function of \\(x\\) and \\(\\tau\\), then by the chain rule we have\n\\[\n\\frac{\\partial f}{\\partial t} = \\frac{\\partial f}{\\partial \\tau} \\cdot \\frac{\\partial \\tau}{\\partial t} = a \\frac{\\partial f}{\\partial \\tau}.\n\\]\nIn particular, with \\(f(x,\\tau) = y(x,\\tau)\\), we have\n\\[\n\\frac{\\partial y}{\\partial t} = a \\frac{\\partial y}{\\partial \\tau}.\n\\]\nBut then, applying the chain rule again with \\(f(x,\\tau) = \\frac{\\partial y}{\\partial \\tau}(x,\\tau)\\), we get\n\\[\n\\frac{\\partial^2 y}{\\partial t^2} = \\frac{\\partial}{\\partial t} \\left( \\frac{\\partial y}{\\partial t} \\right) = \\frac{\\partial}{\\partial t} \\left( a\\frac{\\partial y}{\\partial \\tau} \\right)  = a \\left[ \\frac{\\partial}{\\partial t} \\left( \\frac{\\partial y}{\\partial \\tau} \\right)\\right] = a \\left[ a \\frac{\\partial}{\\partial \\tau} \\left( \\frac{\\partial y}{\\partial \\tau}\\right) \\right] = a^2 \\frac{\\partial^2 y}{\\partial \\tau^2}.\n\\]\nTherefore, we have\n\\[\n\\frac{\\partial^2 y}{\\partial \\tau^2} = \\frac{1}{a^2} \\frac{\\partial^2 y}{\\partial t^2} = \\frac{1}{a^2} \\left( a^2 \\frac{\\partial^2 y}{\\partial \\tau^2} \\right) = \\frac{\\partial^2 y}{\\partial x^2},\n\\]\nas desired."
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/02ex-separation-of-var.html",
    "href": "teaching/pde-sp-26/exercises/02ex-separation-of-var.html",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "In class, we “solved” the heat equation in the form of the following boundary-value problem:\n\\[\n\\frac{\\partial u}{\\partial t} = k \\frac{\\partial^2 u}{\\partial x^2},\n\\]\nwith\n\\[\nu(0,t) = 0 \\quad \\text{and} \\quad u(L,t) = 0, \\quad t &gt; 0.\n\\tag{1}\\]\nIn this problem, instead of the boundary conditions (1), we will consider the boundary conditions\n\\[\n\\frac{\\partial u}{\\partial x}(0,t) = 0 \\quad \\text{and} \\quad \\frac{\\partial u}{\\partial x}(L,t) = 0, \\quad t &gt; 0.\n\\tag{2}\\]\nAgain, notice that these are not initial conditions, since we only require \\(t&gt;0\\).\n\n\n\n\n\n\n\nSuppose that we have our metal rod lying along the \\(x\\)-axis from \\(x = 0\\) to \\(x = L\\), and that \\(u(x,t)\\) models the temperature at position \\(x\\) and time \\(t\\). What do the new boundary conditions (2) imply about the behavior of the temperature at the ends of the rod?\nWith the first set of boundary conditions (1), in class we found the “solution” \\[\nu(x,t) = \\sum_{n=1}^{\\infty} a_n \\exp\\left(\\frac{-n^2 \\pi^2 kt}{L^2} \\right) \\sin\\left( \\frac{n \\pi x}{L} \\right)\n\\] by separation of variables. How does this “solution” change if we use the new boundary conditions (2) instead?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nAt any fixed time \\(t&gt;0\\), the derivatives in (2) are the slopes of the tangent lines at \\(x=0\\) and \\(x=L\\) to the graph of \\(u(x,t)\\), plotted in the \\((x,u)\\)-plane. The boundary conditions tell us that these slopes are both \\(0\\), so the tangent lines are horizontal. This means that there is no “heat flow” into or out of the rod through the ends. Why? Take a simple example—consider the end of the rod at \\(x=0\\). If there was heat flow into the rod from values \\(x&lt;0\\), then the slope at \\(x=0\\) must be negative, since heat “flows” from higher to lower temperatures. Similarly, if there was heat flow out of the rod at \\(x=0\\), the slope would be positive. A similar analysis holds at the other end, where \\(x=L\\). We summarize all of this by saying that the boundary conditions (2) correspond to insulated ends, meaning no heat can enter or leave the rod through its endpoints.\nBasically, I’m telling you to solve the heat equation again, by separating variables. The main difference is now you’ll have the system of ODEs given by \\[\nT'(t) = -k \\lambda T(t), \\quad X''(x) +\\lambda X(x)=0\n\\] with the boundary conditions \\[\nX'(0) = 0 \\quad \\text{and} \\quad X'(L) = 0.\n\\] The general solution to the first is still \\[\nT(t) = C_0 \\exp(-k \\lambda t),\n\\] while the general solution to the second is \\[\nX(x) = C_1 \\cos(\\sqrt{\\lambda} x) + C_2 \\sin(\\sqrt{\\lambda} x).\n\\] The boundary condition \\(X'(0)=0\\) tells us that \\(C_2 = 0\\), so \\[\nX(x) = C_1 \\cos(\\sqrt{\\lambda} x).\n\\] The boundary condition \\(X'(L)=0\\) then gives \\[\n-\\sqrt{\\lambda} C_1 \\sin(\\sqrt{\\lambda} L) = 0,\n\\] which implies that either \\(C_1 = 0\\) or \\(\\sin(\\sqrt{\\lambda} L) = 0\\). The nontrivial solutions correspond to \\[\n\\sqrt{\\lambda} L = n \\pi, \\quad n = 0, \\pm 1, \\pm 2, \\pm 3, \\dots\n\\] so \\[\n\\lambda = \\left(\\frac{n \\pi}{L}\\right)^2.\n\\] Therefore, for each \\(n=0,1,2,3,\\dots\\), we have a solution \\[\nX_n(x) = \\cos\\left(\\frac{n \\pi x}{L}\\right).\n\\] Then, just as in class, this means that the infinite series “solution” is of the form \\[\nu(x,t) = \\sum_{n=0}^{\\infty} a_n \\exp\\left(\\frac{-n^2 \\pi^2 kt}{L^2} \\right) \\cos\\left( \\frac{n \\pi x}{L} \\right).\n\\]\n\n\n\n\n\n\n\nIn this exercise, you will solve the heat equation with real numbers and real initial conditions. Assume that the infinite series “solutions” that we have discovered by separating variables really are solutions, and that there are no problems with convergence.\n\n\n\n\n\n\nImagine a metal rod lying along the \\(x\\)-axis from \\(x=0\\) to \\(x=\\pi\\). Solve the heat equation\n\\[\n\\frac{\\partial u}{\\partial t} = \\frac{\\partial^2 u}{\\partial x^2},\n\\]\nwith boundary conditions \\[\n\\frac{\\partial u}{\\partial x}(0,t) = 0 \\quad \\text{and} \\quad \\frac{\\partial u}{\\partial x}(\\pi,t) = 0, \\quad t &gt; 0,\n\\]\nand initial condition \\[\nu(x,0) = \\cos{2x} +5\\cos{3x}, \\quad 0 \\le x \\le \\pi.\n\\]\nThen, once you have your solution \\(u(x,t)\\), make an animation in Desmos by plotting the solution in the \\((x,u)\\)-plane with a slider for \\(t\\).\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nFrom the previous exercise, we know that the general solution to the heat equation will be\n\\[\nu(x,t) = \\sum_{n=0}^{\\infty} a_n \\exp\\left(-n^2 t \\right) \\cos{nx },\n\\]\nsince \\(k=1\\) and \\(L=\\pi\\). We have\n\\[\nu(x,0) = \\sum_{n=0}^{\\infty} a_n\\cos{n x },\n\\]\nwhich must equal the initial condition \\(u(x,0) = \\cos{2x} +5\\cos{3x}\\), and so \\[\n\\cos{2x} +5\\cos{3x} = \\sum_{n=0}^{\\infty} a_n\\cos{n x }.\n\\]\nBy equating coefficients on both sides, we get that \\(a_2 = 1\\), \\(a_3 = 5\\), and \\(a_n = 0\\) for all other \\(n\\). Therefore, the solution we seek is \\[\nu(x,t) = \\exp(-4t) \\cos(2x) + 5 \\exp(-9t) \\cos(3x).\n\\]\n\n\n\n\n\n\nRecall that the wave equation is \\[\n\\frac{\\partial^2 u}{\\partial t^2} = a^2 \\frac{\\partial^2 u}{\\partial x^2},\n\\]\nwhere \\(a&gt;0\\) is a constant. Let’s suppose that we impose the boundary conditions \\[\nu(0,t) = 0 \\quad \\text{and} \\quad u(L,t) = 0, \\quad t &gt; 0,\n\\]\nwhere \\(L&gt;0\\) is a fixed length.\n\n\n\n\n\n\nJust as with the heat equation, we may “solve” the wave equation by separation of variables, to obtain an infinite series “solution”. Do it!\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nWe suppose that we have a solution of the form \\[\nu(x,t) = X(x)T(t).\n\\]\nSubstituting this into the wave equation, we get \\[\nX(x) T''(t) = a^2 X''(x) T(t).\n\\] Dividing both sides by \\(a^2 X(x) T(t)\\), we obtain \\[\n\\frac{T''(t)}{a^2 T(t)} = \\frac{X''(x)}{X(x)} = -\\lambda,\n\\] where \\(\\lambda\\) is a (real number) constant. This leads us to the system of two ODEs \\[\nT''(t) + a^2 \\lambda T(t) = 0, \\quad X''(x) + \\lambda X(x) = 0,\n\\]\nwith boundary conditions \\(X(0)=0\\) and \\(X(L)=0\\). The general solution to the first equation is \\[\nT(t) = C_1\\sin(\\sqrt{\\lambda}a t) + C_2\\cos(\\sqrt{\\lambda}a t),\n\\]\nwhile the general solution to the second equation is\n\\[\nX(x) = C_3 \\sin(\\sqrt{\\lambda} x) + C_4 \\cos(\\sqrt{\\lambda} x).\n\\]\nThe boundary condition \\(X(0)=0\\) implies \\(C_4 = 0\\), so we have\n\\[\nX(x) = C_3 \\sin(\\sqrt{\\lambda} x).\n\\]\nThen, since \\(X(L)=0\\), we must have\n\\[\n\\sqrt{\\lambda} L = n \\pi, \\quad n = 1, 2, 3, \\dots\n\\]\njust as in the case of the heat equation. So, for each \\(n=1,2,3,\\dots\\), we have a solution \\[\nX_n(x) = \\sin\\left(\\frac{n \\pi x}{L}\\right).\n\\] Then, just as in class, this means that the infinite series “solution” is of the form \\[\nu(x,t) = \\sum_{n=1}^{\\infty} \\left[ a_n \\cos\\left(\\frac{n \\pi a t}{L}\\right) + b_n \\sin\\left(\\frac{n \\pi a t}{L}\\right) \\right] \\sin\\left(\\frac{n \\pi x}{L}\\right),\n\\]\nfor some constants \\(a_n\\) and \\(b_n\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/02ex-separation-of-var.html#exercise-1-separating-variables-in-the-heat-equation-with-different-boundary-conditions",
    "href": "teaching/pde-sp-26/exercises/02ex-separation-of-var.html#exercise-1-separating-variables-in-the-heat-equation-with-different-boundary-conditions",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "In class, we “solved” the heat equation in the form of the following boundary-value problem:\n\\[\n\\frac{\\partial u}{\\partial t} = k \\frac{\\partial^2 u}{\\partial x^2},\n\\]\nwith\n\\[\nu(0,t) = 0 \\quad \\text{and} \\quad u(L,t) = 0, \\quad t &gt; 0.\n\\tag{1}\\]\nIn this problem, instead of the boundary conditions (1), we will consider the boundary conditions\n\\[\n\\frac{\\partial u}{\\partial x}(0,t) = 0 \\quad \\text{and} \\quad \\frac{\\partial u}{\\partial x}(L,t) = 0, \\quad t &gt; 0.\n\\tag{2}\\]\nAgain, notice that these are not initial conditions, since we only require \\(t&gt;0\\).\n\n\n\n\n\n\n\nSuppose that we have our metal rod lying along the \\(x\\)-axis from \\(x = 0\\) to \\(x = L\\), and that \\(u(x,t)\\) models the temperature at position \\(x\\) and time \\(t\\). What do the new boundary conditions (2) imply about the behavior of the temperature at the ends of the rod?\nWith the first set of boundary conditions (1), in class we found the “solution” \\[\nu(x,t) = \\sum_{n=1}^{\\infty} a_n \\exp\\left(\\frac{-n^2 \\pi^2 kt}{L^2} \\right) \\sin\\left( \\frac{n \\pi x}{L} \\right)\n\\] by separation of variables. How does this “solution” change if we use the new boundary conditions (2) instead?\n\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\n\nAt any fixed time \\(t&gt;0\\), the derivatives in (2) are the slopes of the tangent lines at \\(x=0\\) and \\(x=L\\) to the graph of \\(u(x,t)\\), plotted in the \\((x,u)\\)-plane. The boundary conditions tell us that these slopes are both \\(0\\), so the tangent lines are horizontal. This means that there is no “heat flow” into or out of the rod through the ends. Why? Take a simple example—consider the end of the rod at \\(x=0\\). If there was heat flow into the rod from values \\(x&lt;0\\), then the slope at \\(x=0\\) must be negative, since heat “flows” from higher to lower temperatures. Similarly, if there was heat flow out of the rod at \\(x=0\\), the slope would be positive. A similar analysis holds at the other end, where \\(x=L\\). We summarize all of this by saying that the boundary conditions (2) correspond to insulated ends, meaning no heat can enter or leave the rod through its endpoints.\nBasically, I’m telling you to solve the heat equation again, by separating variables. The main difference is now you’ll have the system of ODEs given by \\[\nT'(t) = -k \\lambda T(t), \\quad X''(x) +\\lambda X(x)=0\n\\] with the boundary conditions \\[\nX'(0) = 0 \\quad \\text{and} \\quad X'(L) = 0.\n\\] The general solution to the first is still \\[\nT(t) = C_0 \\exp(-k \\lambda t),\n\\] while the general solution to the second is \\[\nX(x) = C_1 \\cos(\\sqrt{\\lambda} x) + C_2 \\sin(\\sqrt{\\lambda} x).\n\\] The boundary condition \\(X'(0)=0\\) tells us that \\(C_2 = 0\\), so \\[\nX(x) = C_1 \\cos(\\sqrt{\\lambda} x).\n\\] The boundary condition \\(X'(L)=0\\) then gives \\[\n-\\sqrt{\\lambda} C_1 \\sin(\\sqrt{\\lambda} L) = 0,\n\\] which implies that either \\(C_1 = 0\\) or \\(\\sin(\\sqrt{\\lambda} L) = 0\\). The nontrivial solutions correspond to \\[\n\\sqrt{\\lambda} L = n \\pi, \\quad n = 0, \\pm 1, \\pm 2, \\pm 3, \\dots\n\\] so \\[\n\\lambda = \\left(\\frac{n \\pi}{L}\\right)^2.\n\\] Therefore, for each \\(n=0,1,2,3,\\dots\\), we have a solution \\[\nX_n(x) = \\cos\\left(\\frac{n \\pi x}{L}\\right).\n\\] Then, just as in class, this means that the infinite series “solution” is of the form \\[\nu(x,t) = \\sum_{n=0}^{\\infty} a_n \\exp\\left(\\frac{-n^2 \\pi^2 kt}{L^2} \\right) \\cos\\left( \\frac{n \\pi x}{L} \\right).\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/02ex-separation-of-var.html#exercise-2-solving-the-heat-equation-with-real-numbers",
    "href": "teaching/pde-sp-26/exercises/02ex-separation-of-var.html#exercise-2-solving-the-heat-equation-with-real-numbers",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "In this exercise, you will solve the heat equation with real numbers and real initial conditions. Assume that the infinite series “solutions” that we have discovered by separating variables really are solutions, and that there are no problems with convergence.\n\n\n\n\n\n\nImagine a metal rod lying along the \\(x\\)-axis from \\(x=0\\) to \\(x=\\pi\\). Solve the heat equation\n\\[\n\\frac{\\partial u}{\\partial t} = \\frac{\\partial^2 u}{\\partial x^2},\n\\]\nwith boundary conditions \\[\n\\frac{\\partial u}{\\partial x}(0,t) = 0 \\quad \\text{and} \\quad \\frac{\\partial u}{\\partial x}(\\pi,t) = 0, \\quad t &gt; 0,\n\\]\nand initial condition \\[\nu(x,0) = \\cos{2x} +5\\cos{3x}, \\quad 0 \\le x \\le \\pi.\n\\]\nThen, once you have your solution \\(u(x,t)\\), make an animation in Desmos by plotting the solution in the \\((x,u)\\)-plane with a slider for \\(t\\).\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nFrom the previous exercise, we know that the general solution to the heat equation will be\n\\[\nu(x,t) = \\sum_{n=0}^{\\infty} a_n \\exp\\left(-n^2 t \\right) \\cos{nx },\n\\]\nsince \\(k=1\\) and \\(L=\\pi\\). We have\n\\[\nu(x,0) = \\sum_{n=0}^{\\infty} a_n\\cos{n x },\n\\]\nwhich must equal the initial condition \\(u(x,0) = \\cos{2x} +5\\cos{3x}\\), and so \\[\n\\cos{2x} +5\\cos{3x} = \\sum_{n=0}^{\\infty} a_n\\cos{n x }.\n\\]\nBy equating coefficients on both sides, we get that \\(a_2 = 1\\), \\(a_3 = 5\\), and \\(a_n = 0\\) for all other \\(n\\). Therefore, the solution we seek is \\[\nu(x,t) = \\exp(-4t) \\cos(2x) + 5 \\exp(-9t) \\cos(3x).\n\\]"
  },
  {
    "objectID": "teaching/pde-sp-26/exercises/02ex-separation-of-var.html#exercise-3-solving-the-wave-equation",
    "href": "teaching/pde-sp-26/exercises/02ex-separation-of-var.html#exercise-3-solving-the-wave-equation",
    "title": "Partial Differential Equations: Exercises",
    "section": "",
    "text": "Recall that the wave equation is \\[\n\\frac{\\partial^2 u}{\\partial t^2} = a^2 \\frac{\\partial^2 u}{\\partial x^2},\n\\]\nwhere \\(a&gt;0\\) is a constant. Let’s suppose that we impose the boundary conditions \\[\nu(0,t) = 0 \\quad \\text{and} \\quad u(L,t) = 0, \\quad t &gt; 0,\n\\]\nwhere \\(L&gt;0\\) is a fixed length.\n\n\n\n\n\n\nJust as with the heat equation, we may “solve” the wave equation by separation of variables, to obtain an infinite series “solution”. Do it!\n\n\n\n\n\n\n\n\n\nNoteSolutions.\n\n\n\n\n\nWe suppose that we have a solution of the form \\[\nu(x,t) = X(x)T(t).\n\\]\nSubstituting this into the wave equation, we get \\[\nX(x) T''(t) = a^2 X''(x) T(t).\n\\] Dividing both sides by \\(a^2 X(x) T(t)\\), we obtain \\[\n\\frac{T''(t)}{a^2 T(t)} = \\frac{X''(x)}{X(x)} = -\\lambda,\n\\] where \\(\\lambda\\) is a (real number) constant. This leads us to the system of two ODEs \\[\nT''(t) + a^2 \\lambda T(t) = 0, \\quad X''(x) + \\lambda X(x) = 0,\n\\]\nwith boundary conditions \\(X(0)=0\\) and \\(X(L)=0\\). The general solution to the first equation is \\[\nT(t) = C_1\\sin(\\sqrt{\\lambda}a t) + C_2\\cos(\\sqrt{\\lambda}a t),\n\\]\nwhile the general solution to the second equation is\n\\[\nX(x) = C_3 \\sin(\\sqrt{\\lambda} x) + C_4 \\cos(\\sqrt{\\lambda} x).\n\\]\nThe boundary condition \\(X(0)=0\\) implies \\(C_4 = 0\\), so we have\n\\[\nX(x) = C_3 \\sin(\\sqrt{\\lambda} x).\n\\]\nThen, since \\(X(L)=0\\), we must have\n\\[\n\\sqrt{\\lambda} L = n \\pi, \\quad n = 1, 2, 3, \\dots\n\\]\njust as in the case of the heat equation. So, for each \\(n=1,2,3,\\dots\\), we have a solution \\[\nX_n(x) = \\sin\\left(\\frac{n \\pi x}{L}\\right).\n\\] Then, just as in class, this means that the infinite series “solution” is of the form \\[\nu(x,t) = \\sum_{n=1}^{\\infty} \\left[ a_n \\cos\\left(\\frac{n \\pi a t}{L}\\right) + b_n \\sin\\left(\\frac{n \\pi a t}{L}\\right) \\right] \\sin\\left(\\frac{n \\pi x}{L}\\right),\n\\]\nfor some constants \\(a_n\\) and \\(b_n\\)."
  },
  {
    "objectID": "teaching/pde-sp-26/pde-sp-26.html",
    "href": "teaching/pde-sp-26/pde-sp-26.html",
    "title": "Partial Differential Equations",
    "section": "",
    "text": "This course is an introduction to partial differential equations (PDEs) and Fourier analysis.\n\nWe will study the classical PDEs of mathematical physics: the heat equation, the wave equation, and Laplace’s equation.\nWe will learn how to solve these equations using separation of variables and Fourier series.\nComplex variables will be introduced as tools to help us understand Fourier series.\nProperties of complex exponentials, sines, and cosines will be generalized to abstract properties of functions in \\(L^2\\) spaces. Accordingly, will will study the notion of Hilbert spaces.\nAn introduction to Sturm-Liouville theory will be given, including eigenvalue problems and orthogonal functions.\nIf time permits, we will study additional topics such as Fourier and Laplace transforms.\n\n\nCourse information\n\n\n\nInstructor:\n\n\nJohn Myers\n\n\n\n\nOffice:\n\n\nMarano 175\n\n\n\n\nOffice hours:\n\n\n12-12:30 MWF, and by appointment\n\n\n\n\nSyllabus:\n\n\nlink\n\n\n\n\n\nSchedule, slides, exercises, and info\n\n\n\nweek\ndate\ntopics\nslides\nexercises\ninfo\n\n\n\n\nWeek 4\n02.20 fri\n\n\n\nQuiz on Sec 4 and 5\n\n\n\n02.18 wed\nSec 6: A first look at Fourier series, part 2\nSec 6 Slides\nSec 6 Exercises\n\n\n\n\n02.16 mon\nSec 5: A first look at Fourier series, part 1\nSec 5 Slides\nSec 5 Exercises\n\n\n\nWeek 3\n02.13 fri\nSame as below ↓\nSame as below ↓\nSame as below ↓\nQuiz on Sec 2 and 3\n\n\n\n02.11 wed\nSec 4: General PDEs and boundary conditions\nSec 4 Slides\nSec 4 Exercises\n\n\n\n\n02.09 mon\nSec 3: The Laplacian and Laplace’s equation\nSec 3 Slides\nSec 3 Exercises\n\n\n\nWeek 2\n02.06 fri\nSame as below ↓\nSame as below ↓\nSame as below ↓\nQuiz on Sec 1\n\n\n\n02.04 wed\nSec 2: Separation of variables\nSec 2 Slides\nSec 2 Exercises\n\n\n\n\n02.02 mon\nSame as below ↓\nSame as below ↓\nSame as below ↓\n\n\n\nWeek 1\n01.30 fri\nSec 1: An introduction to PDEs\nSec 1 Slides\nSame as below ↓\n\n\n\n\n01.28 wed\nAnother “remote day”: Watch the videos below if you haven’t already, continue working on Exercises 1-8.\n\nSame as below ↓\n\n\n\n\n01.26 mon\n“Remote day”: Watch the videos here, here, and here. Begin working on Exercises 1-8 (see link to the right).\n\nSec 1 Exercises"
  }
]